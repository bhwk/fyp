<!-- prettier-ignore -->

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>ray.rllib.algorithms.algorithm &#8212; Ray 3.0.0.dev0</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="https://docs.ray.io/en/master/_static/styles/theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="https://docs.ray.io/en/master/_static/styles/bootstrap.css?digest=ac02cc09edc035673794" rel="stylesheet" />
<link href="https://docs.ray.io/en/master/_static/styles/pydata-sphinx-theme.css?digest=ac02cc09edc035673794" rel="stylesheet" />

  
  <link href="https://docs.ray.io/en/master/_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=ac02cc09edc035673794" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="https://docs.ray.io/en/master/_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="https://docs.ray.io/en/master/_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="https://docs.ray.io/en/master/_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/css/termynal.css?v=2fc3cb5e" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/css/csat.css?v=c8f39c76" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/css/assistant.css?v=d345c55e" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/css/custom.css?v=2df59da5" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/docsearch.css?v=45fb5152" />
    <link rel="stylesheet" type="text/css" href="https://docs.ray.io/en/master/_static/pydata-docsearch-custom.css?v=7fb92720" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="https://docs.ray.io/en/master/_static/scripts/bootstrap.js?digest=ac02cc09edc035673794" />
<link rel="preload" as="script" href="https://docs.ray.io/en/master/_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794" />
  <script src="https://docs.ray.io/en/master/_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=ac02cc09edc035673794"></script>

    <script src="https://docs.ray.io/en/master/_static/documentation_options.js?v=d1493c90"></script>
    <script src="https://docs.ray.io/en/master/_static/doctools.js?v=9a2dae69"></script>
    <script src="https://docs.ray.io/en/master/_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="https://docs.ray.io/en/master/_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="https://docs.ray.io/en/master/_static/copybutton.js?v=9a29e97e"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/js/termynal.js?v=67cfcf08"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/js/custom.js?v=9e3b357f"></script>
    <script src="https://docs.ray.io/en/master/_static/js/csat.js?v=8e649b1c"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/js/assistant.js?v=73fdc522"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/docsearch.js?v=77274085"></script>
    <script defer="defer" src="https://docs.ray.io/en/master/_static/docsearch_config.js?v=d25523ed"></script>
    <script src="https://docs.ray.io/en/master/_static/design-tabs.js?v=36754332"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/ray/rllib/algorithms/algorithm';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.14.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://docs.ray.io/en/master/_static/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'master';
        DOCUMENTATION_OPTIONS.show_version_warning_banner = false;
        </script>
    <link rel="canonical" href="https://docs.ray.io/en/latest/_modules/ray/rllib/algorithms/algorithm.html" />
    <link rel="icon" href="https://docs.ray.io/en/master/_static/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" /><!-- Extra header to include at the top of each template.
Kept separately so that it can easily be included in any templates
that need to be overridden for individual pages; e.g. included
both in the usual template (layout.html) as well as (index.html). -->

<link rel="preconnect" href="https://fonts.googleapis.com" />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link
  href="https://fonts.googleapis.com/css2?family=Inter:wght@100;200;300;400;500;600;700;900&family=Roboto:wght@400;500;700&display=swap"
  rel="stylesheet"
/>
<link
  rel="stylesheet"
  title="light"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css"
  disabled="disabled"
/>
<link
  rel="stylesheet"
  title="dark"
  href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css"
  disabled="disabled"
/>
<link
  href="https://cdn.jsdelivr.net/npm/remixicon@4.1.0/fonts/remixicon.css"
  rel="stylesheet"
/>

<!-- Used for text embedded in html on the index page  -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- Parser used to call hljs on responses from Ray Assistant -->
<script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>

<!-- Sanitizer for Ray Assistant AI -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/dompurify/2.3.3/purify.min.js"></script>

<!-- Fathom - beautiful, simple website analytics -->
<script src="https://deer.ray.io/script.js" data-site="WYYANYOS" defer></script>
<!-- / Fathom -->

<!-- Google Tag Manager -->
<script>
  (function (w, d, s, l, i) {
    w[l] = w[l] || [];
    w[l].push({
      'gtm.start': new Date().getTime(),
      event: 'gtm.js',
    });
    var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s),
      dl = l != 'dataLayer' ? '&l=' + l : '';
    j.async = true;
    j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
    f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-P8H6KQG');
</script>
<!-- End Google Tag Manager -->
<!-- Data to be shared with JS on every page -->
<script>
  window.data = {
      copyIconSrc: "../../../../_static/copy-button.svg"
  };
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>

  <script async type="text/javascript" src="https://docs.ray.io/_/static/javascript/readthedocs-addons.js"></script><meta name="readthedocs-project-slug" content="anyscale-ray" /><meta name="readthedocs-version-slug" content="master" /><meta name="readthedocs-resolver-filename" content="/_modules/ray/rllib/algorithms/algorithm.html" /><meta name="readthedocs-http-status" content="200" /></head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="algorithm.html#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="https://docs.ray.io/en/master/search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
<div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
    <span class="fa-solid fa-bars"></span>
  </label>
  
  
  <div class=" navbar-header-items__start">
    
      <div class="navbar-item">
  

<a class="navbar-brand logo" href="../../../../index.html">
  <svg
   id="svg"
   version="1.1"
   viewBox="0, 0, 400,200.13540961408262"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <g id="g1">
    <path id="ray-text" d="m 102.875,36.782 c 0.16,0.041 0.423,0.041 0.583,0 0.161,-0.042 0.029,-0.077 -0.291,-0.077 -0.321,0 -0.452,0.035 -0.292,0.077 m 89.958,63.206 v 30.345 h 3.584 3.583 v -9.487 -9.487 l 9.958,-0.059 c 10.822,-0.064 10.056,-0.121 10.664,0.804 0.113,0.172 0.394,0.597 0.625,0.945 0.231,0.348 0.42,0.693 0.42,0.766 0,0.073 0.15,0.269 0.333,0.435 0.183,0.166 0.333,0.358 0.333,0.427 0,0.069 0.366,0.676 0.814,1.349 0.925,1.393 2.687,4.088 2.936,4.493 0.092,0.149 0.312,0.486 0.489,0.751 1.348,2.013 2.261,3.416 2.261,3.474 0,0.038 0.338,0.561 0.75,1.164 0.413,0.602 0.755,1.151 0.761,1.219 0.006,0.067 0.267,0.46 0.58,0.873 0.313,0.412 0.692,0.975 0.843,1.25 0.622,1.138 0.378,1.083 4.846,1.083 h 4.062 l -0.254,-0.458 c -0.139,-0.252 -0.677,-1.058 -1.196,-1.792 -1.001,-1.416 -1.194,-1.694 -5.308,-7.666 -1.452,-2.109 -2.684,-3.871 -2.737,-3.917 -0.053,-0.046 -0.172,-0.213 -0.263,-0.372 -0.326,-0.563 -2.842,-4.226 -3.541,-5.154 -0.969,-1.287 -1.025,-1.162 0.832,-1.838 0.726,-0.263 2.976,-1.446 3.305,-1.737 0.145,-0.128 0.319,-0.232 0.388,-0.232 0.287,0 2.659,-2.095 3.515,-3.105 3.686,-4.345 5.22,-10.506 4.352,-17.484 -0.442,-3.559 -1.381,-5.956 -3.555,-9.078 -0.491,-0.706 -3.235,-3.333 -3.481,-3.333 -0.128,0 -0.232,-0.062 -0.232,-0.136 0,-0.075 -0.319,-0.313 -0.708,-0.53 -0.39,-0.216 -0.746,-0.439 -0.792,-0.496 -0.077,-0.096 -1.012,-0.546 -2.532,-1.218 -1.038,-0.459 -3.173,-1.141 -4.051,-1.293 -0.459,-0.08 -1.359,-0.273 -2,-0.429 -1.04,-0.253 -2.711,-0.292 -15.375,-0.353 l -14.209,-0.07 v 30.346 m 80.935,-28.446 c -0.448,1.031 -1.004,2.287 -1.235,2.791 -0.396,0.861 -0.579,1.27 -1.363,3.038 -0.403,0.911 -1.14,2.528 -1.903,4.177 -0.33,0.714 -0.6,1.373 -0.6,1.464 0,0.092 -0.107,0.333 -0.238,0.536 -0.13,0.203 -0.661,1.344 -1.179,2.535 -0.519,1.192 -1.085,2.467 -1.258,2.834 -0.464,0.979 -1.328,2.9 -2.324,5.166 -1.025,2.334 -1.477,3.34 -1.759,3.917 -0.113,0.229 -1.423,3.154 -2.913,6.5 -1.49,3.346 -2.951,6.608 -3.248,7.25 -1.302,2.818 -8.248,18.378 -8.248,18.475 0,0.06 1.714,0.108 3.808,0.108 h 3.809 l 1.04,-2.375 c 0.573,-1.306 1.191,-2.712 1.374,-3.125 0.183,-0.412 0.483,-1.087 0.665,-1.5 0.183,-0.412 0.467,-1.05 0.63,-1.416 0.164,-0.367 0.656,-1.492 1.093,-2.5 0.437,-1.009 0.93,-2.134 1.097,-2.5 0.166,-0.367 0.399,-0.911 0.517,-1.209 l 0.215,-0.541 h 16.448 16.447 l 0.369,0.791 c 0.203,0.436 0.464,1.017 0.581,1.292 0.209,0.494 1.042,2.363 1.485,3.333 0.21,0.46 2.5,5.679 2.998,6.834 0.118,0.275 0.406,0.912 0.638,1.416 0.232,0.504 0.465,1.048 0.516,1.209 0.085,0.267 0.426,0.291 4.003,0.291 h 3.91 l -0.137,-0.458 c -0.122,-0.41 -1.026,-2.462 -2.331,-5.292 -0.254,-0.55 -1.531,-3.4 -2.838,-6.333 -1.307,-2.933 -2.618,-5.858 -2.913,-6.5 -0.412,-0.894 -5.399,-12.071 -8.292,-18.583 -0.183,-0.413 -0.473,-1.05 -0.644,-1.417 -0.324,-0.694 -1.063,-2.353 -1.56,-3.5 -0.159,-0.367 -0.427,-0.967 -0.595,-1.333 -0.867,-1.885 -2.251,-4.97 -2.916,-6.5 -0.195,-0.45 -0.423,-0.958 -1.421,-3.167 -0.166,-0.367 -0.502,-1.117 -0.747,-1.667 -0.63,-1.413 -1.07,-2.395 -1.419,-3.166 -0.652,-1.443 -0.97,-2.18 -1.059,-2.459 -0.086,-0.266 -0.421,-0.291 -3.891,-0.291 h -3.798 l -0.814,1.875 M 306.5,69.745 c 0,0.096 0.45,0.894 1.132,2.005 0.281,0.458 0.75,1.283 1.042,1.833 0.293,0.55 0.625,1.113 0.738,1.25 0.114,0.138 0.649,1.029 1.189,1.982 0.54,0.952 1.147,2.002 1.348,2.333 0.201,0.331 0.669,1.127 1.04,1.769 0.372,0.641 0.874,1.504 1.117,1.916 0.452,0.769 0.799,1.371 1.408,2.446 0.19,0.337 0.586,0.993 0.878,1.458 0.293,0.466 0.625,1.034 0.738,1.263 0.113,0.229 0.393,0.717 0.622,1.083 0.229,0.367 0.735,1.218 1.124,1.891 0.389,0.673 0.857,1.477 1.041,1.787 0.183,0.31 0.775,1.334 1.316,2.276 0.541,0.942 1.066,1.817 1.167,1.944 0.101,0.127 0.446,0.704 0.767,1.282 0.32,0.579 0.948,1.675 1.393,2.436 0.446,0.762 0.963,1.647 1.149,1.968 0.186,0.32 0.476,0.808 0.644,1.083 0.168,0.275 0.497,0.837 0.73,1.25 0.234,0.412 0.563,0.975 0.731,1.25 0.168,0.275 0.456,0.763 0.64,1.083 0.183,0.321 0.607,1.046 0.94,1.612 l 0.606,1.028 v 10.18 10.18 h 3.583 3.584 v -10.55 c 0,-6.074 0.064,-10.617 0.151,-10.708 0.083,-0.087 0.32,-0.458 0.528,-0.825 0.869,-1.535 1.109,-1.954 1.881,-3.277 1.098,-1.882 1.383,-2.373 1.773,-3.056 0.183,-0.321 0.473,-0.827 0.644,-1.125 0.171,-0.298 0.587,-1.029 0.926,-1.625 0.338,-0.596 0.683,-1.196 0.765,-1.334 0.428,-0.714 1.259,-2.124 1.424,-2.416 0.186,-0.329 0.612,-1.07 1.22,-2.125 0.172,-0.298 0.634,-1.104 1.027,-1.792 0.393,-0.687 0.815,-1.368 0.938,-1.513 0.122,-0.145 0.223,-0.389 0.223,-0.542 0,-0.153 0.061,-0.278 0.137,-0.278 0.075,0 0.302,-0.319 0.503,-0.709 0.202,-0.389 0.551,-1.008 0.775,-1.375 0.225,-0.366 0.551,-0.929 0.724,-1.25 0.63,-1.167 2.234,-3.927 2.52,-4.336 0.161,-0.23 0.547,-0.889 0.858,-1.462 0.312,-0.574 0.732,-1.323 0.935,-1.664 0.203,-0.342 0.561,-0.959 0.796,-1.371 0.234,-0.413 0.585,-1.013 0.779,-1.333 0.325,-0.538 0.934,-1.61 1.374,-2.417 0.1,-0.183 0.267,-0.446 0.371,-0.583 0.103,-0.138 0.425,-0.7 0.715,-1.25 0.29,-0.55 0.647,-1.171 0.793,-1.379 l 0.265,-0.378 -3.801,0.045 -3.801,0.045 -0.3,0.5 c -0.165,0.275 -0.455,0.762 -0.644,1.083 -0.189,0.321 -0.565,0.921 -0.835,1.334 -0.27,0.412 -0.569,0.9 -0.664,1.083 -0.096,0.183 -0.395,0.671 -0.665,1.083 -0.27,0.413 -0.645,1.013 -0.834,1.334 -0.189,0.32 -0.49,0.808 -0.668,1.083 -0.178,0.275 -0.479,0.762 -0.669,1.083 -0.19,0.321 -0.712,1.184 -1.159,1.917 -0.448,0.733 -1.086,1.783 -1.417,2.333 -0.332,0.55 -0.824,1.338 -1.093,1.75 -0.27,0.413 -0.542,0.863 -0.605,1 -0.064,0.138 -0.427,0.738 -0.807,1.334 -0.38,0.596 -0.834,1.308 -1.009,1.583 -0.175,0.275 -0.402,0.65 -0.503,0.833 -0.102,0.184 -0.696,1.159 -1.321,2.167 -0.625,1.008 -1.316,2.133 -1.536,2.5 -0.219,0.367 -0.584,0.967 -0.81,1.333 -0.226,0.367 -0.461,0.777 -0.523,0.911 -0.061,0.134 -0.42,0.696 -0.797,1.25 -0.378,0.553 -0.737,1.118 -0.799,1.256 -0.061,0.138 -0.3,0.55 -0.531,0.917 -0.23,0.366 -0.507,0.837 -0.615,1.046 -0.292,0.563 -0.932,-0.03 -1.58,-1.463 -0.063,-0.138 -0.27,-0.473 -0.461,-0.747 -0.39,-0.556 -0.504,-0.738 -1.098,-1.753 -0.559,-0.954 -0.693,-1.171 -1.084,-1.754 -0.183,-0.274 -0.422,-0.647 -0.531,-0.83 -0.109,-0.183 -0.695,-1.12 -1.302,-2.083 -0.607,-0.962 -1.193,-1.897 -1.302,-2.077 -0.534,-0.884 -1.693,-2.723 -1.97,-3.124 -0.171,-0.249 -0.311,-0.495 -0.311,-0.549 0,-0.053 -0.141,-0.3 -0.312,-0.548 C 323.21,84.6 322,82.674 322,82.582 c 0,-0.053 -0.139,-0.299 -0.309,-0.547 -0.761,-1.11 -1.663,-2.54 -1.81,-2.868 -0.061,-0.138 -0.435,-0.724 -0.83,-1.303 -0.395,-0.578 -0.718,-1.088 -0.718,-1.133 0,-0.044 -0.3,-0.532 -0.666,-1.084 C 317.3,75.095 317,74.611 317,74.571 c 0,-0.039 -0.356,-0.628 -0.792,-1.308 -0.435,-0.681 -1.12,-1.768 -1.522,-2.417 l -0.731,-1.179 h -3.727 c -2.051,0 -3.728,0.035 -3.728,0.078 m -86.75,7.023 c 3.486,0.433 8.055,2.022 8.389,2.917 0.03,0.081 0.138,0.148 0.239,0.148 0.278,0 2.413,2.27 2.792,2.969 0.184,0.338 0.399,0.727 0.48,0.865 2.258,3.865 1.774,11.495 -0.942,14.836 -0.206,0.254 -0.375,0.504 -0.375,0.556 0,0.13 -1.451,1.504 -2.196,2.079 -1.372,1.059 -4.274,2.252 -6.804,2.797 -1.265,0.272 -2.482,0.311 -11.375,0.359 L 200,104.348 V 90.424 76.5 l 8.792,-0.001 c 7.089,0 9.211,0.052 10.958,0.269 m 58.745,1.979 c 0.003,0.09 0.235,0.653 0.516,1.25 0.281,0.598 0.662,1.424 0.846,1.836 0.185,0.413 0.48,1.05 0.656,1.417 0.177,0.367 0.321,0.719 0.321,0.783 -10e-4,0.101 0.437,1.075 1.82,4.05 0.17,0.367 1.189,2.654 2.263,5.084 1.075,2.429 2.18,4.904 2.456,5.5 0.276,0.596 0.76,1.683 1.075,2.416 0.581,1.349 0.792,1.826 1.478,3.334 0.209,0.458 0.542,1.208 0.74,1.666 0.198,0.459 0.498,1.115 0.666,1.459 l 0.307,0.625 h -13.32 c -8.524,0 -13.319,-0.059 -13.319,-0.163 0,-0.089 0.164,-0.52 0.365,-0.958 0.622,-1.356 1.332,-2.938 1.572,-3.504 0.126,-0.298 0.337,-0.786 0.469,-1.084 0.131,-0.298 0.814,-1.854 1.517,-3.458 0.704,-1.604 1.411,-3.217 1.573,-3.583 0.161,-0.367 0.406,-0.93 0.544,-1.25 0.137,-0.321 0.794,-1.821 1.459,-3.334 1.615,-3.672 1.781,-4.049 2.154,-4.875 0.175,-0.389 0.674,-1.533 1.108,-2.541 0.937,-2.177 1.685,-3.833 2.006,-4.443 0.128,-0.243 0.233,-0.576 0.233,-0.74 0,-0.333 0.484,0.168 0.495,0.513" />
    <path id="ray-logo" d="m 101.476,36.871 c -1.303,0.145 -3.55,0.666 -4.332,1.004 -2.044,0.884 -2.472,1.092 -3.144,1.529 -7.177,4.661 -9.961,13.497 -6.746,21.409 0.465,1.144 0.582,1.352 1.707,3.057 0.986,1.494 3.928,4.463 4.424,4.463 0.054,0 0.429,0.223 0.832,0.496 4.639,3.13 12.64,3.283 17.395,0.332 0.457,-0.284 22.892,21.99 22.486,22.327 -0.218,0.181 -0.803,1.491 -1.437,3.22 l -0.352,0.959 h -6.238 c -5.795,0 -6.238,-0.021 -6.238,-0.288 0,-0.977 -1.759,-4.418 -3.151,-6.165 -4.027,-5.051 -11.482,-7.68 -17.226,-6.075 -0.711,0.198 -1.405,0.361 -1.542,0.361 -0.745,0 -4.681,2.22 -5.914,3.336 -0.321,0.291 -0.792,0.706 -1.046,0.923 -0.855,0.728 -2.606,3.23 -3.29,4.701 -0.372,0.801 -0.749,1.587 -0.837,1.746 -0.088,0.159 -0.16,0.434 -0.16,0.611 0,0.888 0.287,0.85 -6.415,0.85 H 74.04 l -0.198,-0.542 c -0.108,-0.298 -0.347,-0.917 -0.531,-1.375 -0.183,-0.458 -0.358,-0.946 -0.389,-1.083 -0.087,-0.39 -1.399,-2.503 -2.064,-3.324 -2.725,-3.366 -5.769,-5.225 -10.441,-6.377 -3.309,-0.816 -10.96,0.695 -12.419,2.453 -0.113,0.136 -0.278,0.248 -0.367,0.248 -0.392,0 -4.298,3.657 -4.298,4.024 0,0.086 -0.131,0.297 -0.291,0.469 -2.793,2.998 -3.936,11.297 -2.141,15.549 0.844,1.999 1.072,2.472 1.583,3.291 0.314,0.504 0.634,0.942 0.711,0.972 0.076,0.031 0.138,0.154 0.138,0.274 0,0.246 2.689,3.043 3.334,3.468 0.229,0.151 0.716,0.486 1.083,0.744 6.843,4.811 18.162,3.018 23.001,-3.643 0.093,-0.128 0.412,-0.542 0.709,-0.922 0.297,-0.38 0.54,-0.747 0.54,-0.815 0,-0.069 0.224,-0.489 0.498,-0.935 0.275,-0.445 0.461,-0.81 0.414,-0.81 -0.046,0 0.104,-0.372 0.335,-0.827 0.231,-0.456 0.42,-0.94 0.42,-1.076 0,-0.136 0.087,-0.439 0.193,-0.672 l 0.194,-0.425 h 6.22 6.22 l 0.101,0.459 c 1.063,4.828 6.623,10.565 11.322,11.682 0.366,0.087 1.154,0.288 1.75,0.446 7.975,2.118 18.533,-4.211 20.101,-12.05 l 0.107,-0.537 h 6.217 6.217 l 0.352,0.959 c 0.642,1.751 1.22,3.04 1.447,3.228 0.178,0.148 -2.094,2.503 -10.941,11.344 l -11.165,11.158 -0.793,-0.404 c -1.29,-0.658 -1.561,-0.769 -2.959,-1.21 -5.239,-1.651 -12.178,-0.296 -16.032,3.131 -1.412,1.256 -2.907,2.908 -3.508,3.878 -4.539,7.323 -3.445,16.326 2.69,22.138 0.992,0.939 3.39,2.578 4.35,2.972 1.875,0.771 2.482,1.005 2.729,1.053 0.149,0.03 0.849,0.174 1.556,0.32 10.388,2.155 20.37,-6.164 20.38,-16.983 0.002,-3.118 -0.379,-4.767 -1.788,-7.728 l -0.505,-1.061 11.107,-11.106 11.107,-11.105 0.873,0.447 c 1.444,0.739 3.341,1.372 5.068,1.691 1.606,0.297 5.042,0.225 6.39,-0.134 0.595,-0.159 1.37,-0.358 1.72,-0.442 1.198,-0.288 4.129,-1.822 5.32,-2.786 9.711,-7.857 8.716,-22.425 -1.957,-28.665 -3.504,-2.048 -8.663,-2.872 -12.167,-1.942 -0.55,0.146 -1.262,0.327 -1.583,0.403 -0.614,0.144 -1.774,0.627 -2.958,1.232 l -0.709,0.362 -11.105,-11.105 -11.105,-11.106 0.289,-0.561 c 3.009,-5.83 2.687,-12.491 -0.861,-17.825 -0.257,-0.386 -0.543,-0.821 -0.634,-0.965 -1.398,-2.203 -6.344,-5.524 -9.084,-6.099 -3.096,-0.651 -4.424,-0.766 -6.357,-0.551 m 4.095,8.912 c 6.101,1.712 8.316,9.521 3.993,14.078 -6.028,6.355 -16.205,1.325 -14.858,-7.345 0.559,-3.597 3.168,-6.047 7.544,-7.084 0.546,-0.13 2.249,0.05 3.321,0.351 M 59.946,91.812 c 2.314,0.806 4.061,2.37 5.152,4.615 l 0.644,1.323 v 2.25 2.25 l -0.648,1.332 c -3.748,7.711 -14.726,6.286 -16.362,-2.124 -1.223,-6.284 5.142,-11.76 11.214,-9.646 M 105,91.512 c 7.468,1.695 9.27,11.549 2.87,15.694 -7.325,4.746 -16.184,-3.082 -12.434,-10.986 0.56,-1.179 2.473,-3.387 2.935,-3.387 0.102,0 0.312,-0.113 0.465,-0.252 1.043,-0.944 4.259,-1.502 6.164,-1.069 m 46.441,0.171 c 4.417,1.35 7.025,5.54 6.148,9.877 -0.306,1.514 -0.863,2.879 -1.336,3.271 -0.139,0.116 -0.253,0.306 -0.253,0.424 0,0.307 -1.541,1.65 -2.554,2.224 -7.406,4.199 -15.743,-3.631 -12.051,-11.318 0.752,-1.565 3.286,-3.994 4.167,-3.994 0.112,0 0.276,-0.069 0.363,-0.153 0.713,-0.687 3.754,-0.87 5.516,-0.331 m -44.774,46.358 c 8.432,4.162 5.806,16.556 -3.5,16.523 C 100.595,154.555 96,152.103 96,150.74 c 0,-0.11 -0.106,-0.287 -0.236,-0.395 -1.065,-0.884 -1.517,-5.232 -0.754,-7.262 1.796,-4.781 7.334,-7.176 11.657,-5.042" />
  </g>
</svg>

</a></div>
    
  </div>
  
  <div class=" navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <div class="navbar-content docutils container">
<ul class="navbar-toplevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/getting-started.html" title="Get Started"><span class="navbar-link-title">Get Started</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/use-cases.html" title="Use Cases"><span class="navbar-link-title">Use Cases</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/examples.html" title="Example Gallery"><span class="navbar-link-title">Example Gallery</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-more-libs/installation.html" title="Library"><span class="navbar-link-title">Library</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-core/walkthrough.html" title="Ray Core"><span class="navbar-link-title">Ray Core</span>Scale general Python applications</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../data/data.html" title="Ray Data"><span class="navbar-link-title">Ray Data</span>Scale data ingest and preprocessing</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../train/train.html" title="Ray Train"><span class="navbar-link-title">Ray Train</span>Scale machine learning training</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../tune.html" title="Ray Tune"><span class="navbar-link-title">Ray Tune</span>Scale hyperparameter tuning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../serve/index.html" title="Ray Serve"><span class="navbar-link-title">Ray Serve</span>Scale model serving</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../rllib/index.html" title="Ray RLlib"><span class="navbar-link-title">Ray RLlib</span>Scale reinforcement learning</a></p>
</div>
</li>
</ul>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../index.html" title="Docs"><span class="navbar-link-title">Docs</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Resources"><span class="navbar-link-title">Resources</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Discussion Forum"><span class="navbar-link-title">Discussion Forum</span>Get your Ray questions answered</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://github.com/ray-project/ray-educational-materials" title="Training"><span class="navbar-link-title">Training</span>Hands-on learning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog" title="Blog"><span class="navbar-link-title">Blog</span>Updates, best practices, user-stories</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/events" title="Events"><span class="navbar-link-title">Events</span>Webinars, meetups, office hours</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on" title="Success Stories"><span class="navbar-link-title">Success Stories</span>Real-world workload examples</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/ray-libraries.html" title="Ecosystem"><span class="navbar-link-title">Ecosystem</span>Libraries integrated with Ray</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.ray.io/community" title="Community"><span class="navbar-link-title">Community</span>Connect with us</a></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>

</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          <div id="docsearch"></div>
        </div>
      
      
        <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="versionswitcherbutton" type="button" role="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown" aria-haspopup="listbox" aria-controls="versionswitcherlist" aria-label="Version switcher list">
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="versionswitcherlist" class="version-switcher__menu dropdown-menu list-group-flush py-0" role="listbox" aria-labelledby="versionswitcherbutton">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
      
        <div class="navbar-item"><a id="try-anyscale-href" href="https://console.anyscale.com/?utm_source=ray_docs&utm_medium=docs&utm_campaign=navbar">
  <div id="try-anyscale-text">
    <span>Try Ray on Anyscale</span>
  </div>
</a></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile"><div id="docsearch"></div>
    </div>
  

  
</div>

    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          <div class="navbar-item"><nav class="navbar-nav">
  <p class="sidebar-header-items__title"
     role="heading"
     aria-level="1"
     aria-label="Site Navigation">
    Site Navigation
  </p>
  <div class="navbar-content docutils container">
<ul class="navbar-toplevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/getting-started.html" title="Get Started"><span class="navbar-link-title">Get Started</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/use-cases.html" title="Use Cases"><span class="navbar-link-title">Use Cases</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/examples.html" title="Example Gallery"><span class="navbar-link-title">Example Gallery</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-more-libs/installation.html" title="Library"><span class="navbar-link-title">Library</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-core/walkthrough.html" title="Ray Core"><span class="navbar-link-title">Ray Core</span>Scale general Python applications</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../data/data.html" title="Ray Data"><span class="navbar-link-title">Ray Data</span>Scale data ingest and preprocessing</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../train/train.html" title="Ray Train"><span class="navbar-link-title">Ray Train</span>Scale machine learning training</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../tune.html" title="Ray Tune"><span class="navbar-link-title">Ray Tune</span>Scale hyperparameter tuning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../serve/index.html" title="Ray Serve"><span class="navbar-link-title">Ray Serve</span>Scale model serving</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../rllib/index.html" title="Ray RLlib"><span class="navbar-link-title">Ray RLlib</span>Scale reinforcement learning</a></p>
</div>
</li>
</ul>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../index.html" title="Docs"><span class="navbar-link-title">Docs</span></a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Resources"><span class="navbar-link-title">Resources</span></a></p>
<i class="fa-solid fa-chevron-down"></i></div>
<div class="navbar-dropdown docutils container">
<ul class="navbar-sublevel">
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://discuss.ray.io" title="Discussion Forum"><span class="navbar-link-title">Discussion Forum</span>Get your Ray questions answered</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://github.com/ray-project/ray-educational-materials" title="Training"><span class="navbar-link-title">Training</span>Hands-on learning</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog" title="Blog"><span class="navbar-link-title">Blog</span>Updates, best practices, user-stories</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/events" title="Events"><span class="navbar-link-title">Events</span>Webinars, meetups, office hours</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.anyscale.com/blog/how-ray-and-anyscale-make-it-easy-to-do-massive-scale-machine-learning-on" title="Success Stories"><span class="navbar-link-title">Success Stories</span>Real-world workload examples</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference internal" href="../../../../ray-overview/ray-libraries.html" title="Ecosystem"><span class="navbar-link-title">Ecosystem</span>Libraries integrated with Ray</a></p>
</div>
</li>
<li><div class="ref-container docutils container">
<p><a class="reference external" href="https://www.ray.io/community" title="Community"><span class="navbar-link-title">Community</span>Connect with us</a></p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>

</nav></div>
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">
<script>
document.write(`
  <div class="version-switcher__container dropdown">
    <button id="versionswitcherbutton" type="button" role="button" class="version-switcher__button btn btn-sm navbar-btn dropdown-toggle" data-bs-toggle="dropdown" aria-haspopup="listbox" aria-controls="versionswitcherlist" aria-label="Version switcher list">
      Choose version  <!-- this text may get changed later by javascript -->
      <span class="caret"></span>
    </button>
    <div id="versionswitcherlist" class="version-switcher__menu dropdown-menu list-group-flush py-0" role="listbox" aria-labelledby="versionswitcherbutton">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
  </div>
`);
</script></div>
        
          <div class="navbar-item"><a id="try-anyscale-href" href="https://console.anyscale.com/?utm_source=ray_docs&utm_medium=docs&utm_campaign=navbar">
  <div id="try-anyscale-text">
    <span>Try Ray on Anyscale</span>
  </div>
</a></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item"><nav id="main-sidebar" class="bd-docs-nav bd-links" aria-label="Section Navigation">
  <div class="bd-toc-item navbar-nav"><ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/getting-started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-more-libs/installation.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-overview/use-cases.html">Use Cases</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-air/getting-started.html">Ray for ML Infrastructure</a></li>

</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/examples.html">Example Gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-overview/ray-libraries.html">Ecosystem</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-core/walkthrough.html">Ray Core</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-core/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-core/user-guide.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/tasks.html">Tasks</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/tasks/nested-tasks.html">Nested Remote Functions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/tasks/generators.html">Dynamic generators</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-more-libs/actors.html">Actors</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/named-actors.html">Named Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/terminating-actors.html">Terminating Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/async_api.html">AsyncIO / Concurrency for Actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/concurrency_group_api.html">Limiting Concurrency Per-Method with Concurrency Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/actor-utils.html">Utility Classes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/out-of-band-communication.html">Out-of-band Communication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/actors/task-orders.html">Actor Task Execution Order</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/objects.html">Objects</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/objects/serialization.html">Serialization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/objects/object-spilling.html">Object Spilling</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/handling-dependencies.html">Environment Dependencies</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/scheduling/index.html">Scheduling</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/resources.html">Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/accelerators.html">Accelerator Support</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/placement-group.html">Placement Groups</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/memory-management.html">Memory Management</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/scheduling/ray-oom-prevention.html">Out-Of-Memory Prevention</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/fault-tolerance.html">Fault Tolerance</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/tasks.html">Task Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/actors.html">Actor Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/objects.html">Object Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/nodes.html">Node Fault Tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/fault_tolerance/gcs.html">GCS Fault Tolerance</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/patterns/index.html">Design Patterns &amp; Anti-patterns</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/nested-tasks.html">Pattern: Using nested tasks to achieve nested parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/generators.html">Pattern: Using generators to reduce heap memory usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/limit-pending-tasks.html">Pattern: Using ray.wait to limit the number of pending tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/limit-running-tasks.html">Pattern: Using resources to limit the number of concurrently running tasks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/concurrent-operations-async-actor.html">Pattern: Using asyncio to run actor methods concurrently</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/actor-sync.html">Pattern: Using an actor to synchronize other tasks and actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/tree-of-actors.html">Pattern: Using a supervisor actor to manage a tree of actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/pipelining.html">Pattern: Using pipelining to increase throughput</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/return-ray-put.html">Anti-pattern: Returning ray.put() ObjectRefs from a task harms performance and fault tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/ray-get-loop.html">Anti-pattern: Calling ray.get in a loop harms parallelism</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/unnecessary-ray-get.html">Anti-pattern: Calling ray.get unnecessarily harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/ray-get-submission-order.html">Anti-pattern: Processing results in submission order using ray.get increases runtime</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/ray-get-too-many-objects.html">Anti-pattern: Fetching too many objects at once with ray.get causes failure</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/too-fine-grained-tasks.html">Anti-pattern: Over-parallelizing with too fine-grained tasks harms speedup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/redefine-task-actor-loop.html">Anti-pattern: Redefining the same remote function or class harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/pass-large-arg-by-value.html">Anti-pattern: Passing the same large argument by value repeatedly harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/closure-capture-large-objects.html">Anti-pattern: Closure capturing large objects harms performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/global-variables.html">Anti-pattern: Using global variables to share state between tasks and actors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/patterns/out-of-band-object-ref-serialization.html">Anti-pattern: Serialize ray.ObjectRef out of band</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-core/advanced-topics.html">Advanced Topics</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/tips-for-first-time.html">Tips for first-time users</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/starting-ray.html">Starting Ray</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/ray-generator.html">Ray Generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/namespaces.html">Using Namespaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/cross-language.html">Cross-Language Programming</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/using-ray-with-jupyter.html">Working with Jupyter Notebooks &amp; JupyterLab</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/ray-dag.html">Lazy Computation Graphs with the Ray DAG API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/miscellaneous.html">Miscellaneous Topics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/runtime_env_auth.html">Authenticating Remote URIs in runtime_env</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-core/user-spawn-processes.html">Lifetimes of a User-Spawn Process</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-core/examples/overview.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/automl_for_time_series.html">Simple AutoML for time series with Ray Core</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/batch_prediction.html">Batch Prediction with Ray Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/gentle_walkthrough.html">A Gentle Introduction to Ray Core by Example</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/highly_parallel.html">Using Ray for Highly Parallelizable Tasks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/map_reduce.html">A Simple MapReduce Example with Ray Core</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/monte_carlo_pi.html">Monte Carlo Estimation of π</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/plot_hyperparameter.html">Simple Parallel Model Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/plot_parameter_server.html">Parameter Server</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/plot_pong_example.html">Learning to Play Pong</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/examples/web-crawler.html">Speed up your web crawler by parallelizing it with Ray</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-core/api/index.html">Ray Core API</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/core.html">Core API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/scheduling.html">Scheduling API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/runtime-env.html">Runtime Env API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/utility.html">Utility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/exceptions.html">Exceptions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-core/api/cli.html">Ray Core CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/cli.html">State CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/api.html">State API</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../data/data.html">Ray Data</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/overview.html">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/quickstart.html">Quickstart</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../data/user-guide.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/loading-data.html">Loading Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/inspecting-data.html">Inspecting Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/transforming-data.html">Transforming Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/iterating-over-data.html">Iterating over Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/shuffling-data.html">Shuffling Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/saving-data.html">Saving Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-images.html">Working with Images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-text.html">Working with Text</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-tensors.html">Working with Tensors / NumPy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/working-with-pytorch.html">Working with PyTorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/monitoring-your-workload.html">Monitoring Your Workload</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/execution-configurations.html">Execution Configurations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/batch_inference.html">End-to-end: Offline Batch Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/performance-tips.html">Advanced: Performance Tips and Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/custom-datasource-example.html">Advanced: Read and Write Custom File Types</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/examples.html">Examples</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../data/api/api.html">Ray Data API</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/input_output.html">Input/Output</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/dataset.html">Dataset API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/data_iterator.html">DataIterator API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/execution_options.html">ExecutionOptions API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/grouped_data.html">GroupedData API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/data_context.html">Global configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/utility.html">Utility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/preprocessor.html">Preprocessor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../data/api/from_other_data_libs.html">API Guide for Users from Other Data Libraries</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../data/data-internals.html">Ray Data Internals</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../train/train.html">Ray Train</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/overview.html">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/getting-started-pytorch.html">PyTorch Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/getting-started-pytorch-lightning.html">PyTorch Lightning Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/getting-started-transformers.html">Hugging Face Transformers Guide</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../train/more-frameworks.html">More Frameworks</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/huggingface-accelerate.html">Hugging Face Accelerate Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/deepspeed.html">DeepSpeed Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/distributed-tensorflow-keras.html">TensorFlow and Keras Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/distributed-xgboost-lightgbm.html">XGBoost and LightGBM Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/horovod.html">Horovod Guide</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../train/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/data-loading-preprocessing.html">Data Loading and Preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/using-gpus.html">Configuring Scale and GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/persistent-storage.html">Configuring Persistent Storage</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/monitoring-logging.html">Monitoring and Logging Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/checkpoints.html">Saving and Loading Checkpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/experiment-tracking.html">Experiment Tracking</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/results.html">Inspecting Training Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/fault-tolerance.html">Handling Failures and Node Preemption</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/reproducibility.html">Reproducibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../train/user-guides/hyperparameter-optimization.html">Hyperparameter Optimization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/benchmarks.html">Benchmarks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../train/api/api.html">Ray Train API</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../tune.html">Ray Tune</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../tune/getting-started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tune/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../tune/tutorials/overview.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-run.html">Running Basic Experiments</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-output.html">Logging and Outputs in Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-resources.html">Setting Trial Resources</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-search-spaces.html">Using Search Spaces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-stopping.html">How to Define Stopping Criteria for a Ray Tune Experiment</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-trial-checkpoints.html">How to Save and Load Trial Checkpoints</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-storage.html">How to Configure Persistent Storage in Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-fault-tolerance.html">How to Enable Fault Tolerance in Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-metrics.html">Using Callbacks and Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune_get_data_in_and_out.html">Getting Data in and out of Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/tune_analyze_results.html">Analyzing Tune Experiment Results</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../tune/examples/pbt_guide.html">A Guide to Population Based Training with Tune</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/pbt_visualization/pbt_visualization.html">Visualizing and Understanding PBT</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-distributed.html">Deploying Tune in the Cloud</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-lifecycle.html">Tune Architecture</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/tutorials/tune-scalability.html">Scalability Benchmarks</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../tune/examples/index.html">Ray Tune Examples</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../tune/examples/ml-frameworks.html">Examples using Ray Tune with ML Frameworks</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune_mnist_keras.html">Keras Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-pytorch-cifar.html">PyTorch Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-pytorch-lightning.html">PyTorch Lightning Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/pbt_ppo_example.html">Ray RLlib Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-xgboost.html">XGBoost Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/lightgbm_example.html">LightGBM Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/horovod_simple.html">Horovod Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/pbt_transformers.html">Hugging Face Transformers Example</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../tune/examples/experiment-tracking.html">Tune Experiment Tracking Examples</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-wandb.html">Weights &amp; Biases Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-mlflow.html">MLflow Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-aim.html">Aim Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/tune-comet.html">Comet Example</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../tune/examples/hpo-frameworks.html">Tune Hyperparameter Optimization Framework Examples</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-25"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/ax_example.html">Ax Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/hyperopt_example.html">HyperOpt Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/bayesopt_example.html">Bayesopt Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/bohb_example.html">BOHB Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/nevergrad_example.html">Nevergrad Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../tune/examples/optuna_example.html">Optuna Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/other-examples.html">Other Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/examples/exercises.html">Exercises</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../tune/faq.html">Ray Tune FAQ</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../tune/api/api.html">Ray Tune API</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-26"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/execution.html">Tune Execution (tune.Tuner)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/result_grid.html">Tune Experiment Results (tune.ResultGrid)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/trainable.html">Training in Tune (tune.Trainable, train.report)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/search_space.html">Tune Search Space API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/suggestion.html">Tune Search Algorithms (tune.search)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/schedulers.html">Tune Trial Schedulers (tune.schedulers)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/stoppers.html">Tune Stopping Mechanisms (tune.stopper)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/reporters.html">Tune Console Output (Reporters)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/syncing.html">Syncing in Tune (train.SyncConfig)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/logging.html">Tune Loggers (tune.logger)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/callbacks.html">Tune Callbacks (tune.Callback)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/env.html">Environment variables used by Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/integration.html">External library integrations for Ray Tune</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/internals.html">Tune Internals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../tune/api/cli.html">Tune CLI (Experimental)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../serve/index.html">Ray Serve</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-27"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/getting_started.html">Getting Started</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/develop-and-deploy.html">Develop and Deploy an ML Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/model_composition.html">Deploy Compositions of Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/multi-app.html">Deploy Multiple Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/model-multiplexing.html">Model Multiplexing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/configure-serve-deployment.html">Configure Ray Serve deployments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/http-guide.html">Set Up FastAPI and HTTP</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../serve/production-guide/index.html">Production Guide</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-28"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/config.html">Serve Config Files</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/kubernetes.html">Deploy on Kubernetes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/docker.html">Custom Docker Images</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/fault-tolerance.html">Add End-to-End Fault Tolerance</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/handling-dependencies.html">Handle Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/production-guide/best-practices.html">Best practices in production</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/monitoring.html">Monitor Your Application</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/resource-allocation.html">Resource Allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/autoscaling-guide.html">Ray Serve Autoscaling</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../serve/advanced-guides/index.html">Advanced Guides</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-29"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/app-builder-guide.html">Pass Arguments to Applications</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/advanced-autoscaling.html">Advanced Ray Serve Autoscaling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/performance.html">Performance Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/dyn-req-batch.html">Dynamic Request Batching</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/inplace-updates.html">Updating Applications In-Place</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/dev-workflow.html">Development Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/grpc-guide.html">Set Up a gRPC Service</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/managing-java-deployments.html">Experimental Java API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/deploy-vm.html">Deploy on VM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../serve/advanced-guides/multi-app-container.html">Run Multiple Applications in Different Containers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/architecture.html">Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../serve/api/index.html">Ray Serve API</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../rllib/index.html">Ray RLlib</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-30"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/rllib-training.html">Getting Started with RLlib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/rllib-env.html">Environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/rllib-algorithms.html">Algorithms</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../rllib/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-31"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-advanced-api.html">Advanced Python APIs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-models.html">Models, Preprocessors, and Action Distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-saving-and-loading-algos-and-policies.html">Saving and Loading your RL Algorithms and Policies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-concepts.html">How To Customize Policies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/single-agent-episode.html">Episodes</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-replay-buffers.html">Replay Buffers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-offline.html">Working With Offline Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-catalogs.html">Catalog (Alpha)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-rlmodule.html">RL Modules (Alpha)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-learner.html">Learner (Alpha)</a></li>



<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-torch2x.html">Using RLlib with torch 2.x compile</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-fault-tolerance.html">Fault Tolerance And Elastic Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/rllib-dev.html">Install RLlib for Development</a></li>




</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/rllib-examples.html">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/rllib-new-api-stack.html">RLlib’s new API stack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../rllib/new-api-stack-migration-guide.html">New API stack migration guide</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../rllib/package_ref/index.html">Ray RLlib API</a><input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-32"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/algorithm.html">Algorithms</a><input class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-33"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.copy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.freeze</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.callbacks.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.callbacks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.debugging.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.debugging</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.environment.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.evaluation.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.evaluation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.experimental.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.experimental</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.fault_tolerance.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.fault_tolerance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.framework.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.multi_agent.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.multi_agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.offline_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.python_environment.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.python_environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.reporting.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.reporting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.resources.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rollouts.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rollouts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.training.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_learner_class</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_default_rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_evaluation_config_object</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_rl_module_spec.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_rl_module_spec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_multi_agent_setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.get_rollout_fragment_length</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate_train_batch_size_vs_rollout_fragment_length.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.validate_train_batch_size_vs_rollout_fragment_length</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html">ray.rllib.algorithms.algorithm.Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.setup.html">ray.rllib.algorithms.algorithm.Algorithm.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_default_config.html">ray.rllib.algorithms.algorithm.Algorithm.get_default_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.compute_actions.html">ray.rllib.algorithms.algorithm.Algorithm.compute_actions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.compute_single_action.html">ray.rllib.algorithms.algorithm.Algorithm.compute_single_action</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.evaluate.html">ray.rllib.algorithms.algorithm.Algorithm.evaluate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint.html">ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.from_state.html">ray.rllib.algorithms.algorithm.Algorithm.from_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_weights.html">ray.rllib.algorithms.algorithm.Algorithm.get_weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.set_weights.html">ray.rllib.algorithms.algorithm.Algorithm.set_weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.export_model.html">ray.rllib.algorithms.algorithm.Algorithm.export_model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.export_policy_checkpoint.html">ray.rllib.algorithms.algorithm.Algorithm.export_policy_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.export_policy_model.html">ray.rllib.algorithms.algorithm.Algorithm.export_policy_model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.restore.html">ray.rllib.algorithms.algorithm.Algorithm.restore</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.restore_workers.html">ray.rllib.algorithms.algorithm.Algorithm.restore_workers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.save.html">ray.rllib.algorithms.algorithm.Algorithm.save</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.save_checkpoint.html">ray.rllib.algorithms.algorithm.Algorithm.save_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.train.html">ray.rllib.algorithms.algorithm.Algorithm.train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.training_step.html">ray.rllib.algorithms.algorithm.Algorithm.training_step</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.add_policy.html">ray.rllib.algorithms.algorithm.Algorithm.add_policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.remove_policy.html">ray.rllib.algorithms.algorithm.Algorithm.remove_policy</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/env.html">Environments</a><input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-34"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/base_env.html">BaseEnv API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/multi_agent_env.html">MultiAgentEnv API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/vector_env.html">VectorEnv API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/external_env.html">ExternalEnv API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/env/single_agent_episode.html">SingleAgentEpisode API</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/policy.html">Policy API</a><input class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-35"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.policy.Policy.html">ray.rllib.policy.policy.Policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.html">ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.html">ray.rllib.policy.torch_policy_v2.TorchPolicyV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.make_model.html">ray.rllib.policy.torch_policy_v2.TorchPolicyV2.make_model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.make_model_and_action_dist.html">ray.rllib.policy.torch_policy_v2.TorchPolicyV2.make_model_and_action_dist</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.make_model.html">ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.make_model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.policy.Policy.compute_actions.html">ray.rllib.policy.policy.Policy.compute_actions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.policy.Policy.compute_actions_from_input_dict.html">ray.rllib.policy.policy.Policy.compute_actions_from_input_dict</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.policy.Policy.compute_single_action.html">ray.rllib.policy.policy.Policy.compute_single_action</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.action_sampler_fn.html">ray.rllib.policy.torch_policy_v2.TorchPolicyV2.action_sampler_fn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.action_distribution_fn.html">ray.rllib.policy.torch_policy_v2.TorchPolicyV2.action_distribution_fn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.extra_action_out.html">ray.rllib.policy.torch_policy_v2.TorchPolicyV2.extra_action_out</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.action_sampler_fn.html">ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.action_sampler_fn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.action_distribution_fn.html">ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.action_distribution_fn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.extra_action_out_fn.html">ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.extra_action_out_fn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.compute_gradients.html">ray.rllib.policy.Policy.compute_gradients</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.apply_gradients.html">ray.rllib.policy.Policy.apply_gradients</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.extra_compute_grad_fetches.html">ray.rllib.policy.torch_policy_v2.TorchPolicyV2.extra_compute_grad_fetches</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.extra_grad_process.html">ray.rllib.policy.torch_policy_v2.TorchPolicyV2.extra_grad_process</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.grad_stats_fn.html">ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.grad_stats_fn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.compute_gradients_fn.html">ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.compute_gradients_fn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.apply_gradients_fn.html">ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.apply_gradients_fn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.extra_learn_fetches_fn.html">ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.extra_learn_fetches_fn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.learn_on_batch.html">ray.rllib.policy.Policy.learn_on_batch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.load_batch_into_buffer.html">ray.rllib.policy.Policy.load_batch_into_buffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.learn_on_loaded_batch.html">ray.rllib.policy.Policy.learn_on_loaded_batch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.learn_on_batch_from_replay_buffer.html">ray.rllib.policy.Policy.learn_on_batch_from_replay_buffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.get_num_samples_loaded_into_buffer.html">ray.rllib.policy.Policy.get_num_samples_loaded_into_buffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.loss.html">ray.rllib.policy.Policy.loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.compute_log_likelihoods.html">ray.rllib.policy.Policy.compute_log_likelihoods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.on_global_var_update.html">ray.rllib.policy.Policy.on_global_var_update</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.postprocess_trajectory.html">ray.rllib.policy.Policy.postprocess_trajectory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.optimizer.html">ray.rllib.policy.torch_policy_v2.TorchPolicyV2.optimizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.get_tower_stats.html">ray.rllib.policy.torch_policy_v2.TorchPolicyV2.get_tower_stats</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.optimizer.html">ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.optimizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.stats_fn.html">ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.stats_fn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.from_checkpoint.html">ray.rllib.policy.Policy.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.export_checkpoint.html">ray.rllib.policy.Policy.export_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.export_model.html">ray.rllib.policy.Policy.export_model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.from_state.html">ray.rllib.policy.Policy.from_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.get_weights.html">ray.rllib.policy.Policy.get_weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.set_weights.html">ray.rllib.policy.Policy.set_weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.get_state.html">ray.rllib.policy.Policy.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.set_state.html">ray.rllib.policy.Policy.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.import_model_from_h5.html">ray.rllib.policy.Policy.import_model_from_h5</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.Policy.get_initial_state.html">ray.rllib.Policy.get_initial_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.Policy.num_state_tensors.html">ray.rllib.Policy.num_state_tensors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.Policy.is_recurrent.html">ray.rllib.Policy.is_recurrent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.apply.html">ray.rllib.policy.Policy.apply</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.get_session.html">ray.rllib.policy.Policy.get_session</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.init_view_requirements.html">ray.rllib.policy.Policy.init_view_requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.get_host.html">ray.rllib.policy.Policy.get_host</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.Policy.get_exploration_state.html">ray.rllib.policy.Policy.get_exploration_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.torch_policy_v2.TorchPolicyV2.get_batch_divisibility_req.html">ray.rllib.policy.torch_policy_v2.TorchPolicyV2.get_batch_divisibility_req</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.variables.html">ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.get_batch_divisibility_req.html">ray.rllib.policy.eager_tf_policy_v2.EagerTFPolicyV2.get_batch_divisibility_req</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/models.html">Model APIs</a><input class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-36"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.modelv2.ModelV2.html">ray.rllib.models.modelv2.ModelV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.torch.torch_modelv2.TorchModelV2.html">ray.rllib.models.torch.torch_modelv2.TorchModelV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.tf.tf_modelv2.TFModelV2.html">ray.rllib.models.tf.tf_modelv2.TFModelV2</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.modelv2.ModelV2.forward.html">ray.rllib.models.modelv2.ModelV2.forward</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.modelv2.ModelV2.value_function.html">ray.rllib.models.modelv2.ModelV2.value_function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.modelv2.ModelV2.last_output.html">ray.rllib.models.modelv2.ModelV2.last_output</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.modelv2.ModelV2.get_initial_state.html">ray.rllib.models.modelv2.ModelV2.get_initial_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.modelv2.ModelV2.is_time_major.html">ray.rllib.models.modelv2.ModelV2.is_time_major</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.modelv2.ModelV2.variables.html">ray.rllib.models.modelv2.ModelV2.variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.modelv2.ModelV2.trainable_variables.html">ray.rllib.models.modelv2.ModelV2.trainable_variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.distributions.Distribution.html">ray.rllib.models.distributions.Distribution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.modelv2.ModelV2.custom_loss.html">ray.rllib.models.modelv2.ModelV2.custom_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.models.modelv2.ModelV2.metrics.html">ray.rllib.models.modelv2.ModelV2.metrics</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/catalogs.html">Catalog API</a><input class="toctree-checkbox" id="toctree-checkbox-37" name="toctree-checkbox-37" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-37"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.models.catalog.Catalog.html">ray.rllib.core.models.catalog.Catalog</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.models.catalog.Catalog.build_encoder.html">ray.rllib.core.models.catalog.Catalog.build_encoder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.models.catalog.Catalog.get_action_dist_cls.html">ray.rllib.core.models.catalog.Catalog.get_action_dist_cls</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.models.catalog.Catalog.get_tokenizer_config.html">ray.rllib.core.models.catalog.Catalog.get_tokenizer_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.models.catalog.Catalog.latent_dims.html">ray.rllib.core.models.catalog.Catalog.latent_dims</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.models.catalog.Catalog._determine_components_hook.html">ray.rllib.core.models.catalog.Catalog._determine_components_hook</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.models.catalog.Catalog._get_encoder_config.html">ray.rllib.core.models.catalog.Catalog._get_encoder_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.models.catalog.Catalog._get_dist_cls_from_action_space.html">ray.rllib.core.models.catalog.Catalog._get_dist_cls_from_action_space</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/rl_modules.html">RLModule API</a><input class="toctree-checkbox" id="toctree-checkbox-38" name="toctree-checkbox-38" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-38"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.build.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.build</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleSpec.get_rl_module_config.html">ray.rllib.core.rl_module.rl_module.RLModuleSpec.get_rl_module_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleConfig.html">ray.rllib.core.rl_module.rl_module.RLModuleConfig</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleConfig.to_dict.html">ray.rllib.core.rl_module.rl_module.RLModuleConfig.to_dict</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleConfig.from_dict.html">ray.rllib.core.rl_module.rl_module.RLModuleConfig.from_dict</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModuleConfig.get_catalog.html">ray.rllib.core.rl_module.rl_module.RLModuleConfig.get_catalog</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.build</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.get_multi_rl_module_config.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModuleSpec.get_multi_rl_module_config</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.html">ray.rllib.core.rl_module.rl_module.RLModule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.as_multi_rl_module.html">ray.rllib.core.rl_module.rl_module.RLModule.as_multi_rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_train.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.forward_inference.html">ray.rllib.core.rl_module.rl_module.RLModule.forward_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_train.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule._forward_inference.html">ray.rllib.core.rl_module.rl_module.RLModule._forward_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.input_specs_inference.html">ray.rllib.core.rl_module.rl_module.RLModule.input_specs_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.input_specs_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule.input_specs_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.input_specs_train.html">ray.rllib.core.rl_module.rl_module.RLModule.input_specs_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.output_specs_inference.html">ray.rllib.core.rl_module.rl_module.RLModule.output_specs_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.output_specs_exploration.html">ray.rllib.core.rl_module.rl_module.RLModule.output_specs_exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.output_specs_train.html">ray.rllib.core.rl_module.rl_module.RLModule.output_specs_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.get_state.html">ray.rllib.core.rl_module.rl_module.RLModule.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.set_state.html">ray.rllib.core.rl_module.rl_module.RLModule.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.save_to_path.html">ray.rllib.core.rl_module.rl_module.RLModule.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.restore_from_path.html">ray.rllib.core.rl_module.rl_module.RLModule.restore_from_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.rl_module.RLModule.from_checkpoint.html">ray.rllib.core.rl_module.rl_module.RLModule.from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.as_multi_rl_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.as_multi_rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.add_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.add_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.remove_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.save_to_path.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.save_to_path</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.restore_from_path.html">ray.rllib.core.rl_module.multi_rl_module.MultiRLModule.restore_from_path</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/learner.html">LearnerGroup API</a><input class="toctree-checkbox" id="toctree-checkbox-39" name="toctree-checkbox-39" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-39"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.resources.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.rl_module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.training.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group.html">ray.rllib.algorithms.algorithm_config.AlgorithmConfig.build_learner_group</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.learner.learner_group.LearnerGroup.html">ray.rllib.core.learner.learner_group.LearnerGroup</a></li>
</ul>
</li>

<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/evaluation.html">Sampling the Environment or offline data</a><input class="toctree-checkbox" id="toctree-checkbox-40" name="toctree-checkbox-40" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-40"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.html">ray.rllib.evaluation.rollout_worker.RolloutWorker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.add_policy.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.add_policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.remove_policy.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.remove_policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.get_policy.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.get_policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.set_is_policy_to_train.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.set_is_policy_to_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.set_policy_mapping_fn.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.set_policy_mapping_fn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.for_policy.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.for_policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.foreach_policy.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.foreach_policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.foreach_policy_to_train.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.foreach_policy_to_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.get_filters.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.get_filters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.get_global_vars.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.get_global_vars</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.set_global_vars.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.set_global_vars</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.get_host.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.get_host</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.get_metrics.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.get_metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.get_node_ip.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.get_node_ip</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.get_weights.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.get_weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.set_weights.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.set_weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.get_state.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.set_state.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.lock.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.lock</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.unlock.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.unlock</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.sample.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.sample_with_count.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.sample_with_count</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.sample_and_learn.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.sample_and_learn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.learn_on_batch.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.learn_on_batch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.setup_torch_data_parallel.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.setup_torch_data_parallel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.compute_gradients.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.compute_gradients</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.apply_gradients.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.apply_gradients</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.foreach_env.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.foreach_env</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.foreach_env_with_context.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.foreach_env_with_context</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.stop.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.stop</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.apply.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.apply</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.sync_filters.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.sync_filters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.find_free_port.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.find_free_port</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.creation_args.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.creation_args</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.rollout_worker.RolloutWorker.assert_healthy.html">ray.rllib.evaluation.rollout_worker.RolloutWorker.assert_healthy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner.EnvRunner.html">ray.rllib.env.env_runner.EnvRunner</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.html">ray.rllib.env.env_runner_group.EnvRunnerGroup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.stop.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.stop</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.reset.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.reset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.add_workers.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.add_workers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_worker.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_worker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_worker_with_id.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_worker_with_id</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_worker_async.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_worker_async</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.fetch_ready_async_reqs.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.fetch_ready_async_reqs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.num_in_flight_async_reqs.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.num_in_flight_async_reqs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.local_worker.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.local_worker</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.remote_workers.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.remote_workers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.num_healthy_remote_workers.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.num_healthy_remote_workers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.num_healthy_workers.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.num_healthy_workers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.num_remote_worker_restarts.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.num_remote_worker_restarts</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.probe_unhealthy_workers.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.probe_unhealthy_workers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.add_policy.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.add_policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_env.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_env</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_env_with_context.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_env_with_context</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_policy.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_policy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_policy_to_train.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.foreach_policy_to_train</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.env.env_runner_group.EnvRunnerGroup.sync_weights.html">ray.rllib.env.env_runner_group.EnvRunnerGroup.sync_weights</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.input_reader.InputReader.html">ray.rllib.offline.input_reader.InputReader</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.input_reader.InputReader.next.html">ray.rllib.offline.input_reader.InputReader.next</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.sampler.SamplerInput.html">ray.rllib.evaluation.sampler.SamplerInput</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.sampler.SamplerInput.get_data.html">ray.rllib.evaluation.sampler.SamplerInput.get_data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.sampler.SamplerInput.get_extra_batches.html">ray.rllib.evaluation.sampler.SamplerInput.get_extra_batches</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.sampler.SamplerInput.get_metrics.html">ray.rllib.evaluation.sampler.SamplerInput.get_metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.evaluation.sampler.SyncSampler.html">ray.rllib.evaluation.sampler.SyncSampler</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.json_reader.JsonReader.html">ray.rllib.offline.json_reader.JsonReader</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.json_reader.JsonReader.read_all_files.html">ray.rllib.offline.json_reader.JsonReader.read_all_files</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.mixed_input.MixedInput.html">ray.rllib.offline.mixed_input.MixedInput</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.d4rl_reader.D4RLReader.html">ray.rllib.offline.d4rl_reader.D4RLReader</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.io_context.IOContext.html">ray.rllib.offline.io_context.IOContext</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.offline.io_context.IOContext.default_sampler_input.html">ray.rllib.offline.io_context.IOContext.default_sampler_input</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.policy_map.PolicyMap.html">ray.rllib.policy.policy_map.PolicyMap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.policy_map.PolicyMap.items.html">ray.rllib.policy.policy_map.PolicyMap.items</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.policy_map.PolicyMap.keys.html">ray.rllib.policy.policy_map.PolicyMap.keys</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.policy_map.PolicyMap.values.html">ray.rllib.policy.policy_map.PolicyMap.values</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.html">ray.rllib.policy.sample_batch.SampleBatch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.set_get_interceptor.html">ray.rllib.policy.sample_batch.SampleBatch.set_get_interceptor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.is_training.html">ray.rllib.policy.sample_batch.SampleBatch.is_training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.set_training.html">ray.rllib.policy.sample_batch.SampleBatch.set_training</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.as_multi_agent.html">ray.rllib.policy.sample_batch.SampleBatch.as_multi_agent</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.get.html">ray.rllib.policy.sample_batch.SampleBatch.get</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.to_device.html">ray.rllib.policy.sample_batch.SampleBatch.to_device</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.right_zero_pad.html">ray.rllib.policy.sample_batch.SampleBatch.right_zero_pad</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.slice.html">ray.rllib.policy.sample_batch.SampleBatch.slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.split_by_episode.html">ray.rllib.policy.sample_batch.SampleBatch.split_by_episode</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.shuffle.html">ray.rllib.policy.sample_batch.SampleBatch.shuffle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.columns.html">ray.rllib.policy.sample_batch.SampleBatch.columns</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.rows.html">ray.rllib.policy.sample_batch.SampleBatch.rows</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.copy.html">ray.rllib.policy.sample_batch.SampleBatch.copy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.is_single_trajectory.html">ray.rllib.policy.sample_batch.SampleBatch.is_single_trajectory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.is_terminated_or_truncated.html">ray.rllib.policy.sample_batch.SampleBatch.is_terminated_or_truncated</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.env_steps.html">ray.rllib.policy.sample_batch.SampleBatch.env_steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.SampleBatch.agent_steps.html">ray.rllib.policy.sample_batch.SampleBatch.agent_steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.MultiAgentBatch.html">ray.rllib.policy.sample_batch.MultiAgentBatch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.MultiAgentBatch.env_steps.html">ray.rllib.policy.sample_batch.MultiAgentBatch.env_steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.policy.sample_batch.MultiAgentBatch.agent_steps.html">ray.rllib.policy.sample_batch.MultiAgentBatch.agent_steps</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/replay-buffers.html">Replay Buffer API</a><input class="toctree-checkbox" id="toctree-checkbox-41" name="toctree-checkbox-41" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-41"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.StorageUnit.html">ray.rllib.utils.replay_buffers.replay_buffer.StorageUnit</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.prioritized_replay_buffer.PrioritizedReplayBuffer.html">ray.rllib.utils.replay_buffers.prioritized_replay_buffer.PrioritizedReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer.html">ray.rllib.utils.replay_buffers.reservoir_replay_buffer.ReservoirReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.sample.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.sample</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.add.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.add</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.get_state.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state.html">ray.rllib.utils.replay_buffers.replay_buffer.ReplayBuffer.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer.html">ray.rllib.utils.replay_buffers.multi_agent_replay_buffer.MultiAgentReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer.html">ray.rllib.utils.replay_buffers.multi_agent_prioritized_replay_buffer.MultiAgentPrioritizedReplayBuffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.utils.update_priorities_in_replay_buffer.html">ray.rllib.utils.replay_buffers.utils.update_priorities_in_replay_buffer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer.html">ray.rllib.utils.replay_buffers.utils.sample_min_n_steps_from_buffer</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../rllib/package_ref/utils.html">RLlib Utilities</a><input class="toctree-checkbox" id="toctree-checkbox-42" name="toctree-checkbox-42" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-42"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.exploration.Exploration.html">ray.rllib.utils.exploration.exploration.Exploration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.random.Random.html">ray.rllib.utils.exploration.random.Random</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.stochastic_sampling.StochasticSampling.html">ray.rllib.utils.exploration.stochastic_sampling.StochasticSampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.epsilon_greedy.EpsilonGreedy.html">ray.rllib.utils.exploration.epsilon_greedy.EpsilonGreedy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.gaussian_noise.GaussianNoise.html">ray.rllib.utils.exploration.gaussian_noise.GaussianNoise</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.ornstein_uhlenbeck_noise.OrnsteinUhlenbeckNoise.html">ray.rllib.utils.exploration.ornstein_uhlenbeck_noise.OrnsteinUhlenbeckNoise</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.random_encoder.RE3.html">ray.rllib.utils.exploration.random_encoder.RE3</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.curiosity.Curiosity.html">ray.rllib.utils.exploration.curiosity.Curiosity</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.parameter_noise.ParameterNoise.html">ray.rllib.utils.exploration.parameter_noise.ParameterNoise</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.exploration.Exploration.get_exploration_action.html">ray.rllib.utils.exploration.exploration.Exploration.get_exploration_action</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.exploration.Exploration.before_compute_actions.html">ray.rllib.utils.exploration.exploration.Exploration.before_compute_actions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.exploration.Exploration.on_episode_start.html">ray.rllib.utils.exploration.exploration.Exploration.on_episode_start</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.exploration.Exploration.on_episode_end.html">ray.rllib.utils.exploration.exploration.Exploration.on_episode_end</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.exploration.Exploration.postprocess_trajectory.html">ray.rllib.utils.exploration.exploration.Exploration.postprocess_trajectory</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.exploration.Exploration.get_state.html">ray.rllib.utils.exploration.exploration.Exploration.get_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.exploration.exploration.Exploration.set_state.html">ray.rllib.utils.exploration.exploration.Exploration.set_state</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.schedule.Schedule.html">ray.rllib.utils.schedules.schedule.Schedule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.constant_schedule.ConstantSchedule.html">ray.rllib.utils.schedules.constant_schedule.ConstantSchedule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.linear_schedule.LinearSchedule.html">ray.rllib.utils.schedules.linear_schedule.LinearSchedule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.piecewise_schedule.PiecewiseSchedule.html">ray.rllib.utils.schedules.piecewise_schedule.PiecewiseSchedule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.exponential_schedule.ExponentialSchedule.html">ray.rllib.utils.schedules.exponential_schedule.ExponentialSchedule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.polynomial_schedule.PolynomialSchedule.html">ray.rllib.utils.schedules.polynomial_schedule.PolynomialSchedule</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.schedule.Schedule.value.html">ray.rllib.utils.schedules.schedule.Schedule.value</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.schedules.schedule.Schedule.__call__.html">ray.rllib.utils.schedules.schedule.Schedule.__call__</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.execution.train_ops.multi_gpu_train_one_step.html">ray.rllib.execution.train_ops.multi_gpu_train_one_step</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.execution.train_ops.train_one_step.html">ray.rllib.execution.train_ops.train_one_step</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.framework.try_import_torch.html">ray.rllib.utils.framework.try_import_torch</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.framework.try_import_tf.html">ray.rllib.utils.framework.try_import_tf</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.framework.try_import_tfp.html">ray.rllib.utils.framework.try_import_tfp</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.explained_variance.html">ray.rllib.utils.tf_utils.explained_variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.flatten_inputs_to_1d_tensor.html">ray.rllib.utils.tf_utils.flatten_inputs_to_1d_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.get_gpu_devices.html">ray.rllib.utils.tf_utils.get_gpu_devices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.get_placeholder.html">ray.rllib.utils.tf_utils.get_placeholder</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.huber_loss.html">ray.rllib.utils.tf_utils.huber_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.l2_loss.html">ray.rllib.utils.tf_utils.l2_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.make_tf_callable.html">ray.rllib.utils.tf_utils.make_tf_callable</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.minimize_and_clip.html">ray.rllib.utils.tf_utils.minimize_and_clip</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.one_hot.html">ray.rllib.utils.tf_utils.one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.reduce_mean_ignore_inf.html">ray.rllib.utils.tf_utils.reduce_mean_ignore_inf</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.scope_vars.html">ray.rllib.utils.tf_utils.scope_vars</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.warn_if_infinite_kl_divergence.html">ray.rllib.utils.tf_utils.warn_if_infinite_kl_divergence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tf_utils.zero_logps_from_actions.html">ray.rllib.utils.tf_utils.zero_logps_from_actions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.apply_grad_clipping.html">ray.rllib.utils.torch_utils.apply_grad_clipping</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.concat_multi_gpu_td_errors.html">ray.rllib.utils.torch_utils.concat_multi_gpu_td_errors</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.convert_to_torch_tensor.html">ray.rllib.utils.torch_utils.convert_to_torch_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.explained_variance.html">ray.rllib.utils.torch_utils.explained_variance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor.html">ray.rllib.utils.torch_utils.flatten_inputs_to_1d_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.global_norm.html">ray.rllib.utils.torch_utils.global_norm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.huber_loss.html">ray.rllib.utils.torch_utils.huber_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.l2_loss.html">ray.rllib.utils.torch_utils.l2_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.minimize_and_clip.html">ray.rllib.utils.torch_utils.minimize_and_clip</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.one_hot.html">ray.rllib.utils.torch_utils.one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.reduce_mean_ignore_inf.html">ray.rllib.utils.torch_utils.reduce_mean_ignore_inf</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.sequence_mask.html">ray.rllib.utils.torch_utils.sequence_mask</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.warn_if_infinite_kl_divergence.html">ray.rllib.utils.torch_utils.warn_if_infinite_kl_divergence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.set_torch_seed.html">ray.rllib.utils.torch_utils.set_torch_seed</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits.html">ray.rllib.utils.torch_utils.softmax_cross_entropy_with_logits</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.aligned_array.html">ray.rllib.utils.numpy.aligned_array</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.concat_aligned.html">ray.rllib.utils.numpy.concat_aligned</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.convert_to_numpy.html">ray.rllib.utils.numpy.convert_to_numpy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.fc.html">ray.rllib.utils.numpy.fc</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.flatten_inputs_to_1d_tensor.html">ray.rllib.utils.numpy.flatten_inputs_to_1d_tensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.make_action_immutable.html">ray.rllib.utils.numpy.make_action_immutable</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.huber_loss.html">ray.rllib.utils.numpy.huber_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.l2_loss.html">ray.rllib.utils.numpy.l2_loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.lstm.html">ray.rllib.utils.numpy.lstm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.one_hot.html">ray.rllib.utils.numpy.one_hot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.relu.html">ray.rllib.utils.numpy.relu</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.sigmoid.html">ray.rllib.utils.numpy.sigmoid</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.numpy.softmax.html">ray.rllib.utils.numpy.softmax</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.checkpoints.Checkpointable.html">ray.rllib.utils.checkpoints.Checkpointable</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.checkpoints.convert_to_msgpack_checkpoint.html">ray.rllib.utils.checkpoints.convert_to_msgpack_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.checkpoints.convert_to_msgpack_policy_checkpoint.html">ray.rllib.utils.checkpoints.convert_to_msgpack_policy_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.checkpoints.get_checkpoint_info.html">ray.rllib.utils.checkpoints.get_checkpoint_info</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.checkpoints.try_import_msgpack.html">ray.rllib.utils.checkpoints.try_import_msgpack</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.policy.compute_log_likelihoods_from_input_dict.html">ray.rllib.utils.policy.compute_log_likelihoods_from_input_dict</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.policy.create_policy_for_framework.html">ray.rllib.utils.policy.create_policy_for_framework</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.policy.local_policy_inference.html">ray.rllib.utils.policy.local_policy_inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.policy.parse_policy_specs_from_checkpoint.html">ray.rllib.utils.policy.parse_policy_specs_from_checkpoint</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.utils.tensor_dtype.get_np_dtype.html">ray.rllib.utils.tensor_dtype.get_np_dtype</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.common.CLIArguments.html">ray.rllib.common.CLIArguments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.common.FrameworkEnum.html">ray.rllib.common.FrameworkEnum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.common.SupportedFileType.html">ray.rllib.common.SupportedFileType</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.core.rl_module.validate_module_id.html">ray.rllib.core.rl_module.validate_module_id</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../rllib/package_ref/doc/ray.rllib.train.load_experiments_from_file.html">ray.rllib.train.load_experiments_from_file</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../rllib/package_ref/external-app.html">External Application API</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-more-libs/index.html">More Libraries</a><input class="toctree-checkbox" id="toctree-checkbox-43" name="toctree-checkbox-43" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-43"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../joblib.html">Distributed Scikit-learn / Joblib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/multiprocessing.html">Distributed multiprocessing.Pool</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/ray-collective.html">Ray Collective Communication Lib</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-more-libs/dask-on-ray.html">Using Dask on Ray</a><input class="toctree-checkbox" id="toctree-checkbox-44" name="toctree-checkbox-44" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-44"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.RayDaskCallback.html">ray.util.dask.RayDaskCallback</a><input class="toctree-checkbox" id="toctree-checkbox-45" name="toctree-checkbox-45" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-45"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.RayDaskCallback.ray_active.html">ray.util.dask.RayDaskCallback.ray_active</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_presubmit.html">ray.util.dask.callbacks.RayDaskCallback._ray_presubmit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit.html">ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_pretask.html">ray.util.dask.callbacks.RayDaskCallback._ray_pretask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_posttask.html">ray.util.dask.callbacks.RayDaskCallback._ray_posttask</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit_all.html">ray.util.dask.callbacks.RayDaskCallback._ray_postsubmit_all</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-more-libs/doc/ray.util.dask.callbacks.RayDaskCallback._ray_finish.html">ray.util.dask.callbacks.RayDaskCallback._ray_finish</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/raydp.html">Using Spark on Ray (RayDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/mars-on-ray.html">Using Mars on Ray</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-more-libs/modin/index.html">Using Pandas on Ray (Modin)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../workflows/index.html">Ray Workflows (Alpha)</a><input class="toctree-checkbox" id="toctree-checkbox-46" name="toctree-checkbox-46" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-46"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/basics.html">Getting Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/management.html">Workflow Management</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/metadata.html">Workflow Metadata</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/events.html">Events</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/comparison.html">API Comparisons</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../workflows/advanced.html">Advanced Topics</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../workflows/api/api.html">Ray Workflows API</a><input class="toctree-checkbox" id="toctree-checkbox-47" name="toctree-checkbox-47" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-47"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../workflows/api/execution.html">Workflow Execution API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../workflows/api/management.html">Workflow Management API</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-more-libs/cluster/index.html">Ray Clusters</a><input class="toctree-checkbox" id="toctree-checkbox-48" name="toctree-checkbox-48" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-48"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/index.html">Deploying on Kubernetes</a><input class="toctree-checkbox" id="toctree-checkbox-49" name="toctree-checkbox-49" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-49"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started.html">Getting Started with KubeRay</a><input class="toctree-checkbox" id="toctree-checkbox-50" name="toctree-checkbox-50" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-50"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started/raycluster-quick-start.html">RayCluster Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started/rayjob-quick-start.html">RayJob Quickstart</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/getting-started/rayservice-quick-start.html">RayService Quickstart</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-51" name="toctree-checkbox-51" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-51"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/rayservice.html">Deploy Ray Serve Apps</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/rayservice-high-availability.html">RayService high availability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/observability.html">KubeRay Observability</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/upgrade-guide.html">KubeRay upgrade guide</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/k8s-cluster-setup.html">Managed Kubernetes services</a></li>



<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/storage.html">Best Practices for Storage and Dependencies</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/config.html">RayCluster Configuration</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/configuring-autoscaling.html">KubeRay Autoscaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/kuberay-gcs-ft.html">GCS fault tolerance in KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/gke-gcs-bucket.html">Configuring KubeRay to use Google Cloud Storage Buckets in GKE</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/logging.html">Log Persistence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/gpu.html">Using GPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/tpu.html">Use TPUs with KubeRay</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/rayserve-dev-doc.html">Developing Ray Serve Python scripts on a RayCluster</a></li>









<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/pod-command.html">Specify container commands for Ray head/worker Pods</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/pod-security.html">Pod Security</a></li>






<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/helm-chart-rbac.html">Helm Chart RBAC</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/tls.html">TLS Authentication</a></li>






<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/k8s-autoscaler.html">(Advanced) Understanding the Ray Autoscaler in the Context of Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.html">(Advanced) Deploying a static Ray cluster without KubeRay</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/examples.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-52" name="toctree-checkbox-52" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-52"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/ml-example.html">Ray Train XGBoostTrainer on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/gpu-training-example.html">Train PyTorch ResNet model with GPUs on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/mnist-training-example.html">Train a PyTorch model on Fashion MNIST with CPUs on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/stable-diffusion-rayservice.html">Serve a StableDiffusion text-to-image model on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/tpu-serve-stable-diffusion.html">Serve a Stable Diffusion model on GKE with TPUs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/mobilenet-rayservice.html">Serve a MobileNet image classifier on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/text-summarizer-rayservice.html">Serve a text summarizer on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/rayjob-batch-inference-example.html">RayJob Batch Inference Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/rayjob-kueue-priority-scheduling.html">Priority Scheduling with RayJob and Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/rayjob-kueue-gang-scheduling.html">Gang Scheduling with RayJob and Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/distributed-checkpointing-with-gcsfuse.html">Distributed checkpointing with KubeRay and GCSFuse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/modin-example.html">Use Modin with Ray on Kubernetes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/examples/vllm-rayservice.html">Serve a Large Language Model with vLLM on Kubernetes</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem.html">KubeRay Ecosystem</a><input class="toctree-checkbox" id="toctree-checkbox-53" name="toctree-checkbox-53" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-53"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/ingress.html">Ingress</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/prometheus-grafana.html">Using Prometheus and Grafana</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/pyspy.html">Profiling with py-spy</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/volcano.html">KubeRay integration with Volcano</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/kubeflow.html">Kubeflow: an interactive development solution</a></li>


<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/kueue.html">Gang scheduling and priority scheduling for RayJob with Kueue</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/k8s-ecosystem/istio.html">mTLS and L7 observability with Istio</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/benchmarks.html">KubeRay Benchmarks</a><input class="toctree-checkbox" id="toctree-checkbox-54" name="toctree-checkbox-54" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-54"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/benchmarks/memory-scalability-benchmark.html">KubeRay memory and scalability benchmark</a></li>

</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/kubernetes/troubleshooting.html">KubeRay Troubleshooting</a><input class="toctree-checkbox" id="toctree-checkbox-55" name="toctree-checkbox-55" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-55"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/troubleshooting/troubleshooting.html">Troubleshooting guide</a></li>

<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/kubernetes/troubleshooting/rayservice-troubleshooting.html">RayService troubleshooting</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/kubernetes/references.html">API Reference</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/vms/index.html">Deploying on VMs</a><input class="toctree-checkbox" id="toctree-checkbox-56" name="toctree-checkbox-56" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-56"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/vms/getting-started.html">Getting Started</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/vms/user-guides/index.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-57" name="toctree-checkbox-57" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-57"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/launching-clusters/index.html">Launching Ray Clusters on AWS, GCP, Azure, vSphere, On-Prem</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/large-cluster-best-practices.html">Best practices for deploying large clusters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/configuring-autoscaling.html">Configuring Autoscaling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/logging.html">Log Persistence</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/user-guides/community/index.html">Community Supported Cluster Managers</a></li>

</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/vms/examples/index.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-58" name="toctree-checkbox-58" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-58"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/examples/ml-example.html">Ray Train XGBoostTrainer on VMs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/vms/references/index.html">API References</a><input class="toctree-checkbox" id="toctree-checkbox-59" name="toctree-checkbox-59" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-59"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/references/ray-cluster-cli.html">Cluster Launcher Commands</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/vms/references/ray-cluster-configuration.html">Cluster YAML Configuration Options</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/metrics.html">Collecting and monitoring metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/configure-manage-dashboard.html">Configuring and Managing Ray Dashboard</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/running-applications/index.html">Applications Guide</a><input class="toctree-checkbox" id="toctree-checkbox-60" name="toctree-checkbox-60" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-60"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/index.html">Ray Jobs Overview</a><input class="toctree-checkbox" id="toctree-checkbox-61" name="toctree-checkbox-61" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-61"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/quickstart.html">Quickstart using the Ray Jobs CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/sdk.html">Python SDK Overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/jobs-package-ref.html">Python SDK API Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/cli.html">Ray Jobs CLI API Reference</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/rest.html">Ray Jobs REST API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/ray-client.html">Ray Client</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/autoscaling/reference.html">Programmatic Cluster Scaling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/faq.html">FAQ</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../cluster/package-overview.html">Ray Cluster Management API</a><input class="toctree-checkbox" id="toctree-checkbox-62" name="toctree-checkbox-62" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-62"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/cli.html">Cluster Management CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/jobs-package-ref.html">Python SDK API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/job-submission/cli.html">Ray Jobs CLI API Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../cluster/running-applications/autoscaling/reference.html">Programmatic Cluster Scaling</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../cluster/usage-stats.html">Usage Stats Collection</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-observability/index.html">Monitoring and Debugging</a><input class="toctree-checkbox" id="toctree-checkbox-63" name="toctree-checkbox-63" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-63"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-observability/getting-started.html">Ray Dashboard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-observability/ray-distributed-debugger.html">Ray Distributed Debugger</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../../../ray-observability/key-concepts.html">Key Concepts</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-observability/user-guides/index.html">User Guides</a><input class="toctree-checkbox" id="toctree-checkbox-64" name="toctree-checkbox-64" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-64"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/index.html">Debugging Applications</a><input class="toctree-checkbox" id="toctree-checkbox-65" name="toctree-checkbox-65" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-65"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/general-debugging.html">General Debugging</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/debug-memory.html">Debugging Memory Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/debug-hangs.html">Debugging Hangs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/debug-failures.html">Debugging Failures</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/optimize-performance.html">Optimizing Performance</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../ray-observability/user-guides/debug-apps/ray-debugging.html">Using the Ray Debugger</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/cli-sdk.html">Monitoring with the CLI or SDK</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/configure-logging.html">Configuring Logging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/profiling.html">Profiling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/add-app-metrics.html">Adding Application-Level Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/user-guides/ray-tracing.html">Tracing</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-observability/reference/index.html">Reference</a><input class="toctree-checkbox" id="toctree-checkbox-66" name="toctree-checkbox-66" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-66"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/api.html">State API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/cli.html">State CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-observability/reference/system-metrics.html">System Metrics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../ray-contribute/index.html">Developer Guides</a><input class="toctree-checkbox" id="toctree-checkbox-67" name="toctree-checkbox-67" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-67"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-contribute/stability.html">API Stability</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-contribute/api-policy.html">API Policy</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../ray-contribute/getting-involved.html">Getting Involved / Contributing</a><input class="toctree-checkbox" id="toctree-checkbox-68" name="toctree-checkbox-68" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-68"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/development.html">Building Ray from Source</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/ci.html">CI Testing Workflow on PRs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/docs.html">Contributing to the Ray Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/writing-code-snippets.html">How to write code snippets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/fake-autoscaler.html">Testing Autoscaling Locally</a></li>

<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/testing-tips.html">Tips for testing Ray programs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/debugging.html">Debugging for Ray Developers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../ray-contribute/profiling.html">Profiling for Ray Developers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-core/configure.html">Configuring Ray</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../ray-contribute/whitepaper.html">Architecture Whitepapers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-references/glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../ray-security/index.html">Security</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">ray.rllib.al...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <h1>Source code for ray.rllib.algorithms.algorithm</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">concurrent</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">importlib</span>
<span class="kn">import</span> <span class="nn">importlib.metadata</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">packaging</span> <span class="kn">import</span> <span class="n">version</span>
<span class="kn">import</span> <span class="nn">pathlib</span>
<span class="kn">import</span> <span class="nn">pyarrow.fs</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">Callable</span><span class="p">,</span>
    <span class="n">Collection</span><span class="p">,</span>
    <span class="n">DefaultDict</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Set</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">TYPE_CHECKING</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">import</span> <span class="nn">tree</span>  <span class="c1"># pip install dm_tree</span>

<span class="kn">import</span> <span class="nn">ray</span>
<span class="kn">from</span> <span class="nn">ray.air.constants</span> <span class="kn">import</span> <span class="n">TRAINING_ITERATION</span>
<span class="kn">from</span> <span class="nn">ray._private.usage.usage_lib</span> <span class="kn">import</span> <span class="n">TagKey</span><span class="p">,</span> <span class="n">record_extra_usage_tag</span>
<span class="kn">from</span> <span class="nn">ray.actor</span> <span class="kn">import</span> <span class="n">ActorHandle</span>
<span class="kn">from</span> <span class="nn">ray.train</span> <span class="kn">import</span> <span class="n">Checkpoint</span>
<span class="kn">import</span> <span class="nn">ray.cloudpickle</span> <span class="k">as</span> <span class="nn">pickle</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.algorithm_config</span> <span class="kn">import</span> <span class="n">AlgorithmConfig</span>
<span class="kn">from</span> <span class="nn">ray.rllib.algorithms.registry</span> <span class="kn">import</span> <span class="n">ALGORITHMS_CLASS_TO_NAME</span> <span class="k">as</span> <span class="n">ALL_ALGORITHMS</span>
<span class="kn">from</span> <span class="nn">ray.rllib.connectors.agent.obs_preproc</span> <span class="kn">import</span> <span class="n">ObsPreprocessorConnector</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">COMPONENT_ENV_RUNNER</span><span class="p">,</span>
    <span class="n">COMPONENT_EVAL_ENV_RUNNER</span><span class="p">,</span>
    <span class="n">COMPONENT_LEARNER</span><span class="p">,</span>
    <span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">,</span>
    <span class="n">COMPONENT_METRICS_LOGGER</span><span class="p">,</span>
    <span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span>
    <span class="n">DEFAULT_MODULE_ID</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.columns</span> <span class="kn">import</span> <span class="n">Columns</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.multi_rl_module</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">MultiRLModule</span><span class="p">,</span>
    <span class="n">MultiRLModuleSpec</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module</span> <span class="kn">import</span> <span class="n">validate_module_id</span>
<span class="kn">from</span> <span class="nn">ray.rllib.core.rl_module.rl_module</span> <span class="kn">import</span> <span class="n">RLModule</span><span class="p">,</span> <span class="n">RLModuleSpec</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.env_context</span> <span class="kn">import</span> <span class="n">EnvContext</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.env_runner</span> <span class="kn">import</span> <span class="n">EnvRunner</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.env_runner_group</span> <span class="kn">import</span> <span class="n">EnvRunnerGroup</span>
<span class="kn">from</span> <span class="nn">ray.rllib.env.utils</span> <span class="kn">import</span> <span class="n">_gym_env_creator</span>
<span class="kn">from</span> <span class="nn">ray.rllib.evaluation.episode</span> <span class="kn">import</span> <span class="n">Episode</span>
<span class="kn">from</span> <span class="nn">ray.rllib.evaluation.metrics</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">collect_episodes</span><span class="p">,</span>
    <span class="n">summarize_episodes</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.execution.rollout_ops</span> <span class="kn">import</span> <span class="n">synchronous_parallel_sample</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline</span> <span class="kn">import</span> <span class="n">get_dataset_and_shards</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.estimators</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">OffPolicyEstimator</span><span class="p">,</span>
    <span class="n">ImportanceSampling</span><span class="p">,</span>
    <span class="n">WeightedImportanceSampling</span><span class="p">,</span>
    <span class="n">DirectMethod</span><span class="p">,</span>
    <span class="n">DoublyRobust</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.offline.offline_evaluator</span> <span class="kn">import</span> <span class="n">OfflineEvaluator</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.policy</span> <span class="kn">import</span> <span class="n">Policy</span><span class="p">,</span> <span class="n">PolicySpec</span>
<span class="kn">from</span> <span class="nn">ray.rllib.policy.sample_batch</span> <span class="kn">import</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span> <span class="n">SampleBatch</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils</span> <span class="kn">import</span> <span class="n">deep_update</span><span class="p">,</span> <span class="n">FilterManager</span><span class="p">,</span> <span class="n">force_list</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.annotations</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DeveloperAPI</span><span class="p">,</span>
    <span class="n">ExperimentalAPI</span><span class="p">,</span>
    <span class="n">OldAPIStack</span><span class="p">,</span>
    <span class="n">override</span><span class="p">,</span>
    <span class="n">OverrideToImplementCustomLogic</span><span class="p">,</span>
    <span class="n">OverrideToImplementCustomLogic_CallToSuperRecommended</span><span class="p">,</span>
    <span class="n">PublicAPI</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.checkpoints</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Checkpointable</span><span class="p">,</span>
    <span class="n">CHECKPOINT_VERSION</span><span class="p">,</span>
    <span class="n">CHECKPOINT_VERSION_LEARNER</span><span class="p">,</span>
    <span class="n">get_checkpoint_info</span><span class="p">,</span>
    <span class="n">try_import_msgpack</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.debug</span> <span class="kn">import</span> <span class="n">update_global_seed_if_necessary</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.deprecation</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
    <span class="n">Deprecated</span><span class="p">,</span>
    <span class="n">deprecation_warning</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.error</span> <span class="kn">import</span> <span class="n">ERR_MSG_INVALID_ENV_DESCRIPTOR</span><span class="p">,</span> <span class="n">EnvError</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.framework</span> <span class="kn">import</span> <span class="n">try_import_tf</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.from_config</span> <span class="kn">import</span> <span class="n">from_config</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.metrics</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">ALL_MODULES</span><span class="p">,</span>
    <span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span>
    <span class="n">ENV_RUNNER_SAMPLING_TIMER</span><span class="p">,</span>
    <span class="n">EPISODE_LEN_MEAN</span><span class="p">,</span>
    <span class="n">EPISODE_RETURN_MEAN</span><span class="p">,</span>
    <span class="n">EVALUATION_ITERATION_TIMER</span><span class="p">,</span>
    <span class="n">EVALUATION_RESULTS</span><span class="p">,</span>
    <span class="n">FAULT_TOLERANCE_STATS</span><span class="p">,</span>
    <span class="n">LEARNER_RESULTS</span><span class="p">,</span>
    <span class="n">LEARNER_UPDATE_TIMER</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_SAMPLED_LIFETIME</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_SAMPLED_THIS_ITER</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">,</span>
    <span class="n">NUM_AGENT_STEPS_TRAINED_LIFETIME</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_SAMPLED_THIS_ITER</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_SAMPLED_FOR_EVALUATION_THIS_ITER</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">,</span>
    <span class="n">NUM_ENV_STEPS_TRAINED_LIFETIME</span><span class="p">,</span>
    <span class="n">NUM_EPISODES</span><span class="p">,</span>
    <span class="n">NUM_EPISODES_LIFETIME</span><span class="p">,</span>
    <span class="n">RESTORE_WORKERS_TIMER</span><span class="p">,</span>
    <span class="n">RESTORE_EVAL_WORKERS_TIMER</span><span class="p">,</span>
    <span class="n">SYNCH_ENV_CONNECTOR_STATES_TIMER</span><span class="p">,</span>
    <span class="n">SYNCH_EVAL_ENV_CONNECTOR_STATES_TIMER</span><span class="p">,</span>
    <span class="n">SYNCH_WORKER_WEIGHTS_TIMER</span><span class="p">,</span>
    <span class="n">TIMERS</span><span class="p">,</span>
    <span class="n">TRAINING_ITERATION_TIMER</span><span class="p">,</span>
    <span class="n">TRAINING_STEP_TIMER</span><span class="p">,</span>
    <span class="n">STEPS_TRAINED_THIS_ITER_COUNTER</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.metrics.learner_info</span> <span class="kn">import</span> <span class="n">LEARNER_INFO</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.metrics.metrics_logger</span> <span class="kn">import</span> <span class="n">MetricsLogger</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.metrics.stats</span> <span class="kn">import</span> <span class="n">Stats</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.replay_buffers</span> <span class="kn">import</span> <span class="n">MultiAgentReplayBuffer</span><span class="p">,</span> <span class="n">ReplayBuffer</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.serialization</span> <span class="kn">import</span> <span class="n">deserialize_type</span><span class="p">,</span> <span class="n">NOT_SERIALIZABLE</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.spaces</span> <span class="kn">import</span> <span class="n">space_utils</span>
<span class="kn">from</span> <span class="nn">ray.rllib.utils.typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AgentConnectorDataType</span><span class="p">,</span>
    <span class="n">AgentID</span><span class="p">,</span>
    <span class="n">AgentToModuleMappingFn</span><span class="p">,</span>
    <span class="n">AlgorithmConfigDict</span><span class="p">,</span>
    <span class="n">EnvCreator</span><span class="p">,</span>
    <span class="n">EnvInfoDict</span><span class="p">,</span>
    <span class="n">EnvType</span><span class="p">,</span>
    <span class="n">EpisodeID</span><span class="p">,</span>
    <span class="n">ModuleID</span><span class="p">,</span>
    <span class="n">PartialAlgorithmConfigDict</span><span class="p">,</span>
    <span class="n">PolicyID</span><span class="p">,</span>
    <span class="n">PolicyState</span><span class="p">,</span>
    <span class="n">ResultDict</span><span class="p">,</span>
    <span class="n">SampleBatchType</span><span class="p">,</span>
    <span class="n">ShouldModuleBeUpdatedFn</span><span class="p">,</span>
    <span class="n">StateDict</span><span class="p">,</span>
    <span class="n">TensorStructType</span><span class="p">,</span>
    <span class="n">TensorType</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">ray.train.constants</span> <span class="kn">import</span> <span class="n">DEFAULT_STORAGE_PATH</span>
<span class="kn">from</span> <span class="nn">ray.tune.execution.placement_groups</span> <span class="kn">import</span> <span class="n">PlacementGroupFactory</span>
<span class="kn">from</span> <span class="nn">ray.tune.experiment.trial</span> <span class="kn">import</span> <span class="n">ExportFormat</span>
<span class="kn">from</span> <span class="nn">ray.tune.logger</span> <span class="kn">import</span> <span class="n">Logger</span><span class="p">,</span> <span class="n">UnifiedLogger</span>
<span class="kn">from</span> <span class="nn">ray.tune.registry</span> <span class="kn">import</span> <span class="n">ENV_CREATOR</span><span class="p">,</span> <span class="n">_global_registry</span>
<span class="kn">from</span> <span class="nn">ray.tune.resources</span> <span class="kn">import</span> <span class="n">Resources</span>
<span class="kn">from</span> <span class="nn">ray.tune.trainable</span> <span class="kn">import</span> <span class="n">Trainable</span>
<span class="kn">from</span> <span class="nn">ray.util</span> <span class="kn">import</span> <span class="n">log_once</span>
<span class="kn">from</span> <span class="nn">ray.util.timer</span> <span class="kn">import</span> <span class="n">_Timer</span>
<span class="kn">from</span> <span class="nn">ray.tune.registry</span> <span class="kn">import</span> <span class="n">get_trainable_cls</span>

<span class="k">if</span> <span class="n">TYPE_CHECKING</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.core.learner.learner_group</span> <span class="kn">import</span> <span class="n">LearnerGroup</span>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">ray.rllib.extensions</span> <span class="kn">import</span> <span class="n">AlgorithmBase</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>

    <span class="k">class</span> <span class="nc">AlgorithmBase</span><span class="p">:</span>
        <span class="nd">@staticmethod</span>
        <span class="k">def</span> <span class="nf">_get_learner_bundles</span><span class="p">(</span><span class="n">cf</span><span class="p">:</span> <span class="n">AlgorithmConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;Selects the right resource bundles for learner workers based off of cf.</span>

<span class="sd">            Args:</span>
<span class="sd">                cf: The AlgorithmConfig instance to extract bundle-information from.</span>

<span class="sd">            Returns:</span>
<span class="sd">                A list of resource bundles for the learner workers.</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="k">if</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_learners</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_gpus_per_learner</span><span class="p">:</span>
                    <span class="n">learner_bundles</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="p">{</span><span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_learners</span> <span class="o">*</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_gpus_per_learner</span><span class="p">}</span>
                    <span class="p">]</span>
                <span class="k">elif</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_per_learner</span><span class="p">:</span>
                    <span class="n">learner_bundles</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="p">{</span>
                            <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_learners</span> <span class="o">*</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_per_learner</span><span class="p">,</span>
                        <span class="p">}</span>
                    <span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">learner_bundles</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="p">{</span>
                        <span class="c1"># sampling and training is not done concurrently when local is</span>
                        <span class="c1"># used, so pick the max.</span>
                        <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="nb">max</span><span class="p">(</span>
                            <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_per_learner</span><span class="p">,</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_for_main_process</span>
                        <span class="p">),</span>
                        <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_gpus_per_learner</span><span class="p">,</span>
                    <span class="p">}</span>
                <span class="p">]</span>
            <span class="k">return</span> <span class="n">learner_bundles</span>


<span class="n">tf1</span><span class="p">,</span> <span class="n">tf</span><span class="p">,</span> <span class="n">tfv</span> <span class="o">=</span> <span class="n">try_import_tf</span><span class="p">()</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<div class="viewcode-block" id="Algorithm">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.html#ray.rllib.algorithms.algorithm.Algorithm">[docs]</a>
<span class="nd">@PublicAPI</span>
<span class="k">class</span> <span class="nc">Algorithm</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">,</span> <span class="n">Trainable</span><span class="p">,</span> <span class="n">AlgorithmBase</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An RLlib algorithm responsible for optimizing one or more Policies.</span>

<span class="sd">    Algorithms contain a EnvRunnerGroup under `self.env_runner_group`. An EnvRunnerGroup</span>
<span class="sd">    is composed of a single local EnvRunner (`self.env_runner_group.local_env_runner`),</span>
<span class="sd">    serving as the reference copy of the NeuralNetwork(s) to be trained and optionally</span>
<span class="sd">    one or more remote EnvRunners used to generate environment samples in parallel.</span>
<span class="sd">    EnvRunnerGroup is fault-tolerant and elastic. It tracks health states for all</span>
<span class="sd">    the managed remote EnvRunner actors. As a result, Algorithm should never</span>
<span class="sd">    access the underlying actor handles directly. Instead, always access them</span>
<span class="sd">    via all the foreach APIs with assigned IDs of the underlying EnvRunners.</span>

<span class="sd">    Each EnvRunners (remotes or local) contains a PolicyMap, which itself</span>
<span class="sd">    may contain either one policy for single-agent training or one or more</span>
<span class="sd">    policies for multi-agent training. Policies are synchronized</span>
<span class="sd">    automatically from time to time using ray.remote calls. The exact</span>
<span class="sd">    synchronization logic depends on the specific algorithm used,</span>
<span class="sd">    but this usually happens from local worker to all remote workers and</span>
<span class="sd">    after each training update.</span>

<span class="sd">    You can write your own Algorithm classes by sub-classing from `Algorithm`</span>
<span class="sd">    or any of its built-in sub-classes.</span>
<span class="sd">    This allows you to override the `training_step` method to implement</span>
<span class="sd">    your own algorithm logic. You can find the different built-in</span>
<span class="sd">    algorithms&#39; `training_step()` methods in their respective main .py files,</span>
<span class="sd">    e.g. rllib.algorithms.dqn.dqn.py or rllib.algorithms.impala.impala.py.</span>

<span class="sd">    The most important API methods a Algorithm exposes are `train()`,</span>
<span class="sd">    `evaluate()`, `save_to_path()` and `restore_from_path()`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Whether to allow unknown top-level config keys.</span>
    <span class="n">_allow_unknown_configs</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># List of top-level keys with value=dict, for which new sub-keys are</span>
    <span class="c1"># allowed to be added to the value dict.</span>
    <span class="n">_allow_unknown_subkeys</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;tf_session_args&quot;</span><span class="p">,</span>
        <span class="s2">&quot;local_tf_session_args&quot;</span><span class="p">,</span>
        <span class="s2">&quot;env_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;model&quot;</span><span class="p">,</span>
        <span class="s2">&quot;optimizer&quot;</span><span class="p">,</span>
        <span class="s2">&quot;custom_resources_per_env_runner&quot;</span><span class="p">,</span>
        <span class="s2">&quot;custom_resources_per_worker&quot;</span><span class="p">,</span>
        <span class="s2">&quot;evaluation_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;exploration_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;extra_python_environs_for_worker&quot;</span><span class="p">,</span>
        <span class="s2">&quot;input_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;output_config&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># List of top level keys with value=dict, for which we always override the</span>
    <span class="c1"># entire value (dict), iff the &quot;type&quot; key in that value dict changes.</span>
    <span class="n">_override_all_subkeys_if_type_changes</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s2">&quot;exploration_config&quot;</span><span class="p">,</span>
        <span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># List of keys that are always fully overridden if present in any dict or sub-dict</span>
    <span class="n">_override_all_key_list</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;off_policy_estimation_methods&quot;</span><span class="p">,</span> <span class="s2">&quot;policies&quot;</span><span class="p">]</span>

    <span class="n">_progress_metrics</span> <span class="o">=</span> <span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ENV_RUNNER_RESULTS</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">EPISODE_RETURN_MEAN</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">EVALUATION_RESULTS</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">ENV_RUNNER_RESULTS</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">EPISODE_RETURN_MEAN</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">NUM_ENV_STEPS_TRAINED_LIFETIME</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">NUM_EPISODES_LIFETIME</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">ENV_RUNNER_RESULTS</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">EPISODE_LEN_MEAN</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Backward compatibility with old checkpoint system (now through the</span>
    <span class="c1"># `Checkpointable` API).</span>
    <span class="n">METADATA_FILE_NAME</span> <span class="o">=</span> <span class="s2">&quot;rllib_checkpoint.json&quot;</span>
    <span class="n">STATE_FILE_NAME</span> <span class="o">=</span> <span class="s2">&quot;algorithm_state.pkl&quot;</span>

<div class="viewcode-block" id="Algorithm.from_checkpoint">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint.html#ray.rllib.algorithms.algorithm.Algorithm.from_checkpoint">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">from_checkpoint</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Checkpoint</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">filesystem</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;pyarrow.fs.FileSystem&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="c1"># @OldAPIStack</span>
        <span class="n">policy_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Collection</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">,</span> <span class="n">EpisodeID</span><span class="p">],</span> <span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policies_to_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">Collection</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">],</span>
                <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatchType</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># deprecated args</span>
        <span class="n">checkpoint</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Algorithm&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Creates a new algorithm instance from a given checkpoint.</span>

<span class="sd">        Args:</span>
<span class="sd">            path: The path (str) to the checkpoint directory to use</span>
<span class="sd">                or an AIR Checkpoint instance to restore from.</span>
<span class="sd">            filesystem: PyArrow FileSystem to use to access data at the `path`. If not</span>
<span class="sd">                specified, this is inferred from the URI scheme of `path`.</span>
<span class="sd">            policy_ids: Optional list of PolicyIDs to recover. This allows users to</span>
<span class="sd">                restore an Algorithm with only a subset of the originally present</span>
<span class="sd">                Policies.</span>
<span class="sd">            policy_mapping_fn: An optional (updated) policy mapping function</span>
<span class="sd">                to use from here on.</span>
<span class="sd">            policies_to_train: An optional list of policy IDs to be trained</span>
<span class="sd">                or a callable taking PolicyID and SampleBatchType and</span>
<span class="sd">                returning a bool (trainable or not?).</span>
<span class="sd">                If None, will keep the existing setup in place. Policies,</span>
<span class="sd">                whose IDs are not in the list (or for which the callable</span>
<span class="sd">                returns False) will not be updated.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The instantiated Algorithm.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">checkpoint</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Algorithm.from_checkpoint(checkpoint=...)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;Algorithm.from_checkpoint(path=...)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">checkpoint</span>
        <span class="k">if</span> <span class="n">path</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`path` not provided in call to Algorithm.from_checkpoint()!&quot;</span>
            <span class="p">)</span>

        <span class="n">checkpoint_info</span> <span class="o">=</span> <span class="n">get_checkpoint_info</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

        <span class="c1"># Not possible for (v0.1) (algo class and config information missing</span>
        <span class="c1"># or very hard to retrieve).</span>
        <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">version</span><span class="o">.</span><span class="n">Version</span><span class="p">(</span><span class="s2">&quot;0.1&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot restore a v0 checkpoint using `Algorithm.from_checkpoint()`!&quot;</span>
                <span class="s2">&quot;In this case, do the following:</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;1) Create a new Algorithm object using your original config.</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;2) Call the `restore()` method of this algo object passing it&quot;</span>
                <span class="s2">&quot; your checkpoint dir or AIR Checkpoint object.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">version</span><span class="o">.</span><span class="n">Version</span><span class="p">(</span><span class="s2">&quot;1.0&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`checkpoint_info[&#39;checkpoint_version&#39;]` in `Algorithm.from_checkpoint&quot;</span>
                <span class="s2">&quot;()` must be 1.0 or later! You are using a checkpoint with &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;version v</span><span class="si">{</span><span class="n">checkpoint_info</span><span class="p">[</span><span class="s1">&#39;checkpoint_version&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># New API stack -&gt; Use Checkpointable&#39;s default implementation.</span>
        <span class="k">elif</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">version</span><span class="o">.</span><span class="n">Version</span><span class="p">(</span><span class="s2">&quot;2.0&quot;</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">from_checkpoint</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">filesystem</span><span class="o">=</span><span class="n">filesystem</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># This is a msgpack checkpoint.</span>
        <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;format&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;msgpack&quot;</span><span class="p">:</span>
            <span class="c1"># User did not provide unserializable function with this call</span>
            <span class="c1"># (`policy_mapping_fn`). Note that if `policies_to_train` is None, it</span>
            <span class="c1"># defaults to training all policies (so it&#39;s ok to not provide this here).</span>
            <span class="k">if</span> <span class="n">policy_mapping_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># Only DEFAULT_POLICY_ID present in this algorithm, provide default</span>
                <span class="c1"># implementations of these two functions.</span>
                <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;policy_ids&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="p">{</span><span class="n">DEFAULT_POLICY_ID</span><span class="p">}:</span>
                    <span class="n">policy_mapping_fn</span> <span class="o">=</span> <span class="n">AlgorithmConfig</span><span class="o">.</span><span class="n">DEFAULT_POLICY_MAPPING_FN</span>
                <span class="c1"># Provide meaningful error message.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;You are trying to restore a multi-agent algorithm from a &quot;</span>
                        <span class="s2">&quot;`msgpack` formatted checkpoint, which do NOT store the &quot;</span>
                        <span class="s2">&quot;`policy_mapping_fn` or `policies_to_train` &quot;</span>
                        <span class="s2">&quot;functions! Make sure that when using the &quot;</span>
                        <span class="s2">&quot;`Algorithm.from_checkpoint()` utility, you also pass the &quot;</span>
                        <span class="s2">&quot;args: `policy_mapping_fn` and `policies_to_train` with your &quot;</span>
                        <span class="s2">&quot;call. You might leave `policies_to_train=None` in case &quot;</span>
                        <span class="s2">&quot;you would like to train all policies anyways.&quot;</span>
                    <span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="n">Algorithm</span><span class="o">.</span><span class="n">_checkpoint_info_to_algorithm_state</span><span class="p">(</span>
            <span class="n">checkpoint_info</span><span class="o">=</span><span class="n">checkpoint_info</span><span class="p">,</span>
            <span class="n">policy_ids</span><span class="o">=</span><span class="n">policy_ids</span><span class="p">,</span>
            <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">policy_mapping_fn</span><span class="p">,</span>
            <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">Algorithm</span><span class="o">.</span><span class="n">from_state</span><span class="p">(</span><span class="n">state</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.from_state">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.from_state.html#ray.rllib.algorithms.algorithm.Algorithm.from_state">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">from_state</span><span class="p">(</span><span class="n">state</span><span class="p">:</span> <span class="n">Dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;Algorithm&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Recovers an Algorithm from a state object.</span>

<span class="sd">        The `state` of an instantiated Algorithm can be retrieved by calling its</span>
<span class="sd">        `get_state` method. It contains all information necessary</span>
<span class="sd">        to create the Algorithm from scratch. No access to the original code (e.g.</span>
<span class="sd">        configs, knowledge of the Algorithm&#39;s class, etc..) is needed.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: The state to recover a new Algorithm instance from.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A new Algorithm instance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">algorithm_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">Algorithm</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">algorithm_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;No `algorithm_class` key was found in given `state`! &quot;</span>
                <span class="s2">&quot;Cannot create new Algorithm.&quot;</span>
            <span class="p">)</span>
        <span class="c1"># algo_class = get_trainable_cls(algo_class_name)</span>
        <span class="c1"># Create the new algo.</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;config&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No `config` found in given Algorithm state!&quot;</span><span class="p">)</span>
        <span class="n">new_algo</span> <span class="o">=</span> <span class="n">algorithm_class</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
        <span class="c1"># Set the new algo&#39;s state.</span>
        <span class="n">new_algo</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

        <span class="c1"># Return the new algo.</span>
        <span class="k">return</span> <span class="n">new_algo</span></div>


<div class="viewcode-block" id="Algorithm.__init__">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.__init__.html#ray.rllib.algorithms.algorithm.Algorithm.__init__">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># deprecated arg</span>
        <span class="n">logger_creator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[],</span> <span class="n">Logger</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initializes an Algorithm instance.</span>

<span class="sd">        Args:</span>
<span class="sd">            config: Algorithm-specific configuration object.</span>
<span class="sd">            logger_creator: Callable that creates a ray.tune.Logger</span>
<span class="sd">                object. If unspecified, a default logger is created.</span>
<span class="sd">            **kwargs: Arguments passed to the Trainable base class.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">config</span>  <span class="c1"># or self.get_default_config()</span>

        <span class="c1"># Translate possible dict into an AlgorithmConfig object, as well as,</span>
        <span class="c1"># resolving generic config objects into specific ones (e.g. passing</span>
        <span class="c1"># an `AlgorithmConfig` super-class instance into a PPO constructor,</span>
        <span class="c1"># which normally would expect a PPOConfig object).</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">default_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span>
            <span class="c1"># `self.get_default_config()` also returned a dict -&gt;</span>
            <span class="c1"># Last resort: Create core AlgorithmConfig from merged dicts.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default_config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="k">if</span> <span class="s2">&quot;class&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
                    <span class="n">AlgorithmConfig</span><span class="o">.</span><span class="n">from_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">config</span> <span class="o">=</span> <span class="n">AlgorithmConfig</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span>
                        <span class="n">config_dict</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">merge_algorithm_configs</span><span class="p">(</span>
                            <span class="n">default_config</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="kc">True</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

            <span class="c1"># Default config is an AlgorithmConfig -&gt; update its properties</span>
            <span class="c1"># from the given config dict.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;class&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="p">:</span>
                    <span class="n">config</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">from_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">config</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">default_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span>
            <span class="c1"># Given AlgorithmConfig is not of the same type as the default config:</span>
            <span class="c1"># This could be the case e.g. if the user is building an algo from a</span>
            <span class="c1"># generic AlgorithmConfig() object.</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">default_config</span><span class="p">)):</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">to_dict</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">config</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">from_state</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">get_state</span><span class="p">())</span>

        <span class="c1"># In case this algo is using a generic config (with no algo_class set), set it</span>
        <span class="c1"># here.</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">algo_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">config</span><span class="o">.</span><span class="n">algo_class</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">env</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;algo = Algorithm(env=&#39;</span><span class="si">{</span><span class="n">env</span><span class="si">}</span><span class="s2">&#39;, ...)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;algo = AlgorithmConfig().environment(&#39;</span><span class="si">{</span><span class="n">env</span><span class="si">}</span><span class="s2">&#39;).build()&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">config</span><span class="o">.</span><span class="n">environment</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>

        <span class="c1"># Validate and freeze our AlgorithmConfig object (no more changes possible).</span>
        <span class="n">config</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
        <span class="n">config</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="c1"># Convert `env` provided in config into a concrete env creator callable, which</span>
        <span class="c1"># takes an EnvContext (config dict) as arg and returning an RLlib supported Env</span>
        <span class="c1"># type (e.g. a gym.Env).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_creator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_env_id_and_creator</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="n">config</span>
        <span class="p">)</span>
        <span class="n">env_descr</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span>
        <span class="p">)</span>

        <span class="c1"># Placeholder for a local replay buffer instance.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Placeholder for our LearnerGroup responsible for updating the RLModule(s).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;LearnerGroup&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># The Algorithm&#39;s `MetricsLogger` object to collect stats from all its</span>
        <span class="c1"># components (including timers, counters and other stats in its own</span>
        <span class="c1"># `training_step()` and other methods) as well as custom callbacks.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span> <span class="o">=</span> <span class="n">MetricsLogger</span><span class="p">()</span>

        <span class="c1"># Create a default logger creator if no logger_creator is specified</span>
        <span class="k">if</span> <span class="n">logger_creator</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Default logdir prefix containing the agent&#39;s name and the</span>
            <span class="c1"># env id.</span>
            <span class="n">timestr</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">today</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y-%m-</span><span class="si">%d</span><span class="s2">_%H-%M-%S&quot;</span><span class="p">)</span>
            <span class="n">env_descr_for_dir</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s2">&quot;[/</span><span class="se">\\\\</span><span class="s2">]&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">env_descr</span><span class="p">))</span>
            <span class="n">logdir_prefix</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">env_descr_for_dir</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">timestr</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">DEFAULT_STORAGE_PATH</span><span class="p">):</span>
                <span class="c1"># Possible race condition if dir is created several times on</span>
                <span class="c1"># rollout workers</span>
                <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">DEFAULT_STORAGE_PATH</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">logdir</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">(</span><span class="n">prefix</span><span class="o">=</span><span class="n">logdir_prefix</span><span class="p">,</span> <span class="nb">dir</span><span class="o">=</span><span class="n">DEFAULT_STORAGE_PATH</span><span class="p">)</span>

            <span class="c1"># Allow users to more precisely configure the created logger</span>
            <span class="c1"># via &quot;logger_config.type&quot;.</span>
            <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">logger_config</span> <span class="ow">and</span> <span class="s2">&quot;type&quot;</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">logger_config</span><span class="p">:</span>

                <span class="k">def</span> <span class="nf">default_logger_creator</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;Creates a custom logger with the default prefix.&quot;&quot;&quot;</span>
                    <span class="n">cfg</span> <span class="o">=</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;logger_config&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
                    <span class="bp">cls</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">)</span>
                    <span class="c1"># Provide default for logdir, in case the user does</span>
                    <span class="c1"># not specify this in the &quot;logger_config&quot; dict.</span>
                    <span class="n">logdir_</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;logdir&quot;</span><span class="p">,</span> <span class="n">logdir</span><span class="p">)</span>
                    <span class="k">return</span> <span class="n">from_config</span><span class="p">(</span><span class="bp">cls</span><span class="o">=</span><span class="bp">cls</span><span class="p">,</span> <span class="n">_args</span><span class="o">=</span><span class="p">[</span><span class="n">cfg</span><span class="p">],</span> <span class="n">logdir</span><span class="o">=</span><span class="n">logdir_</span><span class="p">)</span>

            <span class="c1"># If no `type` given, use tune&#39;s UnifiedLogger as last resort.</span>
            <span class="k">else</span><span class="p">:</span>

                <span class="k">def</span> <span class="nf">default_logger_creator</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="w">                    </span><span class="sd">&quot;&quot;&quot;Creates a Unified logger with the default prefix.&quot;&quot;&quot;</span>
                    <span class="k">return</span> <span class="n">UnifiedLogger</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">logdir</span><span class="p">,</span> <span class="n">loggers</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

            <span class="n">logger_creator</span> <span class="o">=</span> <span class="n">default_logger_creator</span>

        <span class="c1"># Metrics-related properties.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">_Timer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_episodes_to_be_collected</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># The fully qualified AlgorithmConfig used for evaluation</span>
        <span class="c1"># (or None if evaluation not setup).</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="c1"># Evaluation EnvRunnerGroup and metrics last returned by `self.evaluate()`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvRunnerGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
            <span class="n">logger_creator</span><span class="o">=</span><span class="n">logger_creator</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.get_default_config">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_default_config.html#ray.rllib.algorithms.algorithm.Algorithm.get_default_config">[docs]</a>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">get_default_config</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AlgorithmConfig</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">AlgorithmConfig</span><span class="p">()</span></div>


    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">_remote_worker_ids_for_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a list of remote worker IDs to fetch metrics from.</span>

<span class="sd">        Specific Algorithm implementations can override this method to</span>
<span class="sd">        use a subset of the workers for metrics collection.</span>

<span class="sd">        Returns:</span>
<span class="sd">            List of remote worker IDs to fetch metrics from.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">healthy_worker_ids</span><span class="p">()</span>

<div class="viewcode-block" id="Algorithm.setup">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.setup.html#ray.rllib.algorithms.algorithm.Algorithm.setup">[docs]</a>
    <span class="nd">@OverrideToImplementCustomLogic_CallToSuperRecommended</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Setup our config: Merge the user-supplied config dict (which could</span>
        <span class="c1"># be a partial config dict) with the class&#39; default.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">)</span>
            <span class="n">config_obj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config_obj</span><span class="p">,</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">)</span>
                <span class="n">config_obj</span> <span class="o">=</span> <span class="n">AlgorithmConfig</span><span class="p">()</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">config_obj</span><span class="p">)</span>
            <span class="n">config_obj</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
            <span class="n">config_obj</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_env_id</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config_obj</span>

        <span class="c1"># Set Algorithm&#39;s seed after we have - if necessary - enabled</span>
        <span class="c1"># tf eager-execution.</span>
        <span class="n">update_global_seed_if_necessary</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">framework_str</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_record_usage</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Create the callbacks object.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">callbacks_class</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;WARN&quot;</span><span class="p">,</span> <span class="s2">&quot;ERROR&quot;</span><span class="p">]:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Current log_level is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span><span class="si">}</span><span class="s2">. For more information, &quot;</span>
                <span class="s2">&quot;set &#39;log_level&#39;: &#39;INFO&#39; / &#39;DEBUG&#39; or use the -v and &quot;</span>
                <span class="s2">&quot;-vv flags.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="s2">&quot;ray.rllib&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">log_level</span><span class="p">)</span>

        <span class="c1"># Create local replay buffer if necessary.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_local_replay_buffer_if_necessary</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span>
        <span class="p">)</span>

        <span class="c1"># Create a dict, mapping ActorHandles to sets of open remote</span>
        <span class="c1"># requests (object refs). This way, we keep track, of which actors</span>
        <span class="c1"># inside this Algorithm (e.g. a remote EnvRunner) have</span>
        <span class="c1"># already been sent how many (e.g. `sample()`) requests.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">remote_requests_in_flight</span><span class="p">:</span> <span class="n">DefaultDict</span><span class="p">[</span>
            <span class="n">ActorHandle</span><span class="p">,</span> <span class="n">Set</span><span class="p">[</span><span class="n">ray</span><span class="o">.</span><span class="n">ObjectRef</span><span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">set</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvRunnerGroup</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Offline RL settings.</span>
        <span class="n">input_evaluation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;input_evaluation&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_evaluation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">input_evaluation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">ope_dict</span> <span class="o">=</span> <span class="p">{</span><span class="nb">str</span><span class="p">(</span><span class="n">ope</span><span class="p">):</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="n">ope</span><span class="p">}</span> <span class="k">for</span> <span class="n">ope</span> <span class="ow">in</span> <span class="n">input_evaluation</span><span class="p">}</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;config.input_evaluation=</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input_evaluation</span><span class="p">),</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;config.evaluation(evaluation_config=config.overrides(&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;off_policy_estimation_methods=</span><span class="si">{</span><span class="n">ope_dict</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="s2">&quot;))&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Running OPE during training is not recommended.&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span> <span class="o">=</span> <span class="n">ope_dict</span>

        <span class="c1"># Create a set of env runner actors via a EnvRunnerGroup.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span> <span class="o">=</span> <span class="n">EnvRunnerGroup</span><span class="p">(</span>
            <span class="n">env_creator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_creator</span><span class="p">,</span>
            <span class="n">validate_env</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validate_env</span><span class="p">,</span>
            <span class="n">default_policy_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_default_policy_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">),</span>
            <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="n">num_env_runners</span><span class="o">=</span><span class="p">(</span>
                <span class="mi">0</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_</span>
                    <span class="ow">and</span> <span class="p">(</span>
                        <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
                        <span class="ow">or</span> <span class="p">(</span>
                            <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
                            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">)</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
                    <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_</span> <span class="o">!=</span> <span class="s2">&quot;sampler&quot;</span>
                    <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span>
                <span class="p">)</span>
                <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_env_runners</span>
            <span class="p">),</span>
            <span class="n">local_env_runner</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">logdir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logdir</span><span class="p">,</span>
            <span class="n">tune_trial_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trial_id</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># If an input path is available and we are on the new API stack generate</span>
        <span class="c1"># an `OfflineData` instance.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_</span>
            <span class="ow">and</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
                <span class="ow">or</span> <span class="p">(</span>
                    <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">input_</span> <span class="o">!=</span> <span class="s2">&quot;sampler&quot;</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span>
        <span class="p">):</span>
            <span class="kn">from</span> <span class="nn">ray.rllib.offline.offline_data</span> <span class="kn">import</span> <span class="n">OfflineData</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span> <span class="o">=</span> <span class="n">OfflineData</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
        <span class="c1"># Otherwise set the attribute to `None`.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Compile, validate, and freeze an evaluation config.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_evaluation_config_object</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="c1"># Evaluation EnvRunnerGroup setup.</span>
        <span class="c1"># User would like to setup a separate evaluation worker set.</span>
        <span class="c1"># Note: We skip EnvRunnerGroup creation if we need to do offline evaluation.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_should_create_evaluation_rollout_workers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">):</span>
            <span class="n">_</span><span class="p">,</span> <span class="n">env_creator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_env_id_and_creator</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span>
            <span class="p">)</span>

            <span class="c1"># Create a separate evaluation worker set for evaluation.</span>
            <span class="c1"># If evaluation_num_env_runners=0, use the evaluation set&#39;s local</span>
            <span class="c1"># worker for evaluation, otherwise, use its remote workers</span>
            <span class="c1"># (parallelized evaluation).</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span> <span class="n">EnvRunnerGroup</span> <span class="o">=</span> <span class="n">EnvRunnerGroup</span><span class="p">(</span>
                <span class="n">env_creator</span><span class="o">=</span><span class="n">env_creator</span><span class="p">,</span>
                <span class="n">validate_env</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">default_policy_class</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">get_default_policy_class</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">),</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
                <span class="n">num_env_runners</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_num_env_runners</span><span class="p">,</span>
                <span class="n">logdir</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logdir</span><span class="p">,</span>
                <span class="n">tune_trial_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trial_id</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_dataset</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">ope_split_batch_by_episode</span>
        <span class="p">):</span>
            <span class="c1"># the num worker is set to 0 to avoid creating shards. The dataset will not</span>
            <span class="c1"># be repartioned to num_workers blocks.</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Creating evaluation dataset ...&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_dataset</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">get_dataset_and_shards</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Evaluation dataset created&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">OffPolicyEstimator</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">ope_types</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;is&quot;</span><span class="p">:</span> <span class="n">ImportanceSampling</span><span class="p">,</span>
            <span class="s2">&quot;wis&quot;</span><span class="p">:</span> <span class="n">WeightedImportanceSampling</span><span class="p">,</span>
            <span class="s2">&quot;dm&quot;</span><span class="p">:</span> <span class="n">DirectMethod</span><span class="p">,</span>
            <span class="s2">&quot;dr&quot;</span><span class="p">:</span> <span class="n">DoublyRobust</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">method_config</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">method_type</span> <span class="o">=</span> <span class="n">method_config</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;type&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">method_type</span> <span class="ow">in</span> <span class="n">ope_types</span><span class="p">:</span>
                <span class="n">deprecation_warning</span><span class="p">(</span>
                    <span class="n">old</span><span class="o">=</span><span class="n">method_type</span><span class="p">,</span>
                    <span class="n">new</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">ope_types</span><span class="p">[</span><span class="n">method_type</span><span class="p">]),</span>
                    <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">method_type</span> <span class="o">=</span> <span class="n">ope_types</span><span class="p">[</span><span class="n">method_type</span><span class="p">]</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">method_type</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Trying to import from string: &quot;</span> <span class="o">+</span> <span class="n">method_type</span><span class="p">)</span>
                <span class="n">mod</span><span class="p">,</span> <span class="n">obj</span> <span class="o">=</span> <span class="n">method_type</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">mod</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">import_module</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
                <span class="n">method_type</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">obj</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">method_type</span><span class="p">,</span> <span class="nb">type</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">issubclass</span><span class="p">(</span>
                <span class="n">method_type</span><span class="p">,</span> <span class="n">OfflineEvaluator</span>
            <span class="p">):</span>
                <span class="c1"># TODO(kourosh) : Add an integration test for all these</span>
                <span class="c1"># offline evaluators.</span>
                <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">()</span>
                <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">method_type</span><span class="p">,</span> <span class="n">OffPolicyEstimator</span><span class="p">):</span>
                    <span class="n">method_config</span><span class="p">[</span><span class="s2">&quot;gamma&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">gamma</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">method_type</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="o">**</span><span class="n">method_config</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Unknown off_policy_estimation type: </span><span class="si">{</span><span class="n">method_type</span><span class="si">}</span><span class="s2">! Must be &quot;</span>
                    <span class="s2">&quot;either a class path or a sub-class of ray.rllib.&quot;</span>
                    <span class="s2">&quot;offline.offline_evaluator::OfflineEvaluator&quot;</span>
                <span class="p">)</span>
            <span class="c1"># TODO (Rohan138): Refactor this and remove deprecated methods</span>
            <span class="c1"># Need to add back method_type in case Algorithm is restored from checkpoint</span>
            <span class="n">method_config</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">method_type</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="n">module_spec</span><span class="p">:</span> <span class="n">MultiRLModuleSpec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_multi_rl_module_spec</span><span class="p">(</span>
                <span class="n">spaces</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">get_spaces</span><span class="p">(),</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">build_learner_group</span><span class="p">(</span>
                <span class="n">rl_module_spec</span><span class="o">=</span><span class="n">module_spec</span>
            <span class="p">)</span>

            <span class="c1"># Check if there are modules to load from the `module_spec`.</span>
            <span class="n">rl_module_ckpt_dirs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">multi_rl_module_ckpt_dir</span> <span class="o">=</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">load_state_path</span>
            <span class="n">modules_to_load</span> <span class="o">=</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">modules_to_load</span>
            <span class="k">for</span> <span class="n">module_id</span><span class="p">,</span> <span class="n">sub_module_spec</span> <span class="ow">in</span> <span class="n">module_spec</span><span class="o">.</span><span class="n">module_specs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">sub_module_spec</span><span class="o">.</span><span class="n">load_state_path</span><span class="p">:</span>
                    <span class="n">rl_module_ckpt_dirs</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_module_spec</span><span class="o">.</span><span class="n">load_state_path</span>
            <span class="k">if</span> <span class="n">multi_rl_module_ckpt_dir</span> <span class="ow">or</span> <span class="n">rl_module_ckpt_dirs</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">load_module_state</span><span class="p">(</span>
                    <span class="n">multi_rl_module_ckpt_dir</span><span class="o">=</span><span class="n">multi_rl_module_ckpt_dir</span><span class="p">,</span>
                    <span class="n">modules_to_load</span><span class="o">=</span><span class="n">modules_to_load</span><span class="p">,</span>
                    <span class="n">rl_module_ckpt_dirs</span><span class="o">=</span><span class="n">rl_module_ckpt_dirs</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="c1"># Sync the weights from the learner group to the EnvRunners.</span>
            <span class="n">rl_module_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span>
                <span class="n">components</span><span class="o">=</span><span class="n">COMPONENT_LEARNER</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)[</span><span class="n">COMPONENT_LEARNER</span><span class="p">][</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="o">.</span><span class="n">set_state</span><span class="p">({</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">:</span> <span class="n">rl_module_state</span><span class="p">})</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_env_runner_states</span><span class="p">(</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                <span class="n">env_steps_sampled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                    <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span>
                <span class="p">),</span>
                <span class="n">rl_module_state</span><span class="o">=</span><span class="n">rl_module_state</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span><span class="p">:</span>
                <span class="c1"># If the learners are remote we need to provide specific</span>
                <span class="c1"># information and the learner&#39;s actor handles.</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">is_remote</span><span class="p">:</span>
                    <span class="c1"># If learners run on different nodes, locality hints help</span>
                    <span class="c1"># to use the nearest learner in the workers that do the</span>
                    <span class="c1"># data preprocessing.</span>
                    <span class="n">learner_node_ids</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">foreach_learner</span><span class="p">(</span>
                        <span class="k">lambda</span> <span class="n">_</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">get_runtime_context</span><span class="p">()</span><span class="o">.</span><span class="n">get_node_id</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span><span class="o">.</span><span class="n">locality_hints</span> <span class="o">=</span> <span class="p">[</span>
                        <span class="n">node_id</span><span class="o">.</span><span class="n">get</span><span class="p">()</span> <span class="k">for</span> <span class="n">node_id</span> <span class="ow">in</span> <span class="n">learner_node_ids</span>
                    <span class="p">]</span>
                    <span class="c1"># Provide the actor handles for the learners for module</span>
                    <span class="c1"># updating during preprocessing.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span><span class="o">.</span><span class="n">learner_handles</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">_workers</span>
                    <span class="c1"># Provide the module_spec. Note, in the remote case this is needed</span>
                    <span class="c1"># because the learner module cannot be copied, but must be built.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span><span class="o">.</span><span class="n">module_spec</span> <span class="o">=</span> <span class="n">module_spec</span>
                <span class="c1"># Otherwise we can simply pass in the local learner.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span><span class="o">.</span><span class="n">learner_handles</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">_learner</span><span class="p">]</span>

                <span class="c1"># Provide the `OfflineData` instance with space information. It might</span>
                <span class="c1"># need it for reading recorded experiences.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">offline_data</span><span class="o">.</span><span class="n">spaces</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">get_spaces</span><span class="p">()</span>

        <span class="c1"># Run `on_algorithm_init` callback after initialization is done.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_algorithm_init</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.get_default_policy_class">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_default_policy_class.html#ray.rllib.algorithms.algorithm.Algorithm.get_default_policy_class">[docs]</a>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">get_default_policy_class</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">Policy</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns a default Policy class to use, given a config.</span>

<span class="sd">        This class will be used by an Algorithm in case</span>
<span class="sd">        the policy class is not provided by the user in any single- or</span>
<span class="sd">        multi-agent PolicySpec.</span>

<span class="sd">        Note: This method is ignored when the RLModule API is enabled.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="kc">None</span></div>


<div class="viewcode-block" id="Algorithm.step">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.step.html#ray.rllib.algorithms.algorithm.Algorithm.step">[docs]</a>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResultDict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Implements the main `Algorithm.train()` logic.</span>

<span class="sd">        Takes n attempts to perform a single training step. Thereby</span>
<span class="sd">        catches RayErrors resulting from worker failures. After n attempts,</span>
<span class="sd">        fails gracefully.</span>

<span class="sd">        Override this method in your Algorithm sub-classes if you would like to</span>
<span class="sd">        handle worker failures yourself.</span>
<span class="sd">        Otherwise, override only `training_step()` to implement the core</span>
<span class="sd">        algorithm logic.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The results dict with stats/infos on sampling, training,</span>
<span class="sd">            and - if required - evaluation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Do we have to run `self.evaluate()` this iteration?</span>
        <span class="c1"># `self.iteration` gets incremented after this function returns,</span>
        <span class="c1"># meaning that e.g. the first time this function is called,</span>
        <span class="c1"># self.iteration will be 0.</span>
        <span class="n">evaluate_this_iter</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_interval</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_interval</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">)</span>
        <span class="c1"># Results dict for training (and if appolicable: evaluation).</span>
        <span class="n">train_results</span><span class="p">:</span> <span class="n">ResultDict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">eval_results</span><span class="p">:</span> <span class="n">ResultDict</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Parallel eval + training (BUT w/o using a thread pool):</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">evaluate_this_iter</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_evaluation_parallel_to_training_wo_thread</span>
        <span class="p">):</span>
            <span class="p">(</span>
                <span class="n">train_results</span><span class="p">,</span>
                <span class="n">eval_results</span><span class="p">,</span>
                <span class="n">train_iter_ctx</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_training_iteration_and_evaluation_in_parallel_wo_thread</span><span class="p">()</span>
        <span class="c1"># Parallel eval + training: Kick off evaluation-loop and parallel train() call.</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_run_training_always_in_thread</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="n">evaluate_this_iter</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span>
        <span class="p">):</span>
            <span class="p">(</span>
                <span class="n">train_results</span><span class="p">,</span>
                <span class="n">eval_results</span><span class="p">,</span>
                <span class="n">train_iter_ctx</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_training_iteration_and_evaluation_in_parallel</span><span class="p">()</span>

        <span class="c1"># - No evaluation necessary, just run the next training iteration.</span>
        <span class="c1"># - We have to evaluate in this training iteration, but no parallelism -&gt;</span>
        <span class="c1">#   evaluate after the training iteration is entirely done.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">train_results</span><span class="p">,</span> <span class="n">train_iter_ctx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_training_iteration</span><span class="p">()</span>

        <span class="c1"># Sequential: Train (already done above), then evaluate.</span>
        <span class="k">if</span> <span class="n">evaluate_this_iter</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span><span class="p">:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_evaluation</span><span class="p">(</span><span class="n">parallel_train_future</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Sync EnvRunner workers.</span>
        <span class="c1"># TODO (sven): For the new API stack, the common execution pattern for any algo</span>
        <span class="c1">#  should be: [sample + get_metrics + get_state] -&gt; send all these in one remote</span>
        <span class="c1">#  call down to `training_step` (where episodes are sent as ray object</span>
        <span class="c1">#  references). Then distribute the episode refs to the learners, store metrics</span>
        <span class="c1">#  in special key in result dict and perform the connector merge/broadcast</span>
        <span class="c1">#  inside the `training_step` as well. See the new IMPALA for an example.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_dont_auto_sync_env_runner_states</span><span class="p">:</span>
                <span class="c1"># Synchronize EnvToModule and ModuleToEnv connector states and broadcast</span>
                <span class="c1"># new states back to all EnvRunners.</span>
                <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">SYNCH_ENV_CONNECTOR_STATES_TIMER</span><span class="p">)):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_env_runner_states</span><span class="p">(</span>
                        <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
                        <span class="n">env_steps_sampled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                            <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span>
                        <span class="p">),</span>
                    <span class="p">)</span>
            <span class="c1"># Compile final ResultDict from `train_results` and `eval_results`. Note</span>
            <span class="c1"># that, as opposed to the old API stack, EnvRunner stats should already be</span>
            <span class="c1"># in `train_results` and `eval_results`.</span>
            <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile_iteration_results_new_api_stack</span><span class="p">(</span>
                <span class="n">train_results</span><span class="o">=</span><span class="n">train_results</span><span class="p">,</span>
                <span class="n">eval_results</span><span class="o">=</span><span class="n">eval_results</span><span class="p">,</span>
                <span class="n">step_ctx</span><span class="o">=</span><span class="n">train_iter_ctx</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sync_filters_if_needed</span><span class="p">(</span>
                <span class="n">central_worker</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="p">,</span>
                <span class="n">workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">,</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># Get EnvRunner metrics and compile them into results.</span>
            <span class="n">episodes_this_iter</span> <span class="o">=</span> <span class="n">collect_episodes</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_remote_worker_ids_for_metrics</span><span class="p">(),</span>
                <span class="n">timeout_seconds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">metrics_episode_collection_timeout_s</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile_iteration_results_old_api_stack</span><span class="p">(</span>
                <span class="n">episodes_this_iter</span><span class="o">=</span><span class="n">episodes_this_iter</span><span class="p">,</span>
                <span class="n">step_ctx</span><span class="o">=</span><span class="n">train_iter_ctx</span><span class="p">,</span>
                <span class="n">iteration_results</span><span class="o">=</span><span class="p">{</span><span class="o">**</span><span class="n">train_results</span><span class="p">,</span> <span class="o">**</span><span class="n">eval_results</span><span class="p">},</span>
            <span class="p">)</span>

        <span class="c1"># TODO (sven): Deprecate this API, this should be done via a custom Callback.</span>
        <span class="c1">#  Provide example script/update existing one.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">env_task_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">callable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">env_task_fn</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`env_task_fn` must be None or a callable taking&quot;</span>
                    <span class="s2">&quot; [train_results, env, env_ctx] as args!&quot;</span>
                <span class="p">)</span>

            <span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">env_context</span><span class="p">,</span> <span class="n">task_fn</span><span class="p">):</span>
                <span class="n">new_task</span> <span class="o">=</span> <span class="n">task_fn</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">env_context</span><span class="p">)</span>
                <span class="n">cur_task</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_task</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">cur_task</span> <span class="o">!=</span> <span class="n">new_task</span><span class="p">:</span>
                    <span class="n">env</span><span class="o">.</span><span class="n">set_task</span><span class="p">(</span><span class="n">new_task</span><span class="p">)</span>

            <span class="n">fn</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">task_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">env_task_fn</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_env_with_context</span><span class="p">(</span><span class="n">fn</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">results</span></div>


<div class="viewcode-block" id="Algorithm.evaluate">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.evaluate.html#ray.rllib.algorithms.algorithm.Algorithm.evaluate">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">parallel_train_future</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">ThreadPoolExecutor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResultDict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluates current policy under `evaluation_config` settings.</span>

<span class="sd">        Args:</span>
<span class="sd">            parallel_train_future: In case, we are training and avaluating in parallel,</span>
<span class="sd">                this arg carries the currently running ThreadPoolExecutor object that</span>
<span class="sd">                runs the training iteration. Use `parallel_train_future.done()` to</span>
<span class="sd">                check, whether the parallel training job has completed and</span>
<span class="sd">                `parallel_train_future.result()` to get its return values.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A ResultDict only containing the evaluation results from the current</span>
<span class="sd">            iteration.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Call the `_before_evaluate` hook.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_before_evaluate</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_dataset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_offline_evaluation</span><span class="p">()</span>

        <span class="c1"># Sync weights to the evaluation EnvRunners.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
                <span class="c1"># Synchronize EnvToModule and ModuleToEnv connector states and broadcast</span>
                <span class="c1"># new states back to all eval EnvRunners.</span>
                <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">SYNCH_EVAL_ENV_CONNECTOR_STATES_TIMER</span><span class="p">]:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_env_runner_states</span><span class="p">(</span>
                        <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
                        <span class="n">from_worker</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="p">,</span>
                        <span class="n">env_steps_sampled</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                            <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span>
                        <span class="p">),</span>
                    <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_sync_filters_if_needed</span><span class="p">(</span>
                    <span class="n">central_worker</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="p">,</span>
                    <span class="n">workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">,</span>
                    <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_evaluate_start</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>

        <span class="n">env_steps</span> <span class="o">=</span> <span class="n">agent_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">batches</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># We will use a user provided evaluation function.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
                <span class="p">(</span>
                    <span class="n">eval_results</span><span class="p">,</span>
                    <span class="n">env_steps</span><span class="p">,</span>
                    <span class="n">agent_steps</span><span class="p">,</span>
                <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_with_custom_eval_function</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">()</span>
        <span class="c1"># There is no eval EnvRunnerGroup -&gt; Run on local EnvRunner.</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="p">(</span>
                <span class="n">eval_results</span><span class="p">,</span>
                <span class="n">env_steps</span><span class="p">,</span>
                <span class="n">agent_steps</span><span class="p">,</span>
                <span class="n">batches</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_on_local_env_runner</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span>
            <span class="p">)</span>
        <span class="c1"># There is only a local eval EnvRunner -&gt; Run on that.</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="p">(</span>
                <span class="n">eval_results</span><span class="p">,</span>
                <span class="n">env_steps</span><span class="p">,</span>
                <span class="n">agent_steps</span><span class="p">,</span>
                <span class="n">batches</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_on_local_env_runner</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="p">)</span>
        <span class="c1"># There are healthy remote evaluation workers -&gt; Run on these.</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Running in automatic duration mode (parallel with training step).</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">parallel_train_future</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="p">(</span>
                    <span class="n">eval_results</span><span class="p">,</span>
                    <span class="n">env_steps</span><span class="p">,</span>
                    <span class="n">agent_steps</span><span class="p">,</span>
                    <span class="n">batches</span><span class="p">,</span>
                <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_with_auto_duration</span><span class="p">(</span><span class="n">parallel_train_future</span><span class="p">)</span>
            <span class="c1"># Running with a fixed amount of data to sample.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="p">(</span>
                    <span class="n">eval_results</span><span class="p">,</span>
                    <span class="n">env_steps</span><span class="p">,</span>
                    <span class="n">agent_steps</span><span class="p">,</span>
                    <span class="n">batches</span><span class="p">,</span>
                <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_evaluate_with_fixed_duration</span><span class="p">()</span>
        <span class="c1"># Can&#39;t find a good way to run this evaluation -&gt; Wait for next iteration.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="c1"># Lifetime eval counters.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">:</span> <span class="n">env_steps</span><span class="p">,</span>
                    <span class="n">NUM_AGENT_STEPS_SAMPLED_LIFETIME</span><span class="p">:</span> <span class="n">agent_steps</span><span class="p">,</span>
                    <span class="n">NUM_EPISODES_LIFETIME</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                        <span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span> <span class="n">NUM_EPISODES</span><span class="p">),</span>
                        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">},</span>
                <span class="n">key</span><span class="o">=</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span>
                <span class="n">reduce</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">reduce</span><span class="p">(</span>
                <span class="n">key</span><span class="o">=</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">return_stats_obj</span><span class="o">=</span><span class="kc">False</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">ENV_RUNNER_RESULTS</span><span class="p">:</span> <span class="n">eval_results</span><span class="p">}</span>
            <span class="n">eval_results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED_THIS_ITER</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent_steps</span>
            <span class="n">eval_results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED_THIS_ITER</span><span class="p">]</span> <span class="o">=</span> <span class="n">env_steps</span>
            <span class="n">eval_results</span><span class="p">[</span><span class="s2">&quot;timesteps_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">env_steps</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED_FOR_EVALUATION_THIS_ITER</span><span class="p">]</span> <span class="o">=</span> <span class="n">env_steps</span>

        <span class="c1"># Compute off-policy estimates</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">:</span>
            <span class="n">estimates</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
            <span class="c1"># for each batch run the estimator&#39;s fwd pass</span>
            <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">batches</span><span class="p">:</span>
                    <span class="n">estimate_result</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">estimate</span><span class="p">(</span>
                        <span class="n">batch</span><span class="p">,</span>
                        <span class="n">split_batch_by_episode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">ope_split_batch_by_episode</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">estimates</span><span class="p">[</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">estimate_result</span><span class="p">)</span>

            <span class="c1"># collate estimates from all batches</span>
            <span class="k">if</span> <span class="n">estimates</span><span class="p">:</span>
                <span class="n">eval_results</span><span class="p">[</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
                <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">estimate_list</span> <span class="ow">in</span> <span class="n">estimates</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">avg_estimate</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
                        <span class="k">lambda</span> <span class="o">*</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="o">*</span><span class="n">estimate_list</span>
                    <span class="p">)</span>
                    <span class="n">eval_results</span><span class="p">[</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">][</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">avg_estimate</span>

        <span class="c1"># Trigger `on_evaluate_end` callback.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_evaluate_end</span><span class="p">(</span>
            <span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">metrics_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">,</span>
            <span class="n">evaluation_metrics</span><span class="o">=</span><span class="n">eval_results</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Also return the results here for convenience.</span>
        <span class="k">return</span> <span class="n">eval_results</span></div>


    <span class="k">def</span> <span class="nf">_evaluate_with_custom_eval_function</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ResultDict</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Evaluating current state of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> using the custom eval function &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="p">(</span>
                <span class="n">eval_results</span><span class="p">,</span>
                <span class="n">env_steps</span><span class="p">,</span>
                <span class="n">agent_steps</span><span class="p">,</span>
            <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">env_steps</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">agent_steps</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Custom eval function must return &quot;</span>
                    <span class="s2">&quot;`Tuple[ResultDict, int, int]` with `int, int` being &quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;`env_steps` and `agent_steps`! Got </span><span class="si">{</span><span class="n">env_steps</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">agent_steps</span><span class="si">}</span><span class="s2">.&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">custom_evaluation_function</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">eval_results</span> <span class="ow">or</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_results</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Custom eval function must return &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;dict of metrics! Got </span><span class="si">{</span><span class="n">eval_results</span><span class="si">}</span><span class="s2">.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">eval_results</span><span class="p">,</span> <span class="n">env_steps</span><span class="p">,</span> <span class="n">agent_steps</span>

    <span class="k">def</span> <span class="nf">_evaluate_on_local_env_runner</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">env_runner</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">env_runner</span><span class="p">,</span> <span class="s2">&quot;input_reader&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">env_runner</span><span class="o">.</span><span class="n">input_reader</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t evaluate on a local worker if this local worker does not have &quot;</span>
                <span class="s2">&quot;an environment!</span><span class="se">\n</span><span class="s2">Try one of the following:&quot;</span>
                <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">1) Set `evaluation_interval` &gt; 0 to force creating a separate &quot;</span>
                <span class="s2">&quot;evaluation EnvRunnerGroup.</span><span class="se">\n</span><span class="s2">2) Set `create_env_on_driver=True` to &quot;</span>
                <span class="s2">&quot;force the local (non-eval) EnvRunner to have an environment to &quot;</span>
                <span class="s2">&quot;evaluate on.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_parallel_to_training</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cannot run on local evaluation worker parallel to training! Try &quot;</span>
                <span class="s2">&quot;setting `evaluation_parallel_to_training=False`.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># How many episodes/timesteps do we need to run?</span>
        <span class="n">unit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration_unit</span>
        <span class="n">duration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span>
        <span class="n">eval_cfg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span>

        <span class="n">env_steps</span> <span class="o">=</span> <span class="n">agent_steps</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Evaluating current state of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> for </span><span class="si">{</span><span class="n">duration</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">unit</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

        <span class="n">all_batches</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="n">episodes</span> <span class="o">=</span> <span class="n">env_runner</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
                <span class="n">num_timesteps</span><span class="o">=</span><span class="n">duration</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;timesteps&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">num_episodes</span><span class="o">=</span><span class="n">duration</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">agent_steps</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">)</span>
            <span class="n">env_steps</span> <span class="o">+=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">duration</span><span class="p">):</span>
                <span class="n">batch</span> <span class="o">=</span> <span class="n">env_runner</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
                <span class="n">agent_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
                <span class="n">env_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span>
                    <span class="n">all_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="n">env_runner</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="n">agent_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
            <span class="n">env_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span>
                <span class="n">all_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

        <span class="n">env_runner_results</span> <span class="o">=</span> <span class="n">env_runner</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="n">env_runner_results</span> <span class="o">=</span> <span class="n">summarize_episodes</span><span class="p">(</span>
                <span class="n">env_runner_results</span><span class="p">,</span>
                <span class="n">env_runner_results</span><span class="p">,</span>
                <span class="n">keep_custom_metrics</span><span class="o">=</span><span class="n">eval_cfg</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span>
                <span class="n">env_runner_results</span><span class="p">,</span>
                <span class="n">key</span><span class="o">=</span><span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">ENV_RUNNER_RESULTS</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">env_runner_results</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="n">env_runner_results</span><span class="p">,</span> <span class="n">env_steps</span><span class="p">,</span> <span class="n">agent_steps</span><span class="p">,</span> <span class="n">all_batches</span>

    <span class="k">def</span> <span class="nf">_evaluate_with_auto_duration</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">parallel_train_future</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Evaluating current state of </span><span class="si">{</span><span class="bp">self</span><span class="si">}</span><span class="s2"> for as long as the parallelly &quot;</span>
            <span class="s2">&quot;running training step takes.&quot;</span>
        <span class="p">)</span>

        <span class="n">all_metrics</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_batches</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># How many episodes have we run (across all eval workers)?</span>
        <span class="n">num_healthy_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
        <span class="c1"># Do we have to force-reset the EnvRunners before the first round of `sample()`</span>
        <span class="c1"># calls.?</span>
        <span class="n">force_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_force_reset_envs_before_iteration</span>

        <span class="c1"># Remote function used on healthy EnvRunners to sample, get metrics, and</span>
        <span class="c1"># step counts.</span>
        <span class="k">def</span> <span class="nf">_env_runner_remote</span><span class="p">(</span><span class="n">worker</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="nb">round</span><span class="p">,</span> <span class="nb">iter</span><span class="p">):</span>
            <span class="c1"># Sample AND get_metrics, but only return metrics (and steps actually taken)</span>
            <span class="c1"># to save time.</span>
            <span class="n">episodes</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
                <span class="n">num_timesteps</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="n">force_reset</span><span class="o">=</span><span class="n">force_reset</span> <span class="ow">and</span> <span class="nb">round</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">)</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">()</span>
            <span class="n">env_steps</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">)</span>
            <span class="n">agent_steps</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">env_steps</span><span class="p">,</span> <span class="n">agent_steps</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="nb">iter</span>

        <span class="n">env_steps</span> <span class="o">=</span> <span class="n">agent_steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">train_mean_time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">TRAINING_ITERATION_TIMER</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span>
        <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">algo_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span>

        <span class="n">_round</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">while</span> <span class="p">(</span>
            <span class="c1"># In case all the remote evaluation workers die during a round of</span>
            <span class="c1"># evaluation, we need to stop.</span>
            <span class="n">num_healthy_workers</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="c1"># Run at least for one round AND at least for as long as the parallel</span>
            <span class="c1"># training step takes.</span>
            <span class="ow">and</span> <span class="p">(</span><span class="n">_round</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">parallel_train_future</span><span class="o">.</span><span class="n">done</span><span class="p">())</span>
        <span class="p">):</span>
            <span class="n">_round</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="c1"># New API stack -&gt; EnvRunners return Episodes.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
                <span class="c1"># Compute rough number of timesteps it takes for a single EnvRunner</span>
                <span class="c1"># to occupy the estimated (parallelly running) train step.</span>
                <span class="n">_num</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
                    <span class="c1"># Cap at 20k to not put too much memory strain on EnvRunners.</span>
                    <span class="mi">20000</span><span class="p">,</span>
                    <span class="nb">max</span><span class="p">(</span>
                        <span class="c1"># Low-cap at 100 to avoid possibly negative rollouts or very</span>
                        <span class="c1"># short ones.</span>
                        <span class="mi">100</span><span class="p">,</span>
                        <span class="p">(</span>
                            <span class="c1"># How much time do we have left?</span>
                            <span class="p">(</span><span class="n">train_mean_time</span> <span class="o">-</span> <span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>
                            <span class="c1"># Multiply by our own (eval) throughput to get the timesteps</span>
                            <span class="c1"># to do (per worker).</span>
                            <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">EVALUATION_ITERATION_TIMER</span><span class="p">]</span><span class="o">.</span><span class="n">mean_throughput</span>
                            <span class="o">/</span> <span class="n">num_healthy_workers</span>
                        <span class="p">),</span>
                    <span class="p">),</span>
                <span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_worker_async</span><span class="p">(</span>
                    <span class="n">func</span><span class="o">=</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                        <span class="n">_env_runner_remote</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">_num</span><span class="p">,</span> <span class="nb">round</span><span class="o">=</span><span class="n">_round</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=</span><span class="n">algo_iteration</span>
                    <span class="p">),</span>
                <span class="p">)</span>
                <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">fetch_ready_async_reqs</span><span class="p">(</span>
                    <span class="n">return_obj_refs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="o">=</span><span class="mf">0.01</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">wid</span><span class="p">,</span> <span class="p">(</span><span class="n">env_s</span><span class="p">,</span> <span class="n">ag_s</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">iter</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">env_steps</span> <span class="o">+=</span> <span class="n">env_s</span>
                    <span class="n">agent_steps</span> <span class="o">+=</span> <span class="n">ag_s</span>
                    <span class="n">all_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
            <span class="c1"># Old API stack -&gt; RolloutWorkers return batches.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_worker_async</span><span class="p">(</span>
                    <span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span> <span class="n">w</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">(),</span> <span class="n">algo_iteration</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">fetch_ready_async_reqs</span><span class="p">(</span>
                    <span class="n">return_obj_refs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="o">=</span><span class="mf">0.01</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">wid</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">iter</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">env_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>
                    <span class="n">agent_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
                    <span class="n">all_metrics</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span>
                        <span class="c1"># TODO: (kourosh) This approach will cause an OOM issue when</span>
                        <span class="c1">#  the dataset gets huge (should be ok for now).</span>
                        <span class="n">all_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

            <span class="c1"># Update correct number of healthy remote workers.</span>
            <span class="n">num_healthy_workers</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">num_healthy_workers</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Calling `sample()` on your remote evaluation worker(s) &quot;</span>
                <span class="s2">&quot;resulted in all workers crashing! Make sure a) your environment is not&quot;</span>
                <span class="s2">&quot; too unstable, b) you have enough evaluation workers &quot;</span>
                <span class="s2">&quot;(`config.evaluation(evaluation_num_env_runners=...)`) to cover for &quot;</span>
                <span class="s2">&quot;occasional losses, and c) you use the `config.fault_tolerance(&quot;</span>
                <span class="s2">&quot;recreate_failed_env_runners=True)` setting.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="n">env_runner_results</span> <span class="o">=</span> <span class="n">summarize_episodes</span><span class="p">(</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">keep_custom_metrics</span><span class="o">=</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">num_episodes</span> <span class="o">=</span> <span class="n">env_runner_results</span><span class="p">[</span><span class="n">NUM_EPISODES</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">merge_and_log_n_dicts</span><span class="p">(</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">key</span><span class="o">=</span><span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">ENV_RUNNER_RESULTS</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">num_episodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                <span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span> <span class="n">NUM_EPISODES</span><span class="p">),</span>
                <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">env_runner_results</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Warn if results are empty, it could be that this is because the auto-time is</span>
        <span class="c1"># not enough to run through one full episode.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_force_reset_envs_before_iteration</span>
            <span class="ow">and</span> <span class="n">num_episodes</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This evaluation iteration resulted in an empty set of episode summary &quot;</span>
                <span class="s2">&quot;results! It&#39;s possible that the auto-duration time (roughly the mean &quot;</span>
                <span class="s2">&quot;time it takes for the training step to finish) is not enough to finish&quot;</span>
                <span class="s2">&quot; even a single episode. Your current mean training iteration time is &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">train_mean_time</span><span class="si">}</span><span class="s2">sec. Try setting the min iteration time to a higher &quot;</span>
                <span class="s2">&quot;value via the `config.reporting(min_time_s_per_iteration=...)` OR you &quot;</span>
                <span class="s2">&quot;can also set `config.evaluation_force_reset_envs_before_iteration` to &quot;</span>
                <span class="s2">&quot;False. However, keep in mind that then the evaluation results may &quot;</span>
                <span class="s2">&quot;contain some episode stats generated with earlier weights versions.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">env_runner_results</span><span class="p">,</span> <span class="n">env_steps</span><span class="p">,</span> <span class="n">agent_steps</span><span class="p">,</span> <span class="n">all_batches</span>

    <span class="k">def</span> <span class="nf">_evaluate_with_fixed_duration</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># How many episodes/timesteps do we need to run?</span>
        <span class="n">unit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration_unit</span>
        <span class="n">eval_cfg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span>
        <span class="n">num_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_num_env_runners</span>
        <span class="n">force_reset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_force_reset_envs_before_iteration</span>
        <span class="n">time_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_sample_timeout_s</span>

        <span class="c1"># Remote function used on healthy EnvRunners to sample, get metrics, and</span>
        <span class="c1"># step counts.</span>
        <span class="k">def</span> <span class="nf">_env_runner_remote</span><span class="p">(</span><span class="n">worker</span><span class="p">,</span> <span class="n">num</span><span class="p">,</span> <span class="nb">round</span><span class="p">,</span> <span class="nb">iter</span><span class="p">):</span>
            <span class="c1"># Sample AND get_metrics, but only return metrics (and steps actually taken)</span>
            <span class="c1"># to save time. Also return the iteration to check, whether we should</span>
            <span class="c1"># discard and outdated result (from a slow worker).</span>
            <span class="n">episodes</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span>
                <span class="n">num_timesteps</span><span class="o">=</span><span class="p">(</span>
                    <span class="n">num</span><span class="p">[</span><span class="n">worker</span><span class="o">.</span><span class="n">worker_index</span><span class="p">]</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;timesteps&quot;</span> <span class="k">else</span> <span class="kc">None</span>
                <span class="p">),</span>
                <span class="n">num_episodes</span><span class="o">=</span><span class="p">(</span><span class="n">num</span><span class="p">[</span><span class="n">worker</span><span class="o">.</span><span class="n">worker_index</span><span class="p">]</span> <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span> <span class="k">else</span> <span class="kc">None</span><span class="p">),</span>
                <span class="n">force_reset</span><span class="o">=</span><span class="n">force_reset</span> <span class="ow">and</span> <span class="nb">round</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">metrics</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">()</span>
            <span class="n">env_steps</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">)</span>
            <span class="n">agent_steps</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">e</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">env_steps</span><span class="p">,</span> <span class="n">agent_steps</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="nb">iter</span>

        <span class="n">all_metrics</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">all_batches</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># How many episodes have we run (across all eval workers)?</span>
        <span class="n">num_units_done</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">num_healthy_workers</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>

        <span class="n">env_steps</span> <span class="o">=</span> <span class="n">agent_steps</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">t_last_result</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">_round</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">algo_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span>

        <span class="c1"># In case all the remote evaluation workers die during a round of</span>
        <span class="c1"># evaluation, we need to stop.</span>
        <span class="k">while</span> <span class="n">num_healthy_workers</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">units_left_to_do</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">-</span> <span class="n">num_units_done</span>
            <span class="k">if</span> <span class="n">units_left_to_do</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">break</span>

            <span class="n">_round</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="c1"># New API stack -&gt; EnvRunners return Episodes.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
                <span class="n">_num</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span>  <span class="c1"># [None]: skip idx=0 (local worker)</span>
                    <span class="p">(</span><span class="n">units_left_to_do</span> <span class="o">//</span> <span class="n">num_healthy_workers</span><span class="p">)</span>
                    <span class="o">+</span> <span class="nb">bool</span><span class="p">(</span><span class="n">i</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">units_left_to_do</span> <span class="o">%</span> <span class="n">num_healthy_workers</span><span class="p">))</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_workers</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_worker_async</span><span class="p">(</span>
                    <span class="n">func</span><span class="o">=</span><span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                        <span class="n">_env_runner_remote</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">_num</span><span class="p">,</span> <span class="nb">round</span><span class="o">=</span><span class="n">_round</span><span class="p">,</span> <span class="nb">iter</span><span class="o">=</span><span class="n">algo_iteration</span>
                    <span class="p">),</span>
                <span class="p">)</span>
                <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">fetch_ready_async_reqs</span><span class="p">(</span>
                    <span class="n">return_obj_refs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="o">=</span><span class="mf">0.01</span>
                <span class="p">)</span>
                <span class="c1"># Make sure we properly time out if we have not received any results</span>
                <span class="c1"># for more than `time_out` seconds.</span>
                <span class="n">time_now</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">results</span> <span class="ow">and</span> <span class="n">time_now</span> <span class="o">-</span> <span class="n">t_last_result</span> <span class="o">&gt;</span> <span class="n">time_out</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="k">elif</span> <span class="n">results</span><span class="p">:</span>
                    <span class="n">t_last_result</span> <span class="o">=</span> <span class="n">time_now</span>
                <span class="k">for</span> <span class="n">wid</span><span class="p">,</span> <span class="p">(</span><span class="n">env_s</span><span class="p">,</span> <span class="n">ag_s</span><span class="p">,</span> <span class="n">met</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">iter</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">env_steps</span> <span class="o">+=</span> <span class="n">env_s</span>
                    <span class="n">agent_steps</span> <span class="o">+=</span> <span class="n">ag_s</span>
                    <span class="n">all_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">met</span><span class="p">)</span>
                    <span class="n">num_units_done</span> <span class="o">+=</span> <span class="p">(</span>
                        <span class="n">met</span><span class="p">[</span><span class="n">NUM_EPISODES</span><span class="p">]</span><span class="o">.</span><span class="n">peek</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span>
                        <span class="k">else</span> <span class="p">(</span>
                            <span class="n">env_s</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;env_steps&quot;</span> <span class="k">else</span> <span class="n">ag_s</span>
                        <span class="p">)</span>
                    <span class="p">)</span>
            <span class="c1"># Old API stack -&gt; RolloutWorkers return batches.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">units_per_healthy_remote_worker</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="mi">1</span>
                    <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span>
                    <span class="k">else</span> <span class="n">eval_cfg</span><span class="o">.</span><span class="n">rollout_fragment_length</span>
                    <span class="o">*</span> <span class="n">eval_cfg</span><span class="o">.</span><span class="n">num_envs_per_env_runner</span>
                <span class="p">)</span>
                <span class="c1"># Select proper number of evaluation workers for this round.</span>
                <span class="n">selected_eval_worker_ids</span> <span class="o">=</span> <span class="p">[</span>
                    <span class="n">worker_id</span>
                    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">worker_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">healthy_worker_ids</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="n">i</span> <span class="o">*</span> <span class="n">units_per_healthy_remote_worker</span> <span class="o">&lt;</span> <span class="n">units_left_to_do</span>
                <span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_worker_async</span><span class="p">(</span>
                    <span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span> <span class="n">w</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">(),</span> <span class="n">algo_iteration</span><span class="p">),</span>
                    <span class="n">remote_worker_ids</span><span class="o">=</span><span class="n">selected_eval_worker_ids</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">fetch_ready_async_reqs</span><span class="p">(</span>
                    <span class="n">return_obj_refs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="o">=</span><span class="mf">0.01</span>
                <span class="p">)</span>
                <span class="c1"># Make sure we properly time out if we have not received any results</span>
                <span class="c1"># for more than `time_out` seconds.</span>
                <span class="n">time_now</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">results</span> <span class="ow">and</span> <span class="n">time_now</span> <span class="o">-</span> <span class="n">t_last_result</span> <span class="o">&gt;</span> <span class="n">time_out</span><span class="p">:</span>
                    <span class="k">break</span>
                <span class="k">elif</span> <span class="n">results</span><span class="p">:</span>
                    <span class="n">t_last_result</span> <span class="o">=</span> <span class="n">time_now</span>
                <span class="k">for</span> <span class="n">wid</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">iter</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">env_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>
                    <span class="n">agent_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
                    <span class="n">all_metrics</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="p">:</span>
                        <span class="c1"># TODO: (kourosh) This approach will cause an OOM issue when</span>
                        <span class="c1">#  the dataset gets huge (should be ok for now).</span>
                        <span class="n">all_batches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

                <span class="c1"># 1 episode per returned batch.</span>
                <span class="k">if</span> <span class="n">unit</span> <span class="o">==</span> <span class="s2">&quot;episodes&quot;</span><span class="p">:</span>
                    <span class="n">num_units_done</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
                <span class="c1"># n timesteps per returned batch.</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">num_units_done</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">env_steps</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;env_steps&quot;</span>
                        <span class="k">else</span> <span class="n">agent_steps</span>
                    <span class="p">)</span>

            <span class="c1"># Update correct number of healthy remote workers.</span>
            <span class="n">num_healthy_workers</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">num_healthy_workers</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Calling `sample()` on your remote evaluation worker(s) &quot;</span>
                <span class="s2">&quot;resulted in all workers crashing! Make sure a) your environment is not&quot;</span>
                <span class="s2">&quot; too unstable, b) you have enough evaluation workers &quot;</span>
                <span class="s2">&quot;(`config.evaluation(evaluation_num_env_runners=...)`) to cover for &quot;</span>
                <span class="s2">&quot;occasional losses, and c) you use the `config.fault_tolerance(&quot;</span>
                <span class="s2">&quot;recreate_failed_env_runners=True)` setting.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="n">env_runner_results</span> <span class="o">=</span> <span class="n">summarize_episodes</span><span class="p">(</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">keep_custom_metrics</span><span class="o">=</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">num_episodes</span> <span class="o">=</span> <span class="n">env_runner_results</span><span class="p">[</span><span class="n">NUM_EPISODES</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">merge_and_log_n_dicts</span><span class="p">(</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">key</span><span class="o">=</span><span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">ENV_RUNNER_RESULTS</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">num_episodes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                <span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">ENV_RUNNER_RESULTS</span><span class="p">,</span> <span class="n">NUM_EPISODES</span><span class="p">),</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>
            <span class="n">env_runner_results</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Warn if results are empty, it could be that this is because the eval timesteps</span>
        <span class="c1"># are not enough to run through one full episode.</span>
        <span class="k">if</span> <span class="n">num_episodes</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This evaluation iteration resulted in an empty set of episode summary &quot;</span>
                <span class="s2">&quot;results! It&#39;s possible that your configured duration timesteps are not&quot;</span>
                <span class="s2">&quot; enough to finish even a single episode. Your have configured &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration_unit</span><span class="si">}</span><span class="s2">. For &#39;timesteps&#39;, try &quot;</span>
                <span class="s2">&quot;increasing this value via the `config.evaluation(evaluation_duration=&quot;</span>
                <span class="s2">&quot;...)` OR change the unit to &#39;episodes&#39; via `config.evaluation(&quot;</span>
                <span class="s2">&quot;evaluation_duration_unit=&#39;episodes&#39;)` OR try increasing the timeout &quot;</span>
                <span class="s2">&quot;threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR &quot;</span>
                <span class="s2">&quot;you can also set `config.evaluation_force_reset_envs_before_iteration`&quot;</span>
                <span class="s2">&quot; to False. However, keep in mind that in the latter case, the &quot;</span>
                <span class="s2">&quot;evaluation results may contain some episode stats generated with &quot;</span>
                <span class="s2">&quot;earlier weights versions.&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">env_runner_results</span><span class="p">,</span> <span class="n">env_steps</span><span class="p">,</span> <span class="n">agent_steps</span><span class="p">,</span> <span class="n">all_batches</span>

<div class="viewcode-block" id="Algorithm.restore_workers">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.restore_workers.html#ray.rllib.algorithms.algorithm.Algorithm.restore_workers">[docs]</a>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">restore_workers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">workers</span><span class="p">:</span> <span class="n">EnvRunnerGroup</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Try bringing back unhealthy EnvRunners and - if successful - sync with local.</span>

<span class="sd">        Algorithms that use custom EnvRunners may override this method to</span>
<span class="sd">        disable the default, and create custom restoration logics. Note that &quot;restoring&quot;</span>
<span class="sd">        does not include the actual restarting process, but merely what should happen</span>
<span class="sd">        after such a restart of a (previously failed) worker.</span>

<span class="sd">        Args:</span>
<span class="sd">            workers: The EnvRunnerGroup to restore. This may be the training or the</span>
<span class="sd">                evaluation EnvRunnerGroup.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># If `workers` is None, or</span>
        <span class="c1"># 1. `workers` (EnvRunnerGroup) does not have a local worker, and</span>
        <span class="c1"># 2. `self.env_runner_group` (EnvRunnerGroup used for training) does not have a</span>
        <span class="c1"># local EnvRunner -&gt; we don&#39;t have a local worker to get state from, so we can&#39;t</span>
        <span class="c1"># recover remote EnvRunners in this case.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">workers</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">workers</span><span class="o">.</span><span class="n">local_env_runner</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span>
        <span class="p">):</span>
            <span class="k">return</span>

        <span class="c1"># This is really cheap, since probe_unhealthy_workers() is a no-op</span>
        <span class="c1"># if there are no unhealthy workers.</span>
        <span class="n">restored</span> <span class="o">=</span> <span class="n">workers</span><span class="o">.</span><span class="n">probe_unhealthy_workers</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">restored</span><span class="p">:</span>
            <span class="c1"># Count the restored workers.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="s2">&quot;total_num_restored_workers&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="nb">len</span><span class="p">(</span><span class="n">restored</span><span class="p">)</span>

            <span class="n">from_worker</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">workers</span><span class="o">.</span><span class="n">local_env_runner</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span>
            <span class="p">)</span>
            <span class="c1"># Get the state of the correct (reference) worker. For example the local</span>
            <span class="c1"># worker of an EnvRunnerGroup.</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">from_worker</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>
            <span class="c1"># Take out (old) connector states from local worker&#39;s state.</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_connectors</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span>
            <span class="p">):</span>
                <span class="k">for</span> <span class="n">pol_states</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;policy_states&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
                    <span class="n">pol_states</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;connector_configs&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">state_ref</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

            <span class="c1"># By default, entire local worker state is synced after restoration</span>
            <span class="c1"># to bring these workers up to date.</span>
            <span class="n">workers</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span>
                <span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">w</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">state_ref</span><span class="p">)),</span>
                <span class="n">remote_worker_ids</span><span class="o">=</span><span class="n">restored</span><span class="p">,</span>
                <span class="c1"># Don&#39;t update the local EnvRunner, b/c it&#39;s the one we are synching</span>
                <span class="c1"># from.</span>
                <span class="n">local_env_runner</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="n">timeout_seconds</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">env_runner_restore_timeout_s</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="c1"># Fire the callback for re-created workers.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_workers_recreated</span><span class="p">(</span>
                <span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">worker_set</span><span class="o">=</span><span class="n">workers</span><span class="p">,</span>
                <span class="n">worker_ids</span><span class="o">=</span><span class="n">restored</span><span class="p">,</span>
                <span class="n">is_evaluation</span><span class="o">=</span><span class="n">workers</span><span class="o">.</span><span class="n">local_env_runner</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">in_evaluation</span><span class="p">,</span>
            <span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.training_step">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.training_step.html#ray.rllib.algorithms.algorithm.Algorithm.training_step">[docs]</a>
    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResultDict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Default single iteration logic of an algorithm.</span>

<span class="sd">        - Collect on-policy samples (SampleBatches) in parallel using the</span>
<span class="sd">          Algorithm&#39;s EnvRunners (@ray.remote).</span>
<span class="sd">        - Concatenate collected SampleBatches into one train batch.</span>
<span class="sd">        - Note that we may have more than one policy in the multi-agent case:</span>
<span class="sd">          Call the different policies&#39; `learn_on_batch` (simple optimizer) OR</span>
<span class="sd">          `load_batch_into_buffer` + `learn_on_loaded_batch` (multi-GPU</span>
<span class="sd">          optimizer) methods to calculate loss and update the model(s).</span>
<span class="sd">        - Return all collected metrics for the iteration.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The results dict from executing the training iteration.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;The `Algorithm.training_step()` default implementation no longer &quot;</span>
                <span class="s2">&quot;supports the old API stack! If you would like to continue &quot;</span>
                <span class="s2">&quot;using these &quot;</span>
                <span class="s2">&quot;old APIs with this default `training_step`, simply subclass &quot;</span>
                <span class="s2">&quot;`Algorithm` and override its `training_step` method (copy/paste the &quot;</span>
                <span class="s2">&quot;code and delete this error message).&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Collect a list of Episodes from EnvRunners until we reach the train batch</span>
        <span class="c1"># size.</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">ENV_RUNNER_SAMPLING_TIMER</span><span class="p">)):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;agent_steps&quot;</span><span class="p">:</span>
                <span class="n">episodes</span><span class="p">,</span> <span class="n">env_runner_results</span> <span class="o">=</span> <span class="n">synchronous_parallel_sample</span><span class="p">(</span>
                    <span class="n">worker_set</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">,</span>
                    <span class="n">max_agent_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">total_train_batch_size</span><span class="p">,</span>
                    <span class="n">sample_timeout_s</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sample_timeout_s</span><span class="p">,</span>
                    <span class="n">_uses_new_env_runners</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">_return_metrics</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">episodes</span><span class="p">,</span> <span class="n">env_runner_results</span> <span class="o">=</span> <span class="n">synchronous_parallel_sample</span><span class="p">(</span>
                    <span class="n">worker_set</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">,</span>
                    <span class="n">max_env_steps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">total_train_batch_size</span><span class="p">,</span>
                    <span class="n">sample_timeout_s</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">sample_timeout_s</span><span class="p">,</span>
                    <span class="n">_uses_new_env_runners</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">_return_metrics</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
        <span class="c1"># Reduce EnvRunner metrics over the n EnvRunners.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">merge_and_log_n_dicts</span><span class="p">(</span><span class="n">env_runner_results</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">ENV_RUNNER_RESULTS</span><span class="p">)</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">LEARNER_UPDATE_TIMER</span><span class="p">)):</span>
            <span class="n">learner_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">update_from_episodes</span><span class="p">(</span>
                <span class="n">episodes</span><span class="o">=</span><span class="n">episodes</span><span class="p">,</span>
                <span class="n">timesteps</span><span class="o">=</span><span class="p">{</span>
                    <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">:</span> <span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span><span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">)</span>
                    <span class="p">),</span>
                <span class="p">},</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">learner_results</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">LEARNER_RESULTS</span><span class="p">)</span>

        <span class="c1"># Update weights - after learning on the local worker - on all</span>
        <span class="c1"># remote workers (only those RLModules that were actually trained).</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">log_time</span><span class="p">((</span><span class="n">TIMERS</span><span class="p">,</span> <span class="n">SYNCH_WORKER_WEIGHTS_TIMER</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">policies</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">learner_results</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="o">-</span> <span class="p">{</span><span class="n">ALL_MODULES</span><span class="p">}),</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Return reduced metrics (Dict).</span>
        <span class="c1"># Note that these training results will further be processed (e.g.</span>
        <span class="c1"># merged with evaluation results) before eventually being returned from the</span>
        <span class="c1"># encapsulating `Algorithm.step()` call as a plain nested ResultDict.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">reduce</span><span class="p">()</span></div>


<div class="viewcode-block" id="Algorithm.get_module">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_module.html#ray.rllib.algorithms.algorithm.Algorithm.get_module">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">get_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span> <span class="o">=</span> <span class="n">DEFAULT_MODULE_ID</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RLModule</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the (single-agent) RLModule with `model_id` (None if ID not found).</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: ID of the (single-agent) RLModule to return from the MARLModule</span>
<span class="sd">                used by the local EnvRunner.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The SingleAgentRLModule sitting under the ModuleID key inside the</span>
<span class="sd">            local worker&#39;s (EnvRunner&#39;s) MARLModule.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">module</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="o">.</span><span class="n">module</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">MultiRLModule</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">module</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">module</span></div>


<div class="viewcode-block" id="Algorithm.add_module">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.add_module.html#ray.rllib.algorithms.algorithm.Algorithm.add_module">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">add_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span><span class="p">,</span>
        <span class="n">module_spec</span><span class="p">:</span> <span class="n">RLModuleSpec</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">config_overrides</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">new_agent_to_module_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AgentToModuleMappingFn</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">new_should_module_be_updated</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ShouldModuleBeUpdatedFn</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_to_learners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">add_to_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">add_to_eval_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">MultiRLModuleSpec</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Adds a new (single-agent) RLModule to this Algorithm&#39;s MARLModule.</span>

<span class="sd">        Note that an Algorithm has up to 3 different components to which to add</span>
<span class="sd">        the new module to: The LearnerGroup (with n Learners), the EnvRunnerGroup</span>
<span class="sd">        (with m EnvRunners plus a local one) and - if applicable - the eval</span>
<span class="sd">        EnvRunnerGroup (with o EnvRunners plus a local one).</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: ID of the RLModule to add to the MARLModule.</span>
<span class="sd">                IMPORTANT: Must not contain characters that</span>
<span class="sd">                are also not allowed in Unix/Win filesystems, such as: `&lt;&gt;:&quot;/|?*`,</span>
<span class="sd">                or a dot, space or backslash at the end of the ID.</span>
<span class="sd">            module_spec: The SingleAgentRLModuleSpec to use for constructing the new</span>
<span class="sd">                RLModule.</span>
<span class="sd">            config_overrides: The `AlgorithmConfig` overrides that should apply to</span>
<span class="sd">                the new Module, if any.</span>
<span class="sd">            new_agent_to_module_mapping_fn: An optional (updated) AgentID to ModuleID</span>
<span class="sd">                mapping function to use from here on. Note that already ongoing</span>
<span class="sd">                episodes will not change their mapping but will use the old mapping till</span>
<span class="sd">                the end of the episode.</span>
<span class="sd">            new_should_module_be_updated: An optional sequence of ModuleIDs or a</span>
<span class="sd">                callable taking ModuleID and SampleBatchType and returning whether the</span>
<span class="sd">                ModuleID should be updated (trained).</span>
<span class="sd">                If None, will keep the existing setup in place. RLModules,</span>
<span class="sd">                whose IDs are not in the list (or for which the callable</span>
<span class="sd">                returns False) will not be updated.</span>
<span class="sd">            add_to_learners: Whether to add the new RLModule to the LearnerGroup</span>
<span class="sd">                (with its n Learners).</span>
<span class="sd">            add_to_env_runners: Whether to add the new RLModule to the EnvRunnerGroup</span>
<span class="sd">                (with its m EnvRunners plus the local one).</span>
<span class="sd">            add_to_eval_env_runners: Whether to add the new RLModule to the eval</span>
<span class="sd">                EnvRunnerGroup (with its o EnvRunners plus the local one).</span>

<span class="sd">        Returns:</span>
<span class="sd">            The new MultiAgentRLModuleSpec (after the RLModule has been added).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">validate_module_id</span><span class="p">(</span><span class="n">module_id</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># The to-be-returned new MultiAgentRLModuleSpec.</span>
        <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">is_multi_agent</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t add a new RLModule to a single-agent setup! Make sure that your &quot;</span>
                <span class="s2">&quot;setup is already initially multi-agent by either defining &gt;1 &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;RLModules in your `rl_module_spec` or assigning a ModuleID other &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;than </span><span class="si">{</span><span class="n">DEFAULT_MODULE_ID</span><span class="si">}</span><span class="s2"> to your (only) RLModule.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">any</span><span class="p">([</span><span class="n">add_to_learners</span><span class="p">,</span> <span class="n">add_to_env_runners</span><span class="p">,</span> <span class="n">add_to_eval_env_runners</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;At least one of `add_to_learners`, `add_to_env_runners`, or &quot;</span>
                <span class="s2">&quot;`add_to_eval_env_runners` must be set to True!&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Add to Learners and sync weights.</span>
        <span class="k">if</span> <span class="n">add_to_learners</span><span class="p">:</span>
            <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">,</span>
                <span class="n">module_spec</span><span class="o">=</span><span class="n">module_spec</span><span class="p">,</span>
                <span class="n">config_overrides</span><span class="o">=</span><span class="n">config_overrides</span><span class="p">,</span>
                <span class="n">new_should_module_be_updated</span><span class="o">=</span><span class="n">new_should_module_be_updated</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Change our config (AlgorithmConfig) to contain the new Module.</span>
        <span class="c1"># TODO (sven): This is a hack to manipulate the AlgorithmConfig directly,</span>
        <span class="c1">#  but we&#39;ll deprecate config.policies soon anyway.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_is_frozen</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">policies</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">PolicySpec</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">config_overrides</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span>
                <span class="n">algorithm_config_overrides_per_module</span><span class="o">=</span><span class="p">{</span><span class="n">module_id</span><span class="p">:</span> <span class="n">config_overrides</span><span class="p">}</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">new_agent_to_module_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span><span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">new_agent_to_module_mapping_fn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rl_module</span><span class="p">(</span><span class="n">rl_module_spec</span><span class="o">=</span><span class="n">multi_rl_module_spec</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_should_module_be_updated</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span><span class="n">policies_to_train</span><span class="o">=</span><span class="n">new_should_module_be_updated</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">_add</span><span class="p">(</span><span class="n">_env_runner</span><span class="p">,</span> <span class="n">_module_spec</span><span class="o">=</span><span class="n">module_spec</span><span class="p">):</span>
            <span class="c1"># Add the RLModule to the existing one on the EnvRunner.</span>
            <span class="n">_env_runner</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">add_module</span><span class="p">(</span>
                <span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="n">_module_spec</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="c1"># Update the `agent_to_module_mapping_fn` on the EnvRunner.</span>
            <span class="k">if</span> <span class="n">new_agent_to_module_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_env_runner</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span>
                    <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">new_agent_to_module_mapping_fn</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">MultiRLModuleSpec</span><span class="o">.</span><span class="n">from_module</span><span class="p">(</span><span class="n">_env_runner</span><span class="o">.</span><span class="n">module</span><span class="p">)</span>

        <span class="c1"># Add to (training) EnvRunners and sync weights.</span>
        <span class="k">if</span> <span class="n">add_to_env_runners</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">multi_rl_module_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span><span class="n">_add</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span><span class="n">_add</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># Add to eval EnvRunners and sync weights.</span>
        <span class="k">if</span> <span class="n">add_to_eval_env_runners</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">multi_rl_module_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span><span class="n">_add</span><span class="p">)[</span>
                    <span class="mi">0</span>
                <span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span><span class="n">_add</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">multi_rl_module_spec</span></div>


<div class="viewcode-block" id="Algorithm.remove_module">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.remove_module.html#ray.rllib.algorithms.algorithm.Algorithm.remove_module">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">remove_module</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module_id</span><span class="p">:</span> <span class="n">ModuleID</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">new_agent_to_module_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AgentToModuleMappingFn</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">new_should_module_be_updated</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">ShouldModuleBeUpdatedFn</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">remove_from_learners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">remove_from_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">remove_from_eval_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Removes a new (single-agent) RLModule from this Algorithm&#39;s MARLModule.</span>

<span class="sd">        Args:</span>
<span class="sd">            module_id: ID of the RLModule to remove from the MARLModule.</span>
<span class="sd">                IMPORTANT: Must not contain characters that</span>
<span class="sd">                are also not allowed in Unix/Win filesystems, such as: `&lt;&gt;:&quot;/|?*`,</span>
<span class="sd">                or a dot, space or backslash at the end of the ID.</span>
<span class="sd">            new_agent_to_module_mapping_fn: An optional (updated) AgentID to ModuleID</span>
<span class="sd">                mapping function to use from here on. Note that already ongoing</span>
<span class="sd">                episodes will not change their mapping but will use the old mapping till</span>
<span class="sd">                the end of the episode.</span>
<span class="sd">            new_should_module_be_updated: An optional sequence of ModuleIDs or a</span>
<span class="sd">                callable taking ModuleID and SampleBatchType and returning whether the</span>
<span class="sd">                ModuleID should be updated (trained).</span>
<span class="sd">                If None, will keep the existing setup in place. RLModules,</span>
<span class="sd">                whose IDs are not in the list (or for which the callable</span>
<span class="sd">                returns False) will not be updated.</span>
<span class="sd">            remove_from_learners: Whether to remove the RLModule from the LearnerGroup</span>
<span class="sd">                (with its n Learners).</span>
<span class="sd">            remove_from_env_runners: Whether to remove the RLModule from the</span>
<span class="sd">                EnvRunnerGroup (with its m EnvRunners plus the local one).</span>
<span class="sd">            remove_from_eval_env_runners: Whether to remove the RLModule from the eval</span>
<span class="sd">                EnvRunnerGroup (with its o EnvRunners plus the local one).</span>

<span class="sd">        Returns:</span>
<span class="sd">            The new MultiAgentRLModuleSpec (after the RLModule has been removed).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># The to-be-returned new MultiAgentRLModuleSpec.</span>
        <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Remove RLModule from the LearnerGroup.</span>
        <span class="k">if</span> <span class="n">remove_from_learners</span><span class="p">:</span>
            <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">remove_module</span><span class="p">(</span>
                <span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">,</span>
                <span class="n">new_should_module_be_updated</span><span class="o">=</span><span class="n">new_should_module_be_updated</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Change our config (AlgorithmConfig) with the Module removed.</span>
        <span class="c1"># TODO (sven): This is a hack to manipulate the AlgorithmConfig directly,</span>
        <span class="c1">#  but we&#39;ll deprecate config.policies soon anyway.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_is_frozen</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">policies</span><span class="p">[</span><span class="n">module_id</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">algorithm_config_overrides_per_module</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">module_id</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_agent_to_module_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span><span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">new_agent_to_module_mapping_fn</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">rl_module</span><span class="p">(</span><span class="n">rl_module_spec</span><span class="o">=</span><span class="n">multi_rl_module_spec</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">new_should_module_be_updated</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span><span class="n">policies_to_train</span><span class="o">=</span><span class="n">new_should_module_be_updated</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="k">def</span> <span class="nf">_remove</span><span class="p">(</span><span class="n">_env_runner</span><span class="p">):</span>
            <span class="c1"># Remove the RLModule from the existing one on the EnvRunner.</span>
            <span class="n">_env_runner</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">remove_module</span><span class="p">(</span><span class="n">module_id</span><span class="o">=</span><span class="n">module_id</span><span class="p">)</span>
            <span class="c1"># Update the `agent_to_module_mapping_fn` on the EnvRunner.</span>
            <span class="k">if</span> <span class="n">new_agent_to_module_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_env_runner</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span>
                    <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">new_agent_to_module_mapping_fn</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">MultiRLModuleSpec</span><span class="o">.</span><span class="n">from_module</span><span class="p">(</span><span class="n">_env_runner</span><span class="o">.</span><span class="n">module</span><span class="p">)</span>

        <span class="c1"># Remove from (training) EnvRunners and sync weights.</span>
        <span class="k">if</span> <span class="n">remove_from_env_runners</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">multi_rl_module_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span><span class="n">_remove</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span><span class="n">_remove</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Remove from (eval) EnvRunners and sync weights.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">remove_from_eval_env_runners</span> <span class="ow">is</span> <span class="kc">True</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="k">if</span> <span class="n">multi_rl_module_spec</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">multi_rl_module_spec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span>
                    <span class="n">_remove</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span><span class="n">_remove</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">multi_rl_module_spec</span></div>


<div class="viewcode-block" id="Algorithm.get_policy">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_policy.html#ray.rllib.algorithms.algorithm.Algorithm.get_policy">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span> <span class="nf">get_policy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Policy</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return policy for the specified id, or None.</span>

<span class="sd">        Args:</span>
<span class="sd">            policy_id: ID of the policy to return.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.get_weights">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.get_weights.html#ray.rllib.algorithms.algorithm.Algorithm.get_weights">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">get_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">policies</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a dict mapping Module/Policy IDs to weights.</span>

<span class="sd">        Args:</span>
<span class="sd">            policies: Optional list of policies to return weights for,</span>
<span class="sd">                or None for all policies.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># New API stack (get weights from LearnerGroup).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">get_weights</span><span class="p">(</span><span class="n">module_ids</span><span class="o">=</span><span class="n">policies</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="o">.</span><span class="n">get_weights</span><span class="p">(</span><span class="n">policies</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.set_weights">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.set_weights.html#ray.rllib.algorithms.algorithm.Algorithm.set_weights">[docs]</a>
    <span class="nd">@PublicAPI</span>
    <span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set RLModule/Policy weights by Module/Policy ID.</span>

<span class="sd">        Args:</span>
<span class="sd">            weights: Dict mapping ModuleID/PolicyID to weights.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># New API stack -&gt; Use `set_state` API and specify the LearnerGroup state in the</span>
        <span class="c1"># call, which will automatically take care of weight synching to all EnvRunners.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">:</span> <span class="p">{</span>
                        <span class="n">COMPONENT_LEARNER</span><span class="p">:</span> <span class="p">{</span>
                            <span class="n">COMPONENT_RL_MODULE</span><span class="p">:</span> <span class="n">weights</span><span class="p">,</span>
                        <span class="p">},</span>
                    <span class="p">},</span>
                <span class="p">},</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">weights</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.compute_single_action">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.compute_single_action.html#ray.rllib.algorithms.algorithm.Algorithm.compute_single_action">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span> <span class="nf">compute_single_action</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">observation</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prev_action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prev_reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvInfoDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">input_dict</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatch</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
        <span class="n">full_fetch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">explore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">episode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Episode</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">unsquash_action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">clip_action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Kwargs placeholder for future compatibility.</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span>
        <span class="n">TensorStructType</span><span class="p">,</span>
        <span class="n">Tuple</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">TensorType</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">TensorType</span><span class="p">]],</span>
    <span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes an action for the specified policy on the local worker.</span>

<span class="sd">        Note that you can also access the policy object through</span>
<span class="sd">        self.get_policy(policy_id) and call compute_single_action() on it</span>
<span class="sd">        directly.</span>

<span class="sd">        Args:</span>
<span class="sd">            observation: Single (unbatched) observation from the</span>
<span class="sd">                environment.</span>
<span class="sd">            state: List of all RNN hidden (single, unbatched) state tensors.</span>
<span class="sd">            prev_action: Single (unbatched) previous action value.</span>
<span class="sd">            prev_reward: Single (unbatched) previous reward value.</span>
<span class="sd">            info: Env info dict, if any.</span>
<span class="sd">            input_dict: An optional SampleBatch that holds all the values</span>
<span class="sd">                for: obs, state, prev_action, and prev_reward, plus maybe</span>
<span class="sd">                custom defined views of the current env trajectory. Note</span>
<span class="sd">                that only one of `obs` or `input_dict` must be non-None.</span>
<span class="sd">            policy_id: Policy to query (only applies to multi-agent).</span>
<span class="sd">                Default: &quot;default_policy&quot;.</span>
<span class="sd">            full_fetch: Whether to return extra action fetch results.</span>
<span class="sd">                This is always set to True if `state` is specified.</span>
<span class="sd">            explore: Whether to apply exploration to the action.</span>
<span class="sd">                Default: None -&gt; use self.config.explore.</span>
<span class="sd">            timestep: The current (sampling) time step.</span>
<span class="sd">            episode: This provides access to all of the internal episodes&#39;</span>
<span class="sd">                state, which may be useful for model-based or multi-agent</span>
<span class="sd">                algorithms.</span>
<span class="sd">            unsquash_action: Should actions be unsquashed according to the</span>
<span class="sd">                env&#39;s/Policy&#39;s action space? If None, use the value of</span>
<span class="sd">                self.config.normalize_actions.</span>
<span class="sd">            clip_action: Should actions be clipped according to the</span>
<span class="sd">                env&#39;s/Policy&#39;s action space? If None, use the value of</span>
<span class="sd">                self.config.clip_actions.</span>

<span class="sd">        Keyword Args:</span>
<span class="sd">            kwargs: forward compatibility placeholder</span>

<span class="sd">        Returns:</span>
<span class="sd">            The computed action if full_fetch=False, or a tuple of a) the</span>
<span class="sd">            full output of policy.compute_actions() if full_fetch=True</span>
<span class="sd">            or we have an RNN-based Policy.</span>

<span class="sd">        Raises:</span>
<span class="sd">            KeyError: If the `policy_id` cannot be found in this Algorithm&#39;s local</span>
<span class="sd">                worker.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># `unsquash_action` is None: Use value of config[&#39;normalize_actions&#39;].</span>
        <span class="k">if</span> <span class="n">unsquash_action</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">unsquash_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">normalize_actions</span>
        <span class="c1"># `clip_action` is None: Use value of config[&#39;clip_actions&#39;].</span>
        <span class="k">elif</span> <span class="n">clip_action</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">clip_action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">clip_actions</span>

        <span class="c1"># User provided an input-dict: Assert that `obs`, `prev_a|r`, `state`</span>
        <span class="c1"># are all None.</span>
        <span class="n">err_msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="s2">&quot;Provide either `input_dict` OR [`observation`, ...] as &quot;</span>
            <span class="s2">&quot;args to `Algorithm.compute_single_action()`!&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">input_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">observation</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">prev_action</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">prev_reward</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">state</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="n">err_msg</span>
            <span class="n">observation</span> <span class="o">=</span> <span class="n">input_dict</span><span class="p">[</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">observation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="n">err_msg</span>

        <span class="c1"># Get the policy to compute the action for (in the multi-agent case,</span>
        <span class="c1"># Algorithm may hold &gt;1 policies).</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">policy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;PolicyID &#39;</span><span class="si">{</span><span class="n">policy_id</span><span class="si">}</span><span class="s2">&#39; not found in PolicyMap of the &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Algorithm&#39;s local worker!&quot;</span>
            <span class="p">)</span>
        <span class="n">local_worker</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;enable_connectors&quot;</span><span class="p">):</span>
            <span class="c1"># Check the preprocessor and preprocess, if necessary.</span>
            <span class="n">pp</span> <span class="o">=</span> <span class="n">local_worker</span><span class="o">.</span><span class="n">preprocessors</span><span class="p">[</span><span class="n">policy_id</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">pp</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">pp</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">!=</span> <span class="s2">&quot;NoPreprocessor&quot;</span><span class="p">:</span>
                <span class="n">observation</span> <span class="o">=</span> <span class="n">pp</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
            <span class="n">observation</span> <span class="o">=</span> <span class="n">local_worker</span><span class="o">.</span><span class="n">filters</span><span class="p">[</span><span class="n">policy_id</span><span class="p">](</span><span class="n">observation</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Just preprocess observations, similar to how it used to be done before.</span>
            <span class="n">pp</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">agent_connectors</span><span class="p">[</span><span class="n">ObsPreprocessorConnector</span><span class="p">]</span>

            <span class="c1"># convert the observation to array if possible</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">observation</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="nb">dict</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="n">observation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span>
                <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Observation type </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">observation</span><span class="p">)</span><span class="si">}</span><span class="s2"> cannot be converted to &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;np.ndarray.&quot;</span>
                    <span class="p">)</span>
            <span class="k">if</span> <span class="n">pp</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pp</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Only one preprocessor should be in the pipeline&quot;</span>
                <span class="n">pp</span> <span class="o">=</span> <span class="n">pp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="n">pp</span><span class="o">.</span><span class="n">is_identity</span><span class="p">():</span>
                    <span class="c1"># Note(Kourosh): This call will leave the policy&#39;s connector</span>
                    <span class="c1"># in eval mode. would that be a problem?</span>
                    <span class="n">pp</span><span class="o">.</span><span class="n">in_eval</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">observation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">_input_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">:</span> <span class="n">observation</span><span class="p">}</span>
                    <span class="k">elif</span> <span class="n">input_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">_input_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">:</span> <span class="n">input_dict</span><span class="p">[</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">]}</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;Either observation or input_dict must be provided.&quot;</span>
                        <span class="p">)</span>

                    <span class="c1"># TODO (Kourosh): Create a new util method for algorithm that</span>
                    <span class="c1"># computes actions based on raw inputs from env and can keep track</span>
                    <span class="c1"># of its own internal state.</span>
                    <span class="n">acd</span> <span class="o">=</span> <span class="n">AgentConnectorDataType</span><span class="p">(</span><span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">,</span> <span class="n">_input_dict</span><span class="p">)</span>
                    <span class="c1"># make sure the state is reset since we are only applying the</span>
                    <span class="c1"># preprocessor</span>
                    <span class="n">pp</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">env_id</span><span class="o">=</span><span class="s2">&quot;0&quot;</span><span class="p">)</span>
                    <span class="n">ac_o</span> <span class="o">=</span> <span class="n">pp</span><span class="p">([</span><span class="n">acd</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
                    <span class="n">observation</span> <span class="o">=</span> <span class="n">ac_o</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">]</span>

        <span class="c1"># Input-dict.</span>
        <span class="k">if</span> <span class="n">input_dict</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">]</span> <span class="o">=</span> <span class="n">observation</span>
            <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">extra</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">compute_single_action</span><span class="p">(</span>
                <span class="n">input_dict</span><span class="o">=</span><span class="n">input_dict</span><span class="p">,</span>
                <span class="n">explore</span><span class="o">=</span><span class="n">explore</span><span class="p">,</span>
                <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">,</span>
                <span class="n">episode</span><span class="o">=</span><span class="n">episode</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># Individual args.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">extra</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">compute_single_action</span><span class="p">(</span>
                <span class="n">obs</span><span class="o">=</span><span class="n">observation</span><span class="p">,</span>
                <span class="n">state</span><span class="o">=</span><span class="n">state</span><span class="p">,</span>
                <span class="n">prev_action</span><span class="o">=</span><span class="n">prev_action</span><span class="p">,</span>
                <span class="n">prev_reward</span><span class="o">=</span><span class="n">prev_reward</span><span class="p">,</span>
                <span class="n">info</span><span class="o">=</span><span class="n">info</span><span class="p">,</span>
                <span class="n">explore</span><span class="o">=</span><span class="n">explore</span><span class="p">,</span>
                <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">,</span>
                <span class="n">episode</span><span class="o">=</span><span class="n">episode</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># If we work in normalized action space (normalize_actions=True),</span>
        <span class="c1"># we re-translate here into the env&#39;s action space.</span>
        <span class="k">if</span> <span class="n">unsquash_action</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">unsquash_action</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">action_space_struct</span><span class="p">)</span>
        <span class="c1"># Clip, according to env&#39;s action space.</span>
        <span class="k">elif</span> <span class="n">clip_action</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">clip_action</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">action_space_struct</span><span class="p">)</span>

        <span class="c1"># Return 3-Tuple: Action, states, and extra-action fetches.</span>
        <span class="k">if</span> <span class="n">state</span> <span class="ow">or</span> <span class="n">full_fetch</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">extra</span>
        <span class="c1"># Ensure backward compatibility.</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">action</span></div>


<div class="viewcode-block" id="Algorithm.compute_actions">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.compute_actions.html#ray.rllib.algorithms.algorithm.Algorithm.compute_actions">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span> <span class="nf">compute_actions</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">observations</span><span class="p">:</span> <span class="n">TensorStructType</span><span class="p">,</span>
        <span class="n">state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">prev_action</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">prev_reward</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TensorStructType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnvInfoDict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
        <span class="n">full_fetch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">explore</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestep</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">episodes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Episode</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">unsquash_actions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">clip_actions</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Computes an action for the specified policy on the local Worker.</span>

<span class="sd">        Note that you can also access the policy object through</span>
<span class="sd">        self.get_policy(policy_id) and call compute_actions() on it directly.</span>

<span class="sd">        Args:</span>
<span class="sd">            observation: Observation from the environment.</span>
<span class="sd">            state: RNN hidden state, if any. If state is not None,</span>
<span class="sd">                then all of compute_single_action(...) is returned</span>
<span class="sd">                (computed action, rnn state(s), logits dictionary).</span>
<span class="sd">                Otherwise compute_single_action(...)[0] is returned</span>
<span class="sd">                (computed action).</span>
<span class="sd">            prev_action: Previous action value, if any.</span>
<span class="sd">            prev_reward: Previous reward, if any.</span>
<span class="sd">            info: Env info dict, if any.</span>
<span class="sd">            policy_id: Policy to query (only applies to multi-agent).</span>
<span class="sd">            full_fetch: Whether to return extra action fetch results.</span>
<span class="sd">                This is always set to True if RNN state is specified.</span>
<span class="sd">            explore: Whether to pick an exploitation or exploration</span>
<span class="sd">                action (default: None -&gt; use self.config.explore).</span>
<span class="sd">            timestep: The current (sampling) time step.</span>
<span class="sd">            episodes: This provides access to all of the internal episodes&#39;</span>
<span class="sd">                state, which may be useful for model-based or multi-agent</span>
<span class="sd">                algorithms.</span>
<span class="sd">            unsquash_actions: Should actions be unsquashed according</span>
<span class="sd">                to the env&#39;s/Policy&#39;s action space? If None, use</span>
<span class="sd">                self.config.normalize_actions.</span>
<span class="sd">            clip_actions: Should actions be clipped according to the</span>
<span class="sd">                env&#39;s/Policy&#39;s action space? If None, use</span>
<span class="sd">                self.config.clip_actions.</span>

<span class="sd">        Keyword Args:</span>
<span class="sd">            kwargs: forward compatibility placeholder</span>

<span class="sd">        Returns:</span>
<span class="sd">            The computed action if full_fetch=False, or a tuple consisting of</span>
<span class="sd">            the full output of policy.compute_actions_from_input_dict() if</span>
<span class="sd">            full_fetch=True or we have an RNN-based Policy.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># `unsquash_actions` is None: Use value of config[&#39;normalize_actions&#39;].</span>
        <span class="k">if</span> <span class="n">unsquash_actions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">unsquash_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">normalize_actions</span>
        <span class="c1"># `clip_actions` is None: Use value of config[&#39;clip_actions&#39;].</span>
        <span class="k">elif</span> <span class="n">clip_actions</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">clip_actions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">clip_actions</span>

        <span class="c1"># Preprocess obs and states.</span>
        <span class="n">state_defined</span> <span class="o">=</span> <span class="n">state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span>
        <span class="n">filtered_obs</span><span class="p">,</span> <span class="n">filtered_state</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">agent_id</span><span class="p">,</span> <span class="n">ob</span> <span class="ow">in</span> <span class="n">observations</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">worker</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span>
            <span class="k">if</span> <span class="n">worker</span><span class="o">.</span><span class="n">preprocessors</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">preprocessed</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">preprocessors</span><span class="p">[</span><span class="n">policy_id</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">ob</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">preprocessed</span> <span class="o">=</span> <span class="n">ob</span>
            <span class="n">filtered</span> <span class="o">=</span> <span class="n">worker</span><span class="o">.</span><span class="n">filters</span><span class="p">[</span><span class="n">policy_id</span><span class="p">](</span><span class="n">preprocessed</span><span class="p">,</span> <span class="n">update</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">filtered_obs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">filtered</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">elif</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                <span class="n">filtered_state</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">agent_id</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">filtered_state</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">get_initial_state</span><span class="p">())</span>

        <span class="c1"># Batch obs and states</span>
        <span class="n">obs_batch</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">filtered_obs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">state</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">filtered_state</span><span class="p">))</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">state</span><span class="p">]</span>

        <span class="n">input_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">Columns</span><span class="o">.</span><span class="n">OBS</span><span class="p">:</span> <span class="n">obs_batch</span><span class="p">}</span>

        <span class="c1"># prev_action and prev_reward can be None, np.ndarray, or tensor-like structure.</span>
        <span class="c1"># Explicitly check for None here to avoid the error message &quot;The truth value of</span>
        <span class="c1"># an array with more than one element is ambiguous.&quot;, when np arrays are passed</span>
        <span class="c1"># as arguments.</span>
        <span class="k">if</span> <span class="n">prev_action</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">PREV_ACTIONS</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_action</span>
        <span class="k">if</span> <span class="n">prev_reward</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">SampleBatch</span><span class="o">.</span><span class="n">PREV_REWARDS</span><span class="p">]</span> <span class="o">=</span> <span class="n">prev_reward</span>
        <span class="k">if</span> <span class="n">info</span><span class="p">:</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="n">Columns</span><span class="o">.</span><span class="n">INFOS</span><span class="p">]</span> <span class="o">=</span> <span class="n">info</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
            <span class="n">input_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;state_in_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">s</span>

        <span class="c1"># Batch compute actions</span>
        <span class="n">actions</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">compute_actions_from_input_dict</span><span class="p">(</span>
            <span class="n">input_dict</span><span class="o">=</span><span class="n">input_dict</span><span class="p">,</span>
            <span class="n">explore</span><span class="o">=</span><span class="n">explore</span><span class="p">,</span>
            <span class="n">timestep</span><span class="o">=</span><span class="n">timestep</span><span class="p">,</span>
            <span class="n">episodes</span><span class="o">=</span><span class="n">episodes</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Unbatch actions for the environment into a multi-agent dict.</span>
        <span class="n">single_actions</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">unbatch</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">observations</span><span class="p">,</span> <span class="n">single_actions</span><span class="p">):</span>
            <span class="c1"># If we work in normalized action space (normalize_actions=True),</span>
            <span class="c1"># we re-translate here into the env&#39;s action space.</span>
            <span class="k">if</span> <span class="n">unsquash_actions</span><span class="p">:</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">unsquash_action</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">action_space_struct</span><span class="p">)</span>
            <span class="c1"># Clip, according to env&#39;s action space.</span>
            <span class="k">elif</span> <span class="n">clip_actions</span><span class="p">:</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">space_utils</span><span class="o">.</span><span class="n">clip_action</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">policy</span><span class="o">.</span><span class="n">action_space_struct</span><span class="p">)</span>
            <span class="n">actions</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span>

        <span class="c1"># Unbatch states into a multi-agent dict.</span>
        <span class="n">unbatched_states</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">agent_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">observations</span><span class="p">):</span>
            <span class="n">unbatched_states</span><span class="p">[</span><span class="n">agent_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">states</span><span class="p">]</span>

        <span class="c1"># Return only actions or full tuple</span>
        <span class="k">if</span> <span class="n">state_defined</span> <span class="ow">or</span> <span class="n">full_fetch</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">actions</span><span class="p">,</span> <span class="n">unbatched_states</span><span class="p">,</span> <span class="n">infos</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">actions</span></div>


<div class="viewcode-block" id="Algorithm.add_policy">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.add_policy.html#ray.rllib.algorithms.algorithm.Algorithm.add_policy">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span> <span class="nf">add_policy</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span><span class="p">,</span>
        <span class="n">policy_cls</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">[</span><span class="n">Policy</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">observation_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">gym</span><span class="o">.</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_state</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PolicyState</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">,</span> <span class="n">EpisodeID</span><span class="p">],</span> <span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policies_to_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">Collection</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">],</span>
                <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatchType</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">add_to_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">add_to_eval_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">module_spec</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">RLModuleSpec</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Deprecated arg.</span>
        <span class="n">evaluation_workers</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="n">add_to_learners</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Adds a new policy to this Algorithm.</span>

<span class="sd">        Args:</span>
<span class="sd">            policy_id: ID of the policy to add.</span>
<span class="sd">                IMPORTANT: Must not contain characters that</span>
<span class="sd">                are also not allowed in Unix/Win filesystems, such as: `&lt;&gt;:&quot;/|?*`,</span>
<span class="sd">                or a dot, space or backslash at the end of the ID.</span>
<span class="sd">            policy_cls: The Policy class to use for constructing the new Policy.</span>
<span class="sd">                Note: Only one of `policy_cls` or `policy` must be provided.</span>
<span class="sd">            policy: The Policy instance to add to this algorithm. If not None, the</span>
<span class="sd">                given Policy object will be directly inserted into the Algorithm&#39;s</span>
<span class="sd">                local worker and clones of that Policy will be created on all remote</span>
<span class="sd">                workers as well as all evaluation workers.</span>
<span class="sd">                Note: Only one of `policy_cls` or `policy` must be provided.</span>
<span class="sd">            observation_space: The observation space of the policy to add.</span>
<span class="sd">                If None, try to infer this space from the environment.</span>
<span class="sd">            action_space: The action space of the policy to add.</span>
<span class="sd">                If None, try to infer this space from the environment.</span>
<span class="sd">            config: The config object or overrides for the policy to add.</span>
<span class="sd">            policy_state: Optional state dict to apply to the new</span>
<span class="sd">                policy instance, right after its construction.</span>
<span class="sd">            policy_mapping_fn: An optional (updated) policy mapping function</span>
<span class="sd">                to use from here on. Note that already ongoing episodes will</span>
<span class="sd">                not change their mapping but will use the old mapping till</span>
<span class="sd">                the end of the episode.</span>
<span class="sd">            policies_to_train: An optional list of policy IDs to be trained</span>
<span class="sd">                or a callable taking PolicyID and SampleBatchType and</span>
<span class="sd">                returning a bool (trainable or not?).</span>
<span class="sd">                If None, will keep the existing setup in place. Policies,</span>
<span class="sd">                whose IDs are not in the list (or for which the callable</span>
<span class="sd">                returns False) will not be updated.</span>
<span class="sd">            add_to_env_runners: Whether to add the new RLModule to the EnvRunnerGroup</span>
<span class="sd">                (with its m EnvRunners plus the local one).</span>
<span class="sd">            add_to_eval_env_runners: Whether to add the new RLModule to the eval</span>
<span class="sd">                EnvRunnerGroup (with its o EnvRunners plus the local one).</span>
<span class="sd">            module_spec: In the new RLModule API we need to pass in the module_spec for</span>
<span class="sd">                the new module that is supposed to be added. Knowing the policy spec is</span>
<span class="sd">                not sufficient.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The newly added policy (the copy that got added to the local</span>
<span class="sd">            worker). If `workers` was provided, None is returned.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`Algorithm.add_policy()` is not supported on the new API stack w/ &quot;</span>
                <span class="s2">&quot;EnvRunners! Use `Algorithm.add_module()` instead. Also see &quot;</span>
                <span class="s2">&quot;`rllib/examples/self_play_league_based_with_open_spiel.py` for an &quot;</span>
                <span class="s2">&quot;example.&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">evaluation_workers</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Algorithm.add_policy(evaluation_workers=...)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;Algorithm.add_policy(add_to_eval_env_runners=...)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">add_to_learners</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Algorithm.add_policy(add_to_learners=..)&quot;</span><span class="p">,</span>
                <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Hybrid API stack no longer supported by RLlib!&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="n">validate_module_id</span><span class="p">(</span><span class="n">policy_id</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">add_to_env_runners</span> <span class="ow">is</span> <span class="kc">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">add_policy</span><span class="p">(</span>
                <span class="n">policy_id</span><span class="p">,</span>
                <span class="n">policy_cls</span><span class="p">,</span>
                <span class="n">policy</span><span class="p">,</span>
                <span class="n">observation_space</span><span class="o">=</span><span class="n">observation_space</span><span class="p">,</span>
                <span class="n">action_space</span><span class="o">=</span><span class="n">action_space</span><span class="p">,</span>
                <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
                <span class="n">policy_state</span><span class="o">=</span><span class="n">policy_state</span><span class="p">,</span>
                <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">policy_mapping_fn</span><span class="p">,</span>
                <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
                <span class="n">module_spec</span><span class="o">=</span><span class="n">module_spec</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Add to evaluation workers, if necessary.</span>
        <span class="k">if</span> <span class="n">add_to_eval_env_runners</span> <span class="ow">is</span> <span class="kc">True</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">add_policy</span><span class="p">(</span>
                <span class="n">policy_id</span><span class="p">,</span>
                <span class="n">policy_cls</span><span class="p">,</span>
                <span class="n">policy</span><span class="p">,</span>
                <span class="n">observation_space</span><span class="o">=</span><span class="n">observation_space</span><span class="p">,</span>
                <span class="n">action_space</span><span class="o">=</span><span class="n">action_space</span><span class="p">,</span>
                <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
                <span class="n">policy_state</span><span class="o">=</span><span class="n">policy_state</span><span class="p">,</span>
                <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">policy_mapping_fn</span><span class="p">,</span>
                <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
                <span class="n">module_spec</span><span class="o">=</span><span class="n">module_spec</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Return newly added policy (from the local EnvRunner).</span>
        <span class="k">if</span> <span class="n">add_to_env_runners</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">add_to_eval_env_runners</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="o">.</span><span class="n">policy_map</span><span class="p">[</span><span class="n">policy_id</span><span class="p">]</span></div>


<div class="viewcode-block" id="Algorithm.remove_policy">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.remove_policy.html#ray.rllib.algorithms.algorithm.Algorithm.remove_policy">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span> <span class="nf">remove_policy</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">policy_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">],</span> <span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policies_to_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">Collection</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">],</span>
                <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatchType</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">remove_from_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">remove_from_eval_env_runners</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="c1"># Deprecated args.</span>
        <span class="n">evaluation_workers</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
        <span class="n">remove_from_learners</span><span class="o">=</span><span class="n">DEPRECATED_VALUE</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Removes a policy from this Algorithm.</span>

<span class="sd">        Args:</span>
<span class="sd">            policy_id: ID of the policy to be removed.</span>
<span class="sd">            policy_mapping_fn: An optional (updated) policy mapping function</span>
<span class="sd">                to use from here on. Note that already ongoing episodes will</span>
<span class="sd">                not change their mapping but will use the old mapping till</span>
<span class="sd">                the end of the episode.</span>
<span class="sd">            policies_to_train: An optional list of policy IDs to be trained</span>
<span class="sd">                or a callable taking PolicyID and SampleBatchType and</span>
<span class="sd">                returning a bool (trainable or not?).</span>
<span class="sd">                If None, will keep the existing setup in place. Policies,</span>
<span class="sd">                whose IDs are not in the list (or for which the callable</span>
<span class="sd">                returns False) will not be updated.</span>
<span class="sd">            remove_from_env_runners: Whether to remove the Policy from the</span>
<span class="sd">                EnvRunnerGroup (with its m EnvRunners plus the local one).</span>
<span class="sd">            remove_from_eval_env_runners: Whether to remove the RLModule from the eval</span>
<span class="sd">                EnvRunnerGroup (with its o EnvRunners plus the local one).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">evaluation_workers</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Algorithm.remove_policy(evaluation_workers=...)&quot;</span><span class="p">,</span>
                <span class="n">new</span><span class="o">=</span><span class="s2">&quot;Algorithm.remove_policy(remove_from_eval_env_runners=...)&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">remove_from_eval_env_runners</span> <span class="o">=</span> <span class="n">evaluation_workers</span>
        <span class="k">if</span> <span class="n">remove_from_learners</span> <span class="o">!=</span> <span class="n">DEPRECATED_VALUE</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="n">old</span><span class="o">=</span><span class="s2">&quot;Algorithm.remove_policy(remove_from_learners=..)&quot;</span><span class="p">,</span>
                <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Hybrid API stack no longer supported by RLlib!&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">def</span> <span class="nf">fn</span><span class="p">(</span><span class="n">worker</span><span class="p">):</span>
            <span class="n">worker</span><span class="o">.</span><span class="n">remove_policy</span><span class="p">(</span>
                <span class="n">policy_id</span><span class="o">=</span><span class="n">policy_id</span><span class="p">,</span>
                <span class="n">policy_mapping_fn</span><span class="o">=</span><span class="n">policy_mapping_fn</span><span class="p">,</span>
                <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Update all EnvRunner workers.</span>
        <span class="k">if</span> <span class="n">remove_from_env_runners</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">local_env_runner</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Update the evaluation worker set&#39;s workers, if required.</span>
        <span class="k">if</span> <span class="n">remove_from_eval_env_runners</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">local_env_runner</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.export_policy_model">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.export_policy_model.html#ray.rllib.algorithms.algorithm.Algorithm.export_policy_model">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span> <span class="nf">export_policy_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">export_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
        <span class="n">onnx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Exports policy model with given policy_id to a local directory.</span>

<span class="sd">        Args:</span>
<span class="sd">            export_dir: Writable local directory.</span>
<span class="sd">            policy_id: Optional policy id to export.</span>
<span class="sd">            onnx: If given, will export model in ONNX format. The</span>
<span class="sd">                value of this parameter set the ONNX OpSet version to use.</span>
<span class="sd">                If None, the output format will be DL framework specific.</span>

<span class="sd">        .. testcode::</span>

<span class="sd">            from ray.rllib.algorithms.ppo import PPO, PPOConfig</span>
<span class="sd">            config = PPOConfig().environment(&quot;CartPole-v1&quot;)</span>
<span class="sd">            algo = PPO(config=config)</span>
<span class="sd">            algo.train()</span>
<span class="sd">            algo.export_policy_checkpoint(&quot;/tmp/export_dir&quot;)</span>
<span class="sd">            algo.export_policy_model(&quot;/tmp/dir&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span><span class="o">.</span><span class="n">export_model</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">onnx</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.export_policy_checkpoint">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.export_policy_checkpoint.html#ray.rllib.algorithms.algorithm.Algorithm.export_policy_checkpoint">[docs]</a>
    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span> <span class="nf">export_policy_checkpoint</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">export_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">policy_id</span><span class="p">:</span> <span class="n">PolicyID</span> <span class="o">=</span> <span class="n">DEFAULT_POLICY_ID</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Exports Policy checkpoint to a local directory and returns an AIR Checkpoint.</span>

<span class="sd">        Args:</span>
<span class="sd">            export_dir: Writable local directory to store the AIR Checkpoint</span>
<span class="sd">                information into.</span>
<span class="sd">            policy_id: Optional policy ID to export. If not provided, will export</span>
<span class="sd">                &quot;default_policy&quot;. If `policy_id` does not exist in this Algorithm,</span>
<span class="sd">                will raise a KeyError.</span>

<span class="sd">        Raises:</span>
<span class="sd">            KeyError: if `policy_id` cannot be found in this Algorithm.</span>

<span class="sd">        .. testcode::</span>

<span class="sd">            from ray.rllib.algorithms.ppo import PPO, PPOConfig</span>
<span class="sd">            config = PPOConfig().environment(&quot;CartPole-v1&quot;)</span>
<span class="sd">            algo = PPO(config=config)</span>
<span class="sd">            algo.train()</span>
<span class="sd">            algo.export_policy_checkpoint(&quot;/tmp/export_dir&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">policy_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">policy</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Policy with ID </span><span class="si">{</span><span class="n">policy_id</span><span class="si">}</span><span class="s2"> not found in Algorithm!&quot;</span><span class="p">)</span>
        <span class="n">policy</span><span class="o">.</span><span class="n">export_checkpoint</span><span class="p">(</span><span class="n">export_dir</span><span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.save_checkpoint">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.save_checkpoint.html#ray.rllib.algorithms.algorithm.Algorithm.save_checkpoint">[docs]</a>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Exports checkpoint to a local directory.</span>

<span class="sd">        The structure of an Algorithm checkpoint dir will be as follows::</span>

<span class="sd">            policies/</span>
<span class="sd">                pol_1/</span>
<span class="sd">                    policy_state.pkl</span>
<span class="sd">                pol_2/</span>
<span class="sd">                    policy_state.pkl</span>
<span class="sd">            learner/</span>
<span class="sd">                learner_state.json</span>
<span class="sd">                module_state/</span>
<span class="sd">                    module_1/</span>
<span class="sd">                        ...</span>
<span class="sd">                optimizer_state/</span>
<span class="sd">                    optimizers_module_1/</span>
<span class="sd">                        ...</span>
<span class="sd">            rllib_checkpoint.json</span>
<span class="sd">            algorithm_state.pkl</span>

<span class="sd">        Note: `rllib_checkpoint.json` contains a &quot;version&quot; key (e.g. with value 0.1)</span>
<span class="sd">        helping RLlib to remain backward compatible wrt. restoring from checkpoints from</span>
<span class="sd">        Ray 2.0 onwards.</span>

<span class="sd">        Args:</span>
<span class="sd">            checkpoint_dir: The directory where the checkpoint files will be stored.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># New API stack: Delegate to the `Checkpointable` implementation of</span>
        <span class="c1"># `save_to_path()`.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_to_path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>

        <span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">__getstate__</span><span class="p">()</span>

        <span class="c1"># Extract policy states from worker state (Policies get their own</span>
        <span class="c1"># checkpoint sub-dirs).</span>
        <span class="n">policy_states</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="s2">&quot;worker&quot;</span> <span class="ow">in</span> <span class="n">state</span> <span class="ow">and</span> <span class="s2">&quot;policy_states&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]:</span>
            <span class="n">policy_states</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;policy_states&quot;</span><span class="p">,</span> <span class="p">{})</span>

        <span class="c1"># Add RLlib checkpoint version.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">CHECKPOINT_VERSION_LEARNER</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">CHECKPOINT_VERSION</span>

        <span class="c1"># Write state (w/o policies) to disk.</span>
        <span class="n">state_file</span> <span class="o">=</span> <span class="n">checkpoint_dir</span> <span class="o">/</span> <span class="s2">&quot;algorithm_state.pkl&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">state_file</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>

        <span class="c1"># Write rllib_checkpoint.json.</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">checkpoint_dir</span> <span class="o">/</span> <span class="s2">&quot;rllib_checkpoint.json&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Algorithm&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;checkpoint_version&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]),</span>
                    <span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;cloudpickle&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;state_file&quot;</span><span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">state_file</span><span class="p">),</span>
                    <span class="s2">&quot;policy_ids&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="n">policy_states</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
                    <span class="s2">&quot;ray_version&quot;</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">__version__</span><span class="p">,</span>
                    <span class="s2">&quot;ray_commit&quot;</span><span class="p">:</span> <span class="n">ray</span><span class="o">.</span><span class="n">__commit__</span><span class="p">,</span>
                <span class="p">},</span>
                <span class="n">f</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Old API stack: Write individual policies to disk, each in their own</span>
        <span class="c1"># sub-directory.</span>
        <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="n">policy_state</span> <span class="ow">in</span> <span class="n">policy_states</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># From here on, disallow policyIDs that would not work as directory names.</span>
            <span class="n">validate_module_id</span><span class="p">(</span><span class="n">pid</span><span class="p">,</span> <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">policy_dir</span> <span class="o">=</span> <span class="n">checkpoint_dir</span> <span class="o">/</span> <span class="s2">&quot;policies&quot;</span> <span class="o">/</span> <span class="n">pid</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">policy_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">policy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_policy</span><span class="p">(</span><span class="n">pid</span><span class="p">)</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">export_checkpoint</span><span class="p">(</span><span class="n">policy_dir</span><span class="p">,</span> <span class="n">policy_state</span><span class="o">=</span><span class="n">policy_state</span><span class="p">)</span>

        <span class="c1"># If we are using the learner API (hybrid API stack) -&gt; Save the learner group&#39;s</span>
        <span class="c1"># state inside a &quot;learner&quot; subdir. Note that this is not in line with the</span>
        <span class="c1"># new Checkpointable API, but makes this case backward compatible.</span>
        <span class="c1"># The new Checkpointable API is only strictly applied anyways to the</span>
        <span class="c1"># new API stack.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="n">learner_state_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;learner&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">save_to_path</span><span class="p">(</span><span class="n">learner_state_dir</span><span class="p">)</span></div>


    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># New API stack: Delegate to the `Checkpointable` implementation of</span>
        <span class="c1"># `restore_from_path()`.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">restore_from_path</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>

            <span class="c1"># Call the `on_checkpoint_loaded` callback.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_checkpoint_loaded</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="c1"># Checkpoint is provided as a local directory.</span>
        <span class="c1"># Restore from the checkpoint file or dir.</span>
        <span class="n">checkpoint_info</span> <span class="o">=</span> <span class="n">get_checkpoint_info</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">)</span>
        <span class="n">checkpoint_data</span> <span class="o">=</span> <span class="n">Algorithm</span><span class="o">.</span><span class="n">_checkpoint_info_to_algorithm_state</span><span class="p">(</span><span class="n">checkpoint_info</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">checkpoint_data</span><span class="p">)</span>
        <span class="c1"># Call the `on_checkpoint_loaded` callback.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_checkpoint_loaded</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_state</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Collection</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">not_components</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Collection</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">StateDict</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Algorithm.get_state() not supported on the old API stack! &quot;</span>
                <span class="s2">&quot;Use Algorithm.__getstate__() instead.&quot;</span>
            <span class="p">)</span>

        <span class="n">state</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Get (local) EnvRunner state (w/o RLModule).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_component</span><span class="p">(</span><span class="n">COMPONENT_ENV_RUNNER</span><span class="p">,</span> <span class="n">components</span><span class="p">,</span> <span class="n">not_components</span><span class="p">):</span>
            <span class="n">state</span><span class="p">[</span>
                <span class="n">COMPONENT_ENV_RUNNER</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span>
                <span class="n">components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_subcomponents</span><span class="p">(</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span> <span class="n">components</span><span class="p">),</span>
                <span class="n">not_components</span><span class="o">=</span><span class="n">force_list</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_get_subcomponents</span><span class="p">(</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span> <span class="n">not_components</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="c1"># We don&#39;t want the RLModule state from the EnvRunners (it&#39;s</span>
                <span class="c1"># `inference_only` anyway and already provided in full by the Learners).</span>
                <span class="o">+</span> <span class="p">[</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">],</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Get (local) evaluation EnvRunner state (w/o RLModule).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_component</span><span class="p">(</span>
            <span class="n">COMPONENT_EVAL_ENV_RUNNER</span><span class="p">,</span> <span class="n">components</span><span class="p">,</span> <span class="n">not_components</span>
        <span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_EVAL_ENV_RUNNER</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span>
                <span class="n">components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_subcomponents</span><span class="p">(</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span> <span class="n">components</span><span class="p">),</span>
                <span class="n">not_components</span><span class="o">=</span><span class="n">force_list</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_get_subcomponents</span><span class="p">(</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">,</span> <span class="n">not_components</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="c1"># We don&#39;t want the RLModule state from the EnvRunners (it&#39;s</span>
                <span class="c1"># `inference_only` anyway and already provided in full by the Learners).</span>
                <span class="o">+</span> <span class="p">[</span><span class="n">COMPONENT_RL_MODULE</span><span class="p">],</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Get LearnerGroup state (w/ RLModule).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_check_component</span><span class="p">(</span><span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">,</span> <span class="n">components</span><span class="p">,</span> <span class="n">not_components</span><span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">get_state</span><span class="p">(</span>
                <span class="n">components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_subcomponents</span><span class="p">(</span><span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">,</span> <span class="n">components</span><span class="p">),</span>
                <span class="n">not_components</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_subcomponents</span><span class="p">(</span>
                    <span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">,</span> <span class="n">not_components</span>
                <span class="p">),</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Get entire MetricsLogger state.</span>
        <span class="c1"># TODO (sven): Make `MetricsLogger` a Checkpointable.</span>
        <span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_METRICS_LOGGER</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>

        <span class="c1"># Save current `training_iteration`.</span>
        <span class="n">state</span><span class="p">[</span><span class="n">TRAINING_ITERATION</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_iteration</span>

        <span class="k">return</span> <span class="n">state</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">set_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="n">StateDict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Set the (training) EnvRunners&#39; states.</span>
        <span class="k">if</span> <span class="n">COMPONENT_ENV_RUNNER</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span>
                <span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_ENV_RUNNER</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_env_runner_states</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Set the (eval) EnvRunners&#39; states.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">and</span> <span class="n">COMPONENT_EVAL_ENV_RUNNER</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_ENV_RUNNER</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_env_runner_states</span><span class="p">(</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span>
            <span class="p">)</span>

        <span class="c1"># Set the LearnerGroup&#39;s state.</span>
        <span class="k">if</span> <span class="n">COMPONENT_LEARNER_GROUP</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">])</span>
            <span class="c1"># Sync new weights to all EnvRunners.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                    <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                    <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="c1"># TODO (sven): Make `MetricsLogger` a Checkpointable.</span>
        <span class="k">if</span> <span class="n">COMPONENT_METRICS_LOGGER</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="n">COMPONENT_METRICS_LOGGER</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">TRAINING_ITERATION</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_iteration</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="n">TRAINING_ITERATION</span><span class="p">]</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_checkpointable_components</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;Checkpointable&quot;</span><span class="p">]]:</span>
        <span class="n">components</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">(</span><span class="n">COMPONENT_ENV_RUNNER</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="p">),</span>
            <span class="p">(</span><span class="n">COMPONENT_LEARNER_GROUP</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">),</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span>
            <span class="n">components</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="n">COMPONENT_EVAL_ENV_RUNNER</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">components</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_ctor_args_and_kwargs</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_state</span><span class="p">(),),</span>  <span class="c1"># *args,</span>
            <span class="p">{},</span>  <span class="c1"># **kwargs</span>
        <span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Checkpointable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">restore_from_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Override from parent method, b/c we might have to sync the EnvRunner weights</span>
        <span class="c1"># after having restored/loaded the LearnerGroup state.</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">restore_from_path</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># Sync EnvRunners, but only if LearnerGroup&#39;s checkpoint can be found in path.</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">path</span> <span class="o">/</span> <span class="s2">&quot;learner_group&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">is_dir</span><span class="p">():</span>
            <span class="c1"># Make also sure, all (training) EnvRunners get the just loaded weights, but</span>
            <span class="c1"># only the inference-only ones.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learner_group</span><span class="p">,</span>
                <span class="n">inference_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">log_result</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">:</span> <span class="n">ResultDict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Log after the callback is invoked, so that the user has a chance</span>
        <span class="c1"># to mutate the result.</span>
        <span class="c1"># TODO (sven): It might not make sense to pass in the MetricsLogger at this late</span>
        <span class="c1">#  point in time. In here, the result dict has already been &quot;compiled&quot; (reduced)</span>
        <span class="c1">#  by the MetricsLogger and there is probably no point in adding more Stats</span>
        <span class="c1">#  here.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_train_result</span><span class="p">(</span>
            <span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">,</span> <span class="n">result</span><span class="o">=</span><span class="n">result</span>
        <span class="p">)</span>
        <span class="c1"># Then log according to Trainable&#39;s logging logic.</span>
        <span class="n">Trainable</span><span class="o">.</span><span class="n">log_result</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">cleanup</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Stop all workers.</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;env_runner_group&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;eval_env_runner_group&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>

    <span class="nd">@OverrideToImplementCustomLogic</span>
    <span class="nd">@classmethod</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">default_resource_request</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">,</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">Resources</span><span class="p">,</span> <span class="n">PlacementGroupFactory</span><span class="p">]:</span>
        <span class="c1"># Default logic for RLlib Algorithms:</span>
        <span class="c1"># Create one bundle per individual worker (local or remote).</span>
        <span class="c1"># Use `num_cpus_for_main_process` and `num_gpus` for the local worker and</span>
        <span class="c1"># `num_cpus_per_env_runner` and `num_gpus_per_env_runner` for the remote</span>
        <span class="c1"># EnvRunners to determine their CPU/GPU resource needs.</span>

        <span class="c1"># Convenience config handles.</span>
        <span class="n">cf</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="n">cf</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
        <span class="n">cf</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="c1"># get evaluation config</span>
        <span class="n">eval_cf</span> <span class="o">=</span> <span class="n">cf</span><span class="o">.</span><span class="n">get_evaluation_config_object</span><span class="p">()</span>
        <span class="n">eval_cf</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>
        <span class="n">eval_cf</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>

        <span class="c1"># resources for the driver of this trainable</span>
        <span class="k">if</span> <span class="n">cf</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_learners</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># in this case local_worker only does sampling and training is done on</span>
                <span class="c1"># local learner worker</span>
                <span class="n">driver</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_get_learner_bundles</span><span class="p">(</span><span class="n">cf</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># in this case local_worker only does sampling and training is done on</span>
                <span class="c1"># remote learner workers</span>
                <span class="n">driver</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_for_main_process</span><span class="p">,</span> <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">driver</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_for_main_process</span><span class="p">,</span>
                <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">cf</span><span class="o">.</span><span class="n">_fake_gpus</span> <span class="k">else</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_gpus</span><span class="p">,</span>
            <span class="p">}</span>

        <span class="c1"># resources for remote rollout env samplers</span>
        <span class="n">rollout_bundles</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_cpus_per_env_runner</span><span class="p">,</span>
                <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_gpus_per_env_runner</span><span class="p">,</span>
                <span class="o">**</span><span class="n">cf</span><span class="o">.</span><span class="n">custom_resources_per_env_runner</span><span class="p">,</span>
            <span class="p">}</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cf</span><span class="o">.</span><span class="n">num_env_runners</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># resources for remote evaluation env samplers or datasets (if any)</span>
        <span class="k">if</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_should_create_evaluation_rollout_workers</span><span class="p">(</span><span class="n">eval_cf</span><span class="p">):</span>
            <span class="c1"># Evaluation workers.</span>
            <span class="c1"># Note: The local eval worker is located on the driver CPU.</span>
            <span class="n">evaluation_bundles</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;CPU&quot;</span><span class="p">:</span> <span class="n">eval_cf</span><span class="o">.</span><span class="n">num_cpus_per_env_runner</span><span class="p">,</span>
                    <span class="s2">&quot;GPU&quot;</span><span class="p">:</span> <span class="n">eval_cf</span><span class="o">.</span><span class="n">num_gpus_per_env_runner</span><span class="p">,</span>
                    <span class="o">**</span><span class="n">eval_cf</span><span class="o">.</span><span class="n">custom_resources_per_env_runner</span><span class="p">,</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">eval_cf</span><span class="o">.</span><span class="n">evaluation_num_env_runners</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># resources for offline dataset readers during evaluation</span>
            <span class="c1"># Note (Kourosh): we should not claim extra workers for</span>
            <span class="c1"># training on the offline dataset, since rollout workers have already</span>
            <span class="c1"># claimed it.</span>
            <span class="c1"># Another Note (Kourosh): dataset reader will not use placement groups so</span>
            <span class="c1"># whatever we specify here won&#39;t matter because dataset won&#39;t even use it.</span>
            <span class="c1"># Disclaimer: using ray dataset in tune may cause deadlock when multiple</span>
            <span class="c1"># tune trials get scheduled on the same node and do not leave any spare</span>
            <span class="c1"># resources for dataset operations. The workaround is to limit the</span>
            <span class="c1"># max_concurrent trials so that some spare cpus are left for dataset</span>
            <span class="c1"># operations. This behavior should get fixed by the dataset team. more info</span>
            <span class="c1"># found here:</span>
            <span class="c1"># https://docs.ray.io/en/master/data/dataset-internals.html#datasets-tune</span>
            <span class="n">evaluation_bundles</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># resources for remote learner workers</span>
        <span class="n">learner_bundles</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="n">cf</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span> <span class="ow">and</span> <span class="n">cf</span><span class="o">.</span><span class="n">num_learners</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">learner_bundles</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_get_learner_bundles</span><span class="p">(</span><span class="n">cf</span><span class="p">)</span>

        <span class="n">bundles</span> <span class="o">=</span> <span class="p">[</span><span class="n">driver</span><span class="p">]</span> <span class="o">+</span> <span class="n">rollout_bundles</span> <span class="o">+</span> <span class="n">evaluation_bundles</span> <span class="o">+</span> <span class="n">learner_bundles</span>

        <span class="c1"># Return PlacementGroupFactory containing all needed resources</span>
        <span class="c1"># (already properly defined as device bundles).</span>
        <span class="k">return</span> <span class="n">PlacementGroupFactory</span><span class="p">(</span>
            <span class="n">bundles</span><span class="o">=</span><span class="n">bundles</span><span class="p">,</span>
            <span class="n">strategy</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;placement_strategy&quot;</span><span class="p">,</span> <span class="s2">&quot;PACK&quot;</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">_before_evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Pre-evaluation callback.&quot;&quot;&quot;</span>
        <span class="k">pass</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_get_env_id_and_creator</span><span class="p">(</span>
        <span class="n">env_specifier</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">EnvType</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfig</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">EnvCreator</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns env_id and creator callable given original env id from config.</span>

<span class="sd">        Args:</span>
<span class="sd">            env_specifier: An env class, an already tune registered env ID, a known</span>
<span class="sd">                gym env name, or None (if no env is used).</span>
<span class="sd">            config: The AlgorithmConfig object.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple consisting of a) env ID string and b) env creator callable.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Environment is specified via a string.</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="c1"># An already registered env.</span>
            <span class="k">if</span> <span class="n">_global_registry</span><span class="o">.</span><span class="n">contains</span><span class="p">(</span><span class="n">ENV_CREATOR</span><span class="p">,</span> <span class="n">env_specifier</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">env_specifier</span><span class="p">,</span> <span class="n">_global_registry</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">ENV_CREATOR</span><span class="p">,</span> <span class="n">env_specifier</span><span class="p">)</span>

            <span class="c1"># A class path specifier.</span>
            <span class="k">elif</span> <span class="s2">&quot;.&quot;</span> <span class="ow">in</span> <span class="n">env_specifier</span><span class="p">:</span>

                <span class="k">def</span> <span class="nf">env_creator_from_classpath</span><span class="p">(</span><span class="n">env_context</span><span class="p">):</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">env_obj</span> <span class="o">=</span> <span class="n">from_config</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">,</span> <span class="n">env_context</span><span class="p">)</span>
                    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="n">EnvError</span><span class="p">(</span>
                            <span class="n">ERR_MSG_INVALID_ENV_DESCRIPTOR</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">)</span>
                        <span class="p">)</span>
                    <span class="k">return</span> <span class="n">env_obj</span>

                <span class="k">return</span> <span class="n">env_specifier</span><span class="p">,</span> <span class="n">env_creator_from_classpath</span>
            <span class="c1"># Try gym/PyBullet.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">env_specifier</span><span class="p">,</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                    <span class="n">_gym_env_creator</span><span class="p">,</span> <span class="n">env_descriptor</span><span class="o">=</span><span class="n">env_specifier</span>
                <span class="p">)</span>

        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">,</span> <span class="nb">type</span><span class="p">):</span>
            <span class="n">env_id</span> <span class="o">=</span> <span class="n">env_specifier</span>  <span class="c1"># .__name__</span>

            <span class="k">if</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;remote_worker_envs&quot;</span><span class="p">]:</span>
                <span class="c1"># Check gym version (0.22 or higher?).</span>
                <span class="c1"># If &gt; 0.21, can&#39;t perform auto-wrapping of the given class as this</span>
                <span class="c1"># would lead to a pickle error.</span>
                <span class="n">gym_version</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">metadata</span><span class="o">.</span><span class="n">version</span><span class="p">(</span><span class="s2">&quot;gym&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="n">gym_version</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">version</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span><span class="s2">&quot;0.22&quot;</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Cannot specify a gym.Env class via `config.env` while setting &quot;</span>
                        <span class="s2">&quot;`config.remote_worker_env=True` AND your gym version is &gt;= &quot;</span>
                        <span class="s2">&quot;0.22! Try installing an older version of gym or set `config.&quot;</span>
                        <span class="s2">&quot;remote_worker_env=False`.&quot;</span>
                    <span class="p">)</span>

                <span class="nd">@ray</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">num_cpus</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">class</span> <span class="nc">_wrapper</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">):</span>
                    <span class="c1"># Add convenience `_get_spaces` and `_is_multi_agent`</span>
                    <span class="c1"># methods:</span>
                    <span class="k">def</span> <span class="nf">_get_spaces</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span>

                    <span class="k">def</span> <span class="nf">_is_multi_agent</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
                        <span class="kn">from</span> <span class="nn">ray.rllib.env.multi_agent_env</span> <span class="kn">import</span> <span class="n">MultiAgentEnv</span>

                        <span class="k">return</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">MultiAgentEnv</span><span class="p">)</span>

                <span class="k">return</span> <span class="n">env_id</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">_wrapper</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
            <span class="c1"># gym.Env-subclass: Also go through our RLlib gym-creator.</span>
            <span class="k">elif</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">,</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">):</span>
                <span class="k">return</span> <span class="n">env_id</span><span class="p">,</span> <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                    <span class="n">_gym_env_creator</span><span class="p">,</span>
                    <span class="n">env_descriptor</span><span class="o">=</span><span class="n">env_specifier</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="c1"># All other env classes: Call c&#39;tor directly.</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">env_id</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">cfg</span><span class="p">:</span> <span class="n">env_specifier</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>

        <span class="c1"># No env -&gt; Env creator always returns None.</span>
        <span class="k">elif</span> <span class="n">env_specifier</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">env_config</span><span class="p">:</span> <span class="kc">None</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is an invalid env specifier. &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">env_specifier</span><span class="p">)</span>
                <span class="o">+</span> <span class="s2">&quot;You can specify a custom env as either a class &quot;</span>
                <span class="s1">&#39;(e.g., YourEnvCls) or a registered env id (e.g., &quot;your_env&quot;).&#39;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_sync_filters_if_needed</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">central_worker</span><span class="p">:</span> <span class="n">EnvRunner</span><span class="p">,</span>
        <span class="n">workers</span><span class="p">:</span> <span class="n">EnvRunnerGroup</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">AlgorithmConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Synchronizes the filter stats from `workers` to `central_worker`.</span>

<span class="sd">        .. and broadcasts the central_worker&#39;s filter stats back to all `workers`</span>
<span class="sd">        (if configured).</span>

<span class="sd">        Args:</span>
<span class="sd">            central_worker: The worker to sync/aggregate all `workers`&#39; filter stats to</span>
<span class="sd">                and from which to (possibly) broadcast the updated filter stats back to</span>
<span class="sd">                `workers`.</span>
<span class="sd">            workers: The EnvRunnerGroup, whose EnvRunners&#39; filter stats should be used</span>
<span class="sd">                for aggregation on `central_worker` and which (possibly) get updated</span>
<span class="sd">                from `central_worker` after the sync.</span>
<span class="sd">            config: The algorithm config instance. This is used to determine, whether</span>
<span class="sd">                syncing from `workers` should happen at all and whether broadcasting</span>
<span class="sd">                back to `workers` (after possible syncing) should happen.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">central_worker</span> <span class="ow">and</span> <span class="n">config</span><span class="o">.</span><span class="n">observation_filter</span> <span class="o">!=</span> <span class="s2">&quot;NoFilter&quot;</span><span class="p">:</span>
            <span class="n">FilterManager</span><span class="o">.</span><span class="n">synchronize</span><span class="p">(</span>
                <span class="n">central_worker</span><span class="o">.</span><span class="n">filters</span><span class="p">,</span>
                <span class="n">workers</span><span class="p">,</span>
                <span class="n">update_remote</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">update_worker_filter_stats</span><span class="p">,</span>
                <span class="n">timeout_seconds</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">sync_filters_on_rollout_workers_timeout_s</span><span class="p">,</span>
                <span class="n">use_remote_data_for_update</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">use_worker_filter_stats</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="nd">@classmethod</span>
    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">resource_help</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">AlgorithmConfig</span><span class="p">,</span> <span class="n">AlgorithmConfigDict</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2">You can adjust the resource requests of RLlib Algorithms by calling &quot;</span>
            <span class="s2">&quot;`AlgorithmConfig.env_runners(&quot;</span>
            <span class="s2">&quot;num_env_runners=.., num_cpus_per_env_runner=.., &quot;</span>
            <span class="s2">&quot;num_gpus_per_env_runner=.., ..)` and &quot;</span>
            <span class="s2">&quot;`AgorithmConfig.learners(num_learners=.., num_gpus_per_learner=..)`. See &quot;</span>
            <span class="s2">&quot;the `ray.rllib.algorithms.algorithm_config.AlgorithmConfig` classes &quot;</span>
            <span class="s2">&quot;(each Algorithm has its own subclass of this class) for more info.</span><span class="se">\n\n</span><span class="s2">&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;The config of this Algorithm is: </span><span class="si">{</span><span class="n">config</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">get_auto_filled_metrics</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">now</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">datetime</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">time_this_iter</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">timestamp</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">debug_metrics_only</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
        <span class="c1"># Override this method to make sure, the `config` key of the returned results</span>
        <span class="c1"># contains the proper Tune config dict (instead of an AlgorithmConfig object).</span>
        <span class="n">auto_filled</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_auto_filled_metrics</span><span class="p">(</span>
            <span class="n">now</span><span class="p">,</span> <span class="n">time_this_iter</span><span class="p">,</span> <span class="n">timestamp</span><span class="p">,</span> <span class="n">debug_metrics_only</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;config&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">auto_filled</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span><span class="s2">&quot;`config` key not found in auto-filled results dict!&quot;</span><span class="p">)</span>

        <span class="c1"># If `config` key is no dict (but AlgorithmConfig object) -&gt;</span>
        <span class="c1"># make sure, it&#39;s a dict to not break Tune APIs.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">auto_filled</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">auto_filled</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">],</span> <span class="n">AlgorithmConfig</span><span class="p">)</span>
            <span class="n">auto_filled</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">auto_filled</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">auto_filled</span>

<div class="viewcode-block" id="Algorithm.merge_algorithm_configs">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.merge_algorithm_configs.html#ray.rllib.algorithms.algorithm.Algorithm.merge_algorithm_configs">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">merge_algorithm_configs</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">config1</span><span class="p">:</span> <span class="n">AlgorithmConfigDict</span><span class="p">,</span>
        <span class="n">config2</span><span class="p">:</span> <span class="n">PartialAlgorithmConfigDict</span><span class="p">,</span>
        <span class="n">_allow_unknown_configs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AlgorithmConfigDict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Merges a complete Algorithm config dict with a partial override dict.</span>

<span class="sd">        Respects nested structures within the config dicts. The values in the</span>
<span class="sd">        partial override dict take priority.</span>

<span class="sd">        Args:</span>
<span class="sd">            config1: The complete Algorithm&#39;s dict to be merged (overridden)</span>
<span class="sd">                with `config2`.</span>
<span class="sd">            config2: The partial override config dict to merge on top of</span>
<span class="sd">                `config1`.</span>
<span class="sd">            _allow_unknown_configs: If True, keys in `config2` that don&#39;t exist</span>
<span class="sd">                in `config1` are allowed and will be added to the final config.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The merged full algorithm config dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">config1</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">config1</span><span class="p">)</span>
        <span class="k">if</span> <span class="s2">&quot;callbacks&quot;</span> <span class="ow">in</span> <span class="n">config2</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">config2</span><span class="p">[</span><span class="s2">&quot;callbacks&quot;</span><span class="p">])</span> <span class="ow">is</span> <span class="nb">dict</span><span class="p">:</span>
            <span class="n">deprecation_warning</span><span class="p">(</span>
                <span class="s2">&quot;callbacks dict interface&quot;</span><span class="p">,</span>
                <span class="s2">&quot;a class extending rllib.algorithms.callbacks.DefaultCallbacks; &quot;</span>
                <span class="s2">&quot;see `rllib/examples/metrics/custom_metrics_and_callbacks.py` for an &quot;</span>
                <span class="s2">&quot;example.&quot;</span><span class="p">,</span>
                <span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">_allow_unknown_configs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">_allow_unknown_configs</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_allow_unknown_configs</span>
        <span class="k">return</span> <span class="n">deep_update</span><span class="p">(</span>
            <span class="n">config1</span><span class="p">,</span>
            <span class="n">config2</span><span class="p">,</span>
            <span class="n">_allow_unknown_configs</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_allow_unknown_subkeys</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_override_all_subkeys_if_type_changes</span><span class="p">,</span>
            <span class="bp">cls</span><span class="o">.</span><span class="n">_override_all_key_list</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Algorithm.validate_env">
<a class="viewcode-back" href="../../../../rllib/package_ref/doc/ray.rllib.algorithms.algorithm.Algorithm.validate_env.html#ray.rllib.algorithms.algorithm.Algorithm.validate_env">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="nd">@ExperimentalAPI</span>
    <span class="k">def</span> <span class="nf">validate_env</span><span class="p">(</span><span class="n">env</span><span class="p">:</span> <span class="n">EnvType</span><span class="p">,</span> <span class="n">env_context</span><span class="p">:</span> <span class="n">EnvContext</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Env validator function for this Algorithm class.</span>

<span class="sd">        Override this in child classes to define custom validation</span>
<span class="sd">        behavior.</span>

<span class="sd">        Args:</span>
<span class="sd">            env: The (sub-)environment to validate. This is normally a</span>
<span class="sd">                single sub-environment (e.g. a gym.Env) within a vectorized</span>
<span class="sd">                setup.</span>
<span class="sd">            env_context: The EnvContext to configure the environment.</span>

<span class="sd">        Raises:</span>
<span class="sd">            Exception: in case something is wrong with the given environment.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>


    <span class="nd">@override</span><span class="p">(</span><span class="n">Trainable</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">_export_model</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">export_formats</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">export_dir</span><span class="p">:</span> <span class="nb">str</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
        <span class="n">ExportFormat</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="n">export_formats</span><span class="p">)</span>
        <span class="n">exported</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">CHECKPOINT</span> <span class="ow">in</span> <span class="n">export_formats</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">CHECKPOINT</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">export_policy_checkpoint</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
            <span class="n">exported</span><span class="p">[</span><span class="n">ExportFormat</span><span class="o">.</span><span class="n">CHECKPOINT</span><span class="p">]</span> <span class="o">=</span> <span class="n">path</span>
        <span class="k">if</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">MODEL</span> <span class="ow">in</span> <span class="n">export_formats</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">MODEL</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">export_policy_model</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
            <span class="n">exported</span><span class="p">[</span><span class="n">ExportFormat</span><span class="o">.</span><span class="n">MODEL</span><span class="p">]</span> <span class="o">=</span> <span class="n">path</span>
        <span class="k">if</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">ONNX</span> <span class="ow">in</span> <span class="n">export_formats</span><span class="p">:</span>
            <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">export_dir</span><span class="p">,</span> <span class="n">ExportFormat</span><span class="o">.</span><span class="n">ONNX</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">export_policy_model</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">onnx</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ONNX_OPSET&quot;</span><span class="p">,</span> <span class="s2">&quot;11&quot;</span><span class="p">)))</span>
            <span class="n">exported</span><span class="p">[</span><span class="n">ExportFormat</span><span class="o">.</span><span class="n">ONNX</span><span class="p">]</span> <span class="o">=</span> <span class="n">path</span>
        <span class="k">return</span> <span class="n">exported</span>

    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns current state of Algorithm, sufficient to restore it from scratch.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The current state dict of this Algorithm, which can be used to sufficiently</span>
<span class="sd">            restore the algorithm from scratch without any other information.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Algorithm.__getstate__() not supported anymore on the new API stack! &quot;</span>
                <span class="s2">&quot;Use Algorithm.get_state() instead.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Add config to state so complete Algorithm can be reproduced w/o it.</span>
        <span class="n">state</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;algorithm_class&quot;</span><span class="p">:</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">),</span>
            <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get_state</span><span class="p">(),</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;env_runner_group&quot;</span><span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>

        <span class="c1"># Also store eval `policy_mapping_fn` (in case it&#39;s different from main</span>
        <span class="c1"># one). Note, the new `EnvRunner API` has no policy mapping function.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;eval_env_runner_group&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;eval_policy_mapping_fn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner</span><span class="o">.</span><span class="n">policy_mapping_fn</span>

        <span class="c1"># Save counters.</span>
        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;counters&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span>

        <span class="c1"># TODO: Experimental functionality: Store contents of replay buffer</span>
        <span class="c1">#  to checkpoint, only if user has configured this.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;store_buffer_in_checkpoints&quot;</span>
        <span class="p">):</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;local_replay_buffer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span><span class="o">.</span><span class="n">get_state</span><span class="p">()</span>

        <span class="c1"># Save current `training_iteration`.</span>
        <span class="n">state</span><span class="p">[</span><span class="n">TRAINING_ITERATION</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_iteration</span>

        <span class="k">return</span> <span class="n">state</span>

    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sets the algorithm to the provided state.</span>

<span class="sd">        Args:</span>
<span class="sd">            state: The state dict to restore this Algorithm instance to. `state` may</span>
<span class="sd">                have been returned by a call to an Algorithm&#39;s `__getstate__()` method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Algorithm.__setstate__() not supported anymore on the new API stack! &quot;</span>
                <span class="s2">&quot;Use Algorithm.set_state() instead.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Old API stack: The local worker stores its state (together with all the</span>
        <span class="c1"># Module information) in state[&#39;worker&#39;].</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;env_runner_group&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="s2">&quot;worker&quot;</span> <span class="ow">in</span> <span class="n">state</span> <span class="ow">and</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">])</span>
            <span class="n">remote_state_ref</span> <span class="o">=</span> <span class="n">ray</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span>
                <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">w</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">remote_state_ref</span><span class="p">)),</span>
                <span class="n">local_env_runner</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">:</span>
                <span class="c1"># Avoid `state` being pickled into the remote function below.</span>
                <span class="n">_eval_policy_mapping_fn</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;eval_policy_mapping_fn&quot;</span><span class="p">)</span>

                <span class="k">def</span> <span class="nf">_setup_eval_worker</span><span class="p">(</span><span class="n">w</span><span class="p">):</span>
                    <span class="n">w</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">ray</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">remote_state_ref</span><span class="p">))</span>
                    <span class="c1"># Override `policy_mapping_fn` as it might be different for eval</span>
                    <span class="c1"># workers.</span>
                    <span class="n">w</span><span class="o">.</span><span class="n">set_policy_mapping_fn</span><span class="p">(</span><span class="n">_eval_policy_mapping_fn</span><span class="p">)</span>

                <span class="c1"># If evaluation workers are used, also restore the policies</span>
                <span class="c1"># there in case they are used for evaluation purpose.</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_worker</span><span class="p">(</span><span class="n">_setup_eval_worker</span><span class="p">)</span>

        <span class="c1"># Restore replay buffer data.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># TODO: Experimental functionality: Restore contents of replay</span>
            <span class="c1">#  buffer from checkpoint, only if user has configured this.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">store_buffer_in_checkpoints</span><span class="p">:</span>
                <span class="k">if</span> <span class="s2">&quot;local_replay_buffer&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">local_replay_buffer</span><span class="o">.</span><span class="n">set_state</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;local_replay_buffer&quot;</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                        <span class="s2">&quot;`store_buffer_in_checkpoints` is True, but no replay &quot;</span>
                        <span class="s2">&quot;data found in state!&quot;</span>
                    <span class="p">)</span>
            <span class="k">elif</span> <span class="s2">&quot;local_replay_buffer&quot;</span> <span class="ow">in</span> <span class="n">state</span> <span class="ow">and</span> <span class="n">log_once</span><span class="p">(</span>
                <span class="s2">&quot;no_store_buffer_in_checkpoints_but_data_found&quot;</span>
            <span class="p">):</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;`store_buffer_in_checkpoints` is False, but some replay &quot;</span>
                    <span class="s2">&quot;data found in state!&quot;</span>
                <span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;counters&quot;</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;counters&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">TRAINING_ITERATION</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_iteration</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="n">TRAINING_ITERATION</span><span class="p">]</span>

    <span class="nd">@OldAPIStack</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_checkpoint_info_to_algorithm_state</span><span class="p">(</span>
        <span class="n">checkpoint_info</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">policy_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Collection</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policy_mapping_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">AgentID</span><span class="p">,</span> <span class="n">EpisodeID</span><span class="p">],</span> <span class="n">PolicyID</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">policies_to_train</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">Union</span><span class="p">[</span>
                <span class="n">Collection</span><span class="p">[</span><span class="n">PolicyID</span><span class="p">],</span>
                <span class="n">Callable</span><span class="p">[[</span><span class="n">PolicyID</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SampleBatchType</span><span class="p">]],</span> <span class="nb">bool</span><span class="p">],</span>
            <span class="p">]</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Converts a checkpoint info or object to a proper Algorithm state dict.</span>

<span class="sd">        The returned state dict can be used inside self.__setstate__().</span>

<span class="sd">        Args:</span>
<span class="sd">            checkpoint_info: A checkpoint info dict as returned by</span>
<span class="sd">                `ray.rllib.utils.checkpoints.get_checkpoint_info(</span>
<span class="sd">                [checkpoint dir or AIR Checkpoint])`.</span>
<span class="sd">            policy_ids: Optional list/set of PolicyIDs. If not None, only those policies</span>
<span class="sd">                listed here will be included in the returned state. Note that</span>
<span class="sd">                state items such as filters, the `is_policy_to_train` function, as</span>
<span class="sd">                well as the multi-agent `policy_ids` dict will be adjusted as well,</span>
<span class="sd">                based on this arg.</span>
<span class="sd">            policy_mapping_fn: An optional (updated) policy mapping function</span>
<span class="sd">                to include in the returned state.</span>
<span class="sd">            policies_to_train: An optional list of policy IDs to be trained</span>
<span class="sd">                or a callable taking PolicyID and SampleBatchType and</span>
<span class="sd">                returning a bool (trainable or not?) to include in the returned state.</span>

<span class="sd">        Returns:</span>
<span class="sd">             The state dict usable within the `self.__setstate__()` method.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;type&quot;</span><span class="p">]</span> <span class="o">!=</span> <span class="s2">&quot;Algorithm&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`checkpoint` arg passed to &quot;</span>
                <span class="s2">&quot;`Algorithm._checkpoint_info_to_algorithm_state()` must be an &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Algorithm checkpoint (but is </span><span class="si">{</span><span class="n">checkpoint_info</span><span class="p">[</span><span class="s1">&#39;type&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">)!&quot;</span>
            <span class="p">)</span>

        <span class="n">msgpack</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">checkpoint_info</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;format&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;msgpack&quot;</span><span class="p">:</span>
            <span class="n">msgpack</span> <span class="o">=</span> <span class="n">try_import_msgpack</span><span class="p">(</span><span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;state_file&quot;</span><span class="p">],</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">msgpack</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">data</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">msgpack</span><span class="o">.</span><span class="n">unpackb</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">raw</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

        <span class="c1"># Old API stack: Policies are in separate sub-dirs.</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_version&quot;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">version</span><span class="o">.</span><span class="n">Version</span><span class="p">(</span><span class="s2">&quot;0.1&quot;</span><span class="p">)</span>
            <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;worker&quot;</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="n">state</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;worker&quot;</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="n">worker_state</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;worker&quot;</span><span class="p">]</span>

            <span class="c1"># Retrieve the set of all required policy IDs.</span>
            <span class="n">policy_ids</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
                <span class="n">policy_ids</span> <span class="k">if</span> <span class="n">policy_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_ids&quot;</span><span class="p">]</span>
            <span class="p">)</span>

            <span class="c1"># Remove those policies entirely from filters that are not in</span>
            <span class="c1"># `policy_ids`.</span>
            <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;filters&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">pid</span><span class="p">:</span> <span class="nb">filter</span>
                <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="nb">filter</span> <span class="ow">in</span> <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;filters&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policy_ids</span>
            <span class="p">}</span>

            <span class="c1"># Get Algorithm class.</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
                <span class="c1"># Try deserializing from a full classpath.</span>
                <span class="c1"># Or as a last resort: Tune registered algorithm name.</span>
                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">deserialize_type</span><span class="p">(</span>
                    <span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">]</span>
                <span class="p">)</span> <span class="ow">or</span> <span class="n">get_trainable_cls</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">])</span>
            <span class="c1"># Compile actual config object.</span>
            <span class="n">default_config</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;algorithm_class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get_default_config</span><span class="p">()</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">default_config</span><span class="p">,</span> <span class="n">AlgorithmConfig</span><span class="p">):</span>
                <span class="n">new_config</span> <span class="o">=</span> <span class="n">default_config</span><span class="o">.</span><span class="n">update_from_dict</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_config</span> <span class="o">=</span> <span class="n">Algorithm</span><span class="o">.</span><span class="n">merge_algorithm_configs</span><span class="p">(</span>
                    <span class="n">default_config</span><span class="p">,</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span>
                <span class="p">)</span>

            <span class="c1"># Remove policies from multiagent dict that are not in `policy_ids`.</span>
            <span class="n">new_policies</span> <span class="o">=</span> <span class="n">new_config</span><span class="o">.</span><span class="n">policies</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">new_policies</span><span class="p">,</span> <span class="p">(</span><span class="nb">set</span><span class="p">,</span> <span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="n">new_policies</span> <span class="o">=</span> <span class="p">{</span><span class="n">pid</span> <span class="k">for</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">new_policies</span> <span class="k">if</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policy_ids</span><span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_policies</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">pid</span><span class="p">:</span> <span class="n">spec</span> <span class="k">for</span> <span class="n">pid</span><span class="p">,</span> <span class="n">spec</span> <span class="ow">in</span> <span class="n">new_policies</span><span class="o">.</span><span class="n">items</span><span class="p">()</span> <span class="k">if</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policy_ids</span>
                <span class="p">}</span>
            <span class="n">new_config</span><span class="o">.</span><span class="n">multi_agent</span><span class="p">(</span>
                <span class="n">policies</span><span class="o">=</span><span class="n">new_policies</span><span class="p">,</span>
                <span class="n">policies_to_train</span><span class="o">=</span><span class="n">policies_to_train</span><span class="p">,</span>
                <span class="o">**</span><span class="p">(</span>
                    <span class="p">{</span><span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">:</span> <span class="n">policy_mapping_fn</span><span class="p">}</span>
                    <span class="k">if</span> <span class="n">policy_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="p">{}</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_config</span>

            <span class="c1"># Prepare local `worker` state to add policies&#39; states into it,</span>
            <span class="c1"># read from separate policy checkpoint files.</span>
            <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_states&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">pid</span> <span class="ow">in</span> <span class="n">policy_ids</span><span class="p">:</span>
                <span class="n">policy_state_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_dir&quot;</span><span class="p">],</span>
                    <span class="s2">&quot;policies&quot;</span><span class="p">,</span>
                    <span class="n">pid</span><span class="p">,</span>
                    <span class="s2">&quot;policy_state.&quot;</span>
                    <span class="o">+</span> <span class="p">(</span><span class="s2">&quot;msgpck&quot;</span> <span class="k">if</span> <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;format&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="s2">&quot;msgpack&quot;</span> <span class="k">else</span> <span class="s2">&quot;pkl&quot;</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">policy_state_file</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Given checkpoint does not seem to be valid! No policy &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;state file found for PID=</span><span class="si">{</span><span class="n">pid</span><span class="si">}</span><span class="s2">. &quot;</span>
                        <span class="sa">f</span><span class="s2">&quot;The file not found is: </span><span class="si">{</span><span class="n">policy_state_file</span><span class="si">}</span><span class="s2">.&quot;</span>
                    <span class="p">)</span>

                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">policy_state_file</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">msgpack</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_states&quot;</span><span class="p">][</span><span class="n">pid</span><span class="p">]</span> <span class="o">=</span> <span class="n">msgpack</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_states&quot;</span><span class="p">][</span><span class="n">pid</span><span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

            <span class="c1"># These two functions are never serialized in a msgpack checkpoint (which</span>
            <span class="c1"># does not store code, unlike a cloudpickle checkpoint). Hence the user has</span>
            <span class="c1"># to provide them with the `Algorithm.from_checkpoint()` call.</span>
            <span class="k">if</span> <span class="n">policy_mapping_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;policy_mapping_fn&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">policy_mapping_fn</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">policies_to_train</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="c1"># `policies_to_train` might be left None in case all policies should be</span>
                <span class="c1"># trained.</span>
                <span class="ow">or</span> <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;is_policy_to_train&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">NOT_SERIALIZABLE</span>
            <span class="p">):</span>
                <span class="n">worker_state</span><span class="p">[</span><span class="s2">&quot;is_policy_to_train&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">policies_to_train</span>

        <span class="k">if</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;config&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">enable_rl_module_and_learner</span><span class="p">:</span>
            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;learner_state_dir&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                <span class="n">checkpoint_info</span><span class="p">[</span><span class="s2">&quot;checkpoint_dir&quot;</span><span class="p">],</span> <span class="s2">&quot;learner&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">state</span>

    <span class="nd">@DeveloperAPI</span>
    <span class="k">def</span> <span class="nf">_create_local_replay_buffer_if_necessary</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">PartialAlgorithmConfigDict</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiAgentReplayBuffer</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a MultiAgentReplayBuffer instance if necessary.</span>

<span class="sd">        Args:</span>
<span class="sd">            config: Algorithm-specific configuration data.</span>

<span class="sd">        Returns:</span>
<span class="sd">            MultiAgentReplayBuffer instance based on algorithm config.</span>
<span class="sd">            None, if local replay buffer is not needed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
            <span class="s2">&quot;no_local_replay_buffer&quot;</span>
        <span class="p">):</span>
            <span class="k">return</span>

        <span class="k">return</span> <span class="n">from_config</span><span class="p">(</span><span class="n">ReplayBuffer</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;replay_buffer_config&quot;</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">_run_one_training_iteration</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ResultDict</span><span class="p">,</span> <span class="s2">&quot;TrainIterCtx&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs one training iteration (`self.iteration` will be +1 after this).</span>

<span class="sd">        Calls `self.training_step()` repeatedly until the configured minimum time (sec),</span>
<span class="sd">        minimum sample- or minimum training steps have been reached.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The ResultDict from the last call to `training_step()`. Note that even</span>
<span class="sd">            though we only return the last ResultDict, the user stil has full control</span>
<span class="sd">            over the history and reduce behavior of individual metrics at the time these</span>
<span class="sd">            metrics are logged with `self.metrics.log_...()`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">TRAINING_ITERATION_TIMER</span><span class="p">]:</span>
            <span class="c1"># In case we are training (in a thread) parallel to evaluation,</span>
            <span class="c1"># we may have to re-enable eager mode here (gets disabled in the</span>
            <span class="c1"># thread).</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;framework&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;tf2&quot;</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">executing_eagerly</span><span class="p">():</span>
                <span class="n">tf1</span><span class="o">.</span><span class="n">enable_eager_execution</span><span class="p">()</span>

            <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">training_step_results</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="c1"># Create a step context ...</span>
            <span class="k">with</span> <span class="n">TrainIterCtx</span><span class="p">(</span><span class="n">algo</span><span class="o">=</span><span class="bp">self</span><span class="p">)</span> <span class="k">as</span> <span class="n">train_iter_ctx</span><span class="p">:</span>
                <span class="c1"># .. so we can query it whether we should stop the iteration loop (e.g.</span>
                <span class="c1"># when we have reached `min_time_s_per_iteration`).</span>
                <span class="k">while</span> <span class="ow">not</span> <span class="n">train_iter_ctx</span><span class="o">.</span><span class="n">should_stop</span><span class="p">(</span><span class="n">training_step_results</span><span class="p">):</span>
                    <span class="c1"># Before training step, try to bring failed workers back.</span>
                    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">RESTORE_WORKERS_TIMER</span><span class="p">]:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">restore_workers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="p">)</span>

                    <span class="c1"># Try to train one step.</span>
                    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">TRAINING_STEP_TIMER</span><span class="p">]:</span>
                        <span class="c1"># TODO (sven): Should we reduce the different</span>
                        <span class="c1">#  `training_step_results` over time with MetricsLogger.</span>
                        <span class="n">training_step_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">training_step</span><span class="p">()</span>

                    <span class="k">if</span> <span class="n">training_step_results</span><span class="p">:</span>
                        <span class="n">results</span> <span class="o">=</span> <span class="n">training_step_results</span>

        <span class="k">return</span> <span class="n">results</span><span class="p">,</span> <span class="n">train_iter_ctx</span>

    <span class="k">def</span> <span class="nf">_run_one_evaluation</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">parallel_train_future</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">ThreadPoolExecutor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ResultDict</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs evaluation step via `self.evaluate()` and handling worker failures.</span>

<span class="sd">        Args:</span>
<span class="sd">            parallel_train_future: In case, we are training and avaluating in parallel,</span>
<span class="sd">                this arg carries the currently running ThreadPoolExecutor object that</span>
<span class="sd">                runs the training iteration. Use `parallel_train_future.done()` to</span>
<span class="sd">                check, whether the parallel training job has completed and</span>
<span class="sd">                `parallel_train_future.result()` to get its return values.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The results dict from the evaluation call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">RESTORE_EVAL_WORKERS_TIMER</span><span class="p">]:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">restore_workers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">)</span>

        <span class="c1"># Run `self.evaluate()` only once per training iteration.</span>
        <span class="c1"># TODO (sven): Move this timer into new metrics-logger API.</span>
        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">EVALUATION_ITERATION_TIMER</span><span class="p">]:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">parallel_train_future</span><span class="o">=</span><span class="n">parallel_train_future</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">EVALUATION_ITERATION_TIMER</span><span class="p">]</span><span class="o">.</span><span class="n">push_units_processed</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED_FOR_EVALUATION_THIS_ITER</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># After evaluation, do a round of health check on remote eval workers to see if</span>
        <span class="c1"># any of the failed workers are back.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Add number of healthy evaluation workers after this iteration.</span>
            <span class="n">eval_results</span><span class="p">[</span>
                <span class="s2">&quot;num_healthy_workers&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
            <span class="n">eval_results</span><span class="p">[</span>
                <span class="s2">&quot;num_in_flight_async_reqs&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_in_flight_async_reqs</span><span class="p">()</span>
            <span class="n">eval_results</span><span class="p">[</span>
                <span class="s2">&quot;num_remote_worker_restarts&quot;</span>
            <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_remote_worker_restarts</span><span class="p">()</span>

        <span class="k">return</span> <span class="p">{</span><span class="n">EVALUATION_RESULTS</span><span class="p">:</span> <span class="n">eval_results</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">_run_one_training_iteration_and_evaluation_in_parallel</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ResultDict</span><span class="p">,</span> <span class="n">ResultDict</span><span class="p">,</span> <span class="s2">&quot;TrainIterCtx&quot;</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs one training iteration and one evaluation step in parallel.</span>

<span class="sd">        First starts the training iteration (via `self._run_one_training_iteration()`)</span>
<span class="sd">        within a ThreadPoolExecutor, then runs the evaluation step in parallel.</span>
<span class="sd">        In auto-duration mode (config.evaluation_duration=auto), makes sure the</span>
<span class="sd">        evaluation step takes roughly the same time as the training iteration.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple containing the training results, the evaluation results, and</span>
<span class="sd">            the `TrainIterCtx` object returned by the training call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="n">concurrent</span><span class="o">.</span><span class="n">futures</span><span class="o">.</span><span class="n">ThreadPoolExecutor</span><span class="p">()</span> <span class="k">as</span> <span class="n">executor</span><span class="p">:</span>
            <span class="n">parallel_train_future</span> <span class="o">=</span> <span class="n">executor</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span>
                <span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_training_iteration</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="n">evaluation_results</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="c1"># If the debug setting _run_training_always_in_thread is used, do NOT</span>
            <span class="c1"># evaluate, no matter what the settings are,</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">_run_training_always_in_thread</span><span class="p">:</span>
                <span class="c1"># Pass the train_future into `self._run_one_evaluation()` to allow it</span>
                <span class="c1"># to run exactly as long as the training iteration takes in case</span>
                <span class="c1"># evaluation_duration=auto.</span>
                <span class="n">evaluation_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_evaluation</span><span class="p">(</span>
                    <span class="n">parallel_train_future</span><span class="o">=</span><span class="n">parallel_train_future</span>
                <span class="p">)</span>
            <span class="c1"># Collect the training results from the future.</span>
            <span class="n">train_results</span><span class="p">,</span> <span class="n">train_iter_ctx</span> <span class="o">=</span> <span class="n">parallel_train_future</span><span class="o">.</span><span class="n">result</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">train_results</span><span class="p">,</span> <span class="n">evaluation_results</span><span class="p">,</span> <span class="n">train_iter_ctx</span>

    <span class="c1"># Experimental method (trying to achieve evaluation and training in parallel w/o</span>
    <span class="c1"># using a thread pool).</span>
    <span class="k">def</span> <span class="nf">_run_one_training_iteration_and_evaluation_in_parallel_wo_thread</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">ResultDict</span><span class="p">,</span> <span class="n">ResultDict</span><span class="p">,</span> <span class="s2">&quot;TrainIterCtx&quot;</span><span class="p">]:</span>

        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span> <span class="o">!=</span> <span class="s2">&quot;auto&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration_unit</span> <span class="o">==</span> <span class="s2">&quot;timesteps&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="p">[</span><span class="n">RESTORE_EVAL_WORKERS_TIMER</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">restore_workers</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">)</span>

        <span class="c1"># Call the `_before_evaluate` hook.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_before_evaluate</span><span class="p">()</span>

        <span class="c1"># Sync weights to the evaluation EnvRunners.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">sync_weights</span><span class="p">(</span>
                <span class="n">from_worker_or_learner_group</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_sync_filters_if_needed</span><span class="p">(</span>
                <span class="n">central_worker</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="p">,</span>
                <span class="n">workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="p">,</span>
                <span class="n">config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">on_evaluate_start</span><span class="p">(</span><span class="n">algorithm</span><span class="o">=</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics_logger</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">)</span>

        <span class="n">env_steps</span> <span class="o">=</span> <span class="n">agent_steps</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Kick off sampling on all evaluation workers (async).</span>
        <span class="c1"># How many timesteps do we need to run?</span>
        <span class="n">eval_cfg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span>
        <span class="n">time_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_sample_timeout_s</span>

        <span class="n">all_metrics</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">algo_iteration</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span>

        <span class="c1"># In case all the remote evaluation workers die during a round of</span>
        <span class="c1"># evaluation, we need to stop.</span>
        <span class="n">units_per_healthy_remote_worker</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">eval_cfg</span><span class="o">.</span><span class="n">rollout_fragment_length</span> <span class="o">*</span> <span class="n">eval_cfg</span><span class="o">.</span><span class="n">num_envs_per_env_runner</span>
        <span class="p">)</span>
        <span class="c1"># Select proper number of evaluation workers for this round.</span>
        <span class="n">selected_eval_worker_ids</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">worker_id</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">worker_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">healthy_worker_ids</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">*</span> <span class="n">units_per_healthy_remote_worker</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">foreach_worker_async</span><span class="p">(</span>
            <span class="n">func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">sample</span><span class="p">(),</span> <span class="n">w</span><span class="o">.</span><span class="n">get_metrics</span><span class="p">(),</span> <span class="n">algo_iteration</span><span class="p">),</span>
            <span class="n">remote_worker_ids</span><span class="o">=</span><span class="n">selected_eval_worker_ids</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Run training and collect the training results.</span>
        <span class="n">train_results</span><span class="p">,</span> <span class="n">train_iter_ctx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_one_training_iteration</span><span class="p">()</span>

        <span class="c1"># Collect the evaluation results.</span>
        <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">fetch_ready_async_reqs</span><span class="p">(</span>
            <span class="n">return_obj_refs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">timeout_seconds</span><span class="o">=</span><span class="n">time_out</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">wid</span><span class="p">,</span> <span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="nb">iter</span><span class="p">)</span> <span class="ow">in</span> <span class="n">eval_results</span><span class="p">:</span>
            <span class="c1"># Skip results from an older iteration.</span>
            <span class="k">if</span> <span class="nb">iter</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">agent_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">agent_steps</span><span class="p">()</span>
            <span class="n">env_steps</span> <span class="o">+=</span> <span class="n">batch</span><span class="o">.</span><span class="n">env_steps</span><span class="p">()</span>
            <span class="n">all_metrics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="n">summarize_episodes</span><span class="p">(</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">keep_custom_metrics</span><span class="o">=</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="n">eval_results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED_THIS_ITER</span><span class="p">]</span> <span class="o">=</span> <span class="n">agent_steps</span>
            <span class="n">eval_results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED_THIS_ITER</span><span class="p">]</span> <span class="o">=</span> <span class="n">env_steps</span>
            <span class="c1"># TODO: Remove this key at some point. Here for backward compatibility.</span>
            <span class="n">eval_results</span><span class="p">[</span><span class="s2">&quot;timesteps_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">eval_results</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="n">NUM_ENV_STEPS_SAMPLED_THIS_ITER</span><span class="p">,</span> <span class="mi">0</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">merge_and_log_n_dicts</span><span class="p">(</span>
                <span class="n">all_metrics</span><span class="p">,</span>
                <span class="n">key</span><span class="o">=</span><span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">ENV_RUNNER_RESULTS</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">reduce</span><span class="p">((</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="n">ENV_RUNNER_RESULTS</span><span class="p">))</span>

        <span class="c1"># Warn if results are empty, it could be that this is because the eval timesteps</span>
        <span class="c1"># are not enough to run through one full episode.</span>
        <span class="k">if</span> <span class="n">eval_results</span><span class="p">[</span><span class="n">ENV_RUNNER_RESULTS</span><span class="p">][</span><span class="n">NUM_EPISODES</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;This evaluation iteration resulted in an empty set of episode summary &quot;</span>
                <span class="s2">&quot;results! It&#39;s possible that your configured duration timesteps are not&quot;</span>
                <span class="s2">&quot; enough to finish even a single episode. Your have configured &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">evaluation_duration_unit</span><span class="si">}</span><span class="s2">. For &#39;timesteps&#39;, try &quot;</span>
                <span class="s2">&quot;increasing this value via the `config.evaluation(evaluation_duration=&quot;</span>
                <span class="s2">&quot;...)` OR change the unit to &#39;episodes&#39; via `config.evaluation(&quot;</span>
                <span class="s2">&quot;evaluation_duration_unit=&#39;episodes&#39;)` OR try increasing the timeout &quot;</span>
                <span class="s2">&quot;threshold via `config.evaluation(evaluation_sample_timeout_s=...)` OR &quot;</span>
                <span class="s2">&quot;you can also set `config.evaluation_force_reset_envs_before_iteration`&quot;</span>
                <span class="s2">&quot; to False. However, keep in mind that in the latter case, the &quot;</span>
                <span class="s2">&quot;evaluation results may contain some episode stats generated with &quot;</span>
                <span class="s2">&quot;earlier weights versions.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># After evaluation, do a round of health check on remote eval workers to see if</span>
        <span class="c1"># any of the failed workers are back.</span>
        <span class="c1"># Add number of healthy evaluation workers after this iteration.</span>
        <span class="n">eval_results</span><span class="p">[</span>
            <span class="s2">&quot;num_healthy_workers&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
        <span class="n">eval_results</span><span class="p">[</span>
            <span class="s2">&quot;num_in_flight_async_reqs&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_in_flight_async_reqs</span><span class="p">()</span>
        <span class="n">eval_results</span><span class="p">[</span>
            <span class="s2">&quot;num_remote_worker_restarts&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">num_remote_worker_restarts</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">train_results</span><span class="p">,</span> <span class="p">{</span><span class="s2">&quot;evaluation&quot;</span><span class="p">:</span> <span class="n">eval_results</span><span class="p">},</span> <span class="n">train_iter_ctx</span>

    <span class="k">def</span> <span class="nf">_run_offline_evaluation</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Runs offline evaluation via `OfflineEvaluator.estimate_on_dataset()` API.</span>

<span class="sd">        This method will be used when `evaluation_dataset` is provided.</span>
<span class="sd">        Note: This will only work if the policy is a single agent policy.</span>

<span class="sd">        Returns:</span>
<span class="sd">            The results dict from the offline evaluation call.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span><span class="o">.</span><span class="n">policy_map</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>

        <span class="n">parallelism</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">evaluation_num_env_runners</span> <span class="ow">or</span> <span class="mi">1</span>
        <span class="n">offline_eval_results</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">:</span> <span class="p">{}}</span>
        <span class="k">for</span> <span class="n">evaluator_name</span><span class="p">,</span> <span class="n">offline_evaluator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_estimators</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">offline_eval_results</span><span class="p">[</span><span class="s2">&quot;off_policy_estimator&quot;</span><span class="p">][</span>
                <span class="n">evaluator_name</span>
            <span class="p">]</span> <span class="o">=</span> <span class="n">offline_evaluator</span><span class="o">.</span><span class="n">estimate_on_dataset</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">evaluation_dataset</span><span class="p">,</span>
                <span class="n">n_parallelism</span><span class="o">=</span><span class="n">parallelism</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">offline_eval_results</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">_should_create_evaluation_rollout_workers</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">eval_config</span><span class="p">:</span> <span class="s2">&quot;AlgorithmConfig&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Determines whether we need to create evaluation workers.</span>

<span class="sd">        Returns False if we need to run offline evaluation</span>
<span class="sd">        (with ope.estimate_on_dastaset API) or when local worker is to be used for</span>
<span class="sd">        evaluation. Note: We only use estimate_on_dataset API with bandits for now.</span>
<span class="sd">        That is when ope_split_batch_by_episode is False.</span>
<span class="sd">        TODO: In future we will do the same for episodic RL OPE.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">run_offline_evaluation</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">eval_config</span><span class="o">.</span><span class="n">off_policy_estimation_methods</span>
            <span class="ow">and</span> <span class="ow">not</span> <span class="n">eval_config</span><span class="o">.</span><span class="n">ope_split_batch_by_episode</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="ow">not</span> <span class="n">run_offline_evaluation</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="n">eval_config</span><span class="o">.</span><span class="n">evaluation_num_env_runners</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="ow">or</span> <span class="n">eval_config</span><span class="o">.</span><span class="n">evaluation_interval</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_compile_iteration_results_new_api_stack</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">train_results</span><span class="p">,</span> <span class="n">eval_results</span><span class="p">,</span> <span class="n">step_ctx</span>
    <span class="p">):</span>
        <span class="c1"># Return dict (shallow copy of `train_results`).</span>
        <span class="n">results</span><span class="p">:</span> <span class="n">ResultDict</span> <span class="o">=</span> <span class="n">train_results</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="c1"># Collect old-API-stack-style `self._timers` results.</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">timer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">TIMERS</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
                <span class="n">results</span><span class="p">[</span><span class="n">TIMERS</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">results</span><span class="p">[</span><span class="n">TIMERS</span><span class="p">][</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_time_sec&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span> <span class="o">=</span> <span class="n">timer</span><span class="o">.</span><span class="n">mean</span>
            <span class="k">if</span> <span class="n">timer</span><span class="o">.</span><span class="n">has_units_processed</span><span class="p">():</span>
                <span class="n">results</span><span class="p">[</span><span class="n">TIMERS</span><span class="p">][</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_throughput&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span>
                    <span class="n">timer</span><span class="o">.</span><span class="n">mean_throughput</span><span class="p">,</span> <span class="mi">3</span>
                <span class="p">)</span>

        <span class="c1"># Evaluation results.</span>
        <span class="k">if</span> <span class="n">eval_results</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">isinstance</span><span class="p">(</span><span class="n">eval_results</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
                <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">eval_results</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
                <span class="ow">and</span> <span class="n">EVALUATION_RESULTS</span> <span class="ow">in</span> <span class="n">eval_results</span>
            <span class="p">)</span>
            <span class="n">results</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">eval_results</span><span class="p">)</span>
        <span class="c1"># Fault tolerance stats.</span>
        <span class="n">results</span><span class="p">[</span><span class="n">FAULT_TOLERANCE_STATS</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;num_healthy_workers&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">(),</span>
            <span class="s2">&quot;num_in_flight_async_reqs&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">num_in_flight_async_reqs</span><span class="p">()</span>
            <span class="p">),</span>
            <span class="s2">&quot;num_remote_worker_restarts&quot;</span><span class="p">:</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">num_remote_worker_restarts</span><span class="p">()</span>
            <span class="p">),</span>
        <span class="p">}</span>
        <span class="c1"># Resolve all `Stats` leafs by peeking (get their reduced values).</span>
        <span class="k">return</span> <span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span>
            <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="o">.</span><span class="n">peek</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">Stats</span><span class="p">)</span> <span class="k">else</span> <span class="n">s</span><span class="p">,</span>
            <span class="n">results</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@OldAPIStack</span>
    <span class="k">def</span> <span class="nf">_compile_iteration_results_old_api_stack</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">episodes_this_iter</span><span class="p">,</span> <span class="n">step_ctx</span><span class="p">,</span> <span class="n">iteration_results</span>
    <span class="p">):</span>
        <span class="c1"># Results to be returned.</span>
        <span class="n">results</span><span class="p">:</span> <span class="n">ResultDict</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Evaluation results.</span>
        <span class="k">if</span> <span class="s2">&quot;evaluation&quot;</span> <span class="ow">in</span> <span class="n">iteration_results</span><span class="p">:</span>
            <span class="n">eval_results</span> <span class="o">=</span> <span class="n">iteration_results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;evaluation&quot;</span><span class="p">)</span>
            <span class="n">iteration_results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">EVALUATION_RESULTS</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="n">results</span><span class="p">[</span><span class="s2">&quot;evaluation&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="n">EVALUATION_RESULTS</span><span class="p">]</span> <span class="o">=</span> <span class="n">eval_results</span>

        <span class="c1"># Custom metrics and episode media.</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;custom_metrics&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iteration_results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;custom_metrics&quot;</span><span class="p">,</span> <span class="p">{})</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;episode_media&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iteration_results</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;episode_media&quot;</span><span class="p">,</span> <span class="p">{})</span>

        <span class="c1"># Learner info.</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;info&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="n">LEARNER_INFO</span><span class="p">:</span> <span class="n">iteration_results</span><span class="p">}</span>

        <span class="c1"># Calculate how many (if any) of older, historical episodes we have to add to</span>
        <span class="c1"># `episodes_this_iter` in order to reach the required smoothing window.</span>
        <span class="n">episodes_for_metrics</span> <span class="o">=</span> <span class="n">episodes_this_iter</span><span class="p">[:]</span>
        <span class="n">missing</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">metrics_num_episodes_for_smoothing</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span>
            <span class="n">episodes_this_iter</span>
        <span class="p">)</span>
        <span class="c1"># We have to add some older episodes to reach the smoothing window size.</span>
        <span class="k">if</span> <span class="n">missing</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">episodes_for_metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span><span class="p">[</span><span class="o">-</span><span class="n">missing</span><span class="p">:]</span> <span class="o">+</span> <span class="n">episodes_this_iter</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="nb">len</span><span class="p">(</span><span class="n">episodes_for_metrics</span><span class="p">)</span>
                <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">metrics_num_episodes_for_smoothing</span>
            <span class="p">)</span>
        <span class="c1"># Note that when there are more than `metrics_num_episodes_for_smoothing`</span>
        <span class="c1"># episodes in `episodes_for_metrics`, leave them as-is. In this case, we&#39;ll</span>
        <span class="c1"># compute the stats over that larger number.</span>

        <span class="c1"># Add new episodes to our history and make sure it doesn&#39;t grow larger than</span>
        <span class="c1"># needed.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">episodes_this_iter</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_episode_history</span><span class="p">[</span>
            <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">metrics_num_episodes_for_smoothing</span> <span class="p">:</span>
        <span class="p">]</span>
        <span class="n">results</span><span class="p">[</span><span class="n">ENV_RUNNER_RESULTS</span><span class="p">]</span> <span class="o">=</span> <span class="n">summarize_episodes</span><span class="p">(</span>
            <span class="n">episodes_for_metrics</span><span class="p">,</span>
            <span class="n">episodes_this_iter</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">keep_per_episode_custom_metrics</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">results</span><span class="p">[</span>
            <span class="s2">&quot;num_healthy_workers&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">num_healthy_remote_workers</span><span class="p">()</span>
        <span class="n">results</span><span class="p">[</span>
            <span class="s2">&quot;num_in_flight_async_sample_reqs&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">num_in_flight_async_reqs</span><span class="p">()</span>
        <span class="n">results</span><span class="p">[</span>
            <span class="s2">&quot;num_remote_worker_restarts&quot;</span>
        <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">num_remote_worker_restarts</span><span class="p">()</span>

        <span class="c1"># Train-steps- and env/agent-steps this iteration.</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">,</span>
            <span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">,</span>
            <span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">,</span>
            <span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">,</span>
        <span class="p">]:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">c</span><span class="p">]</span>
        <span class="n">time_taken_sec</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">get_time_taken_sec</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;agent_steps&quot;</span><span class="p">:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span> <span class="o">+</span> <span class="s2">&quot;_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">sampled</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_TRAINED</span> <span class="o">+</span> <span class="s2">&quot;_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span> <span class="o">+</span> <span class="s2">&quot;_throughput_per_sec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">step_ctx</span><span class="o">.</span><span class="n">sampled</span> <span class="o">/</span> <span class="n">time_taken_sec</span>
            <span class="p">)</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_TRAINED</span> <span class="o">+</span> <span class="s2">&quot;_throughput_per_sec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span> <span class="o">/</span> <span class="n">time_taken_sec</span>
            <span class="p">)</span>
            <span class="c1"># TODO: For CQL and other algos, count by trained steps.</span>
            <span class="n">results</span><span class="p">[</span><span class="s2">&quot;timesteps_total&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span> <span class="o">+</span> <span class="s2">&quot;_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">sampled</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_TRAINED</span> <span class="o">+</span> <span class="s2">&quot;_this_iter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span> <span class="o">+</span> <span class="s2">&quot;_throughput_per_sec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">step_ctx</span><span class="o">.</span><span class="n">sampled</span> <span class="o">/</span> <span class="n">time_taken_sec</span>
            <span class="p">)</span>
            <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_TRAINED</span> <span class="o">+</span> <span class="s2">&quot;_throughput_per_sec&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span> <span class="o">/</span> <span class="n">time_taken_sec</span>
            <span class="p">)</span>
            <span class="c1"># TODO: For CQL and other algos, count by trained steps.</span>
            <span class="n">results</span><span class="p">[</span><span class="s2">&quot;timesteps_total&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">]</span>

        <span class="c1"># Forward compatibility with new API stack.</span>
        <span class="n">results</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s2">&quot;timesteps_total&quot;</span><span class="p">]</span>
        <span class="n">results</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED_LIFETIME</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span>
            <span class="n">NUM_AGENT_STEPS_SAMPLED</span>
        <span class="p">]</span>

        <span class="c1"># TODO: Backward compatibility.</span>
        <span class="n">results</span><span class="p">[</span><span class="n">STEPS_TRAINED_THIS_ITER_COUNTER</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_ctx</span><span class="o">.</span><span class="n">trained</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;agent_timesteps_total&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span>

        <span class="c1"># Process timer results.</span>
        <span class="n">timers</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">timer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_timers</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">timers</span><span class="p">[</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_time_ms&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">timer</span><span class="o">.</span><span class="n">mean</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">timer</span><span class="o">.</span><span class="n">has_units_processed</span><span class="p">():</span>
                <span class="n">timers</span><span class="p">[</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">_throughput&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">timer</span><span class="o">.</span><span class="n">mean_throughput</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;timers&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">timers</span>

        <span class="c1"># Process counter results.</span>
        <span class="n">counters</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">counter</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_counters</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">counters</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">counter</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;counters&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">counters</span>
        <span class="c1"># TODO: Backward compatibility.</span>
        <span class="n">results</span><span class="p">[</span><span class="s2">&quot;info&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">counters</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">results</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">env_runner</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">eval_env_runner</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span><span class="o">.</span><span class="n">local_env_runner</span>

    <span class="k">def</span> <span class="nf">_record_usage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Record the framework and algorithm used.</span>

<span class="sd">        Args:</span>
<span class="sd">            config: Algorithm config dict.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">record_extra_usage_tag</span><span class="p">(</span><span class="n">TagKey</span><span class="o">.</span><span class="n">RLLIB_FRAMEWORK</span><span class="p">,</span> <span class="n">config</span><span class="p">[</span><span class="s2">&quot;framework&quot;</span><span class="p">])</span>
        <span class="n">record_extra_usage_tag</span><span class="p">(</span><span class="n">TagKey</span><span class="o">.</span><span class="n">RLLIB_NUM_WORKERS</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s2">&quot;num_env_runners&quot;</span><span class="p">]))</span>
        <span class="n">alg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="c1"># We do not want to collect user defined algorithm names.</span>
        <span class="k">if</span> <span class="n">alg</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">ALL_ALGORITHMS</span><span class="p">:</span>
            <span class="n">alg</span> <span class="o">=</span> <span class="s2">&quot;USER_DEFINED&quot;</span>
        <span class="n">record_extra_usage_tag</span><span class="p">(</span><span class="n">TagKey</span><span class="o">.</span><span class="n">RLLIB_ALGORITHM</span><span class="p">,</span> <span class="n">alg</span><span class="p">)</span>

    <span class="nd">@Deprecated</span><span class="p">(</span><span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">import_policy_model_from_h5</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="nd">@Deprecated</span><span class="p">(</span><span class="n">error</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">import_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="nd">@Deprecated</span><span class="p">(</span>
        <span class="n">new</span><span class="o">=</span><span class="s2">&quot;Algorithm.env_runner_group&quot;</span><span class="p">,</span>
        <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">workers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">env_runner_group</span>

    <span class="nd">@Deprecated</span><span class="p">(</span>
        <span class="n">new</span><span class="o">=</span><span class="s2">&quot;Algorithm.eval_env_runner_group&quot;</span><span class="p">,</span>
        <span class="n">error</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">evaluation_workers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">eval_env_runner_group</span></div>



<span class="k">class</span> <span class="nc">TrainIterCtx</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">algo</span><span class="p">:</span> <span class="n">Algorithm</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">algo</span> <span class="o">=</span> <span class="n">algo</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_start</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_stop</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Before first call to `step()`, `results` is expected to be None -&gt;</span>
        <span class="c1"># Start with self.failures=-1 -&gt; set to 0 before the very first call</span>
        <span class="c1"># to `self.step()`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">failures</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">time_start</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_sampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                <span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                <span class="n">NUM_ENV_STEPS_TRAINED_LIFETIME</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_sampled</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                    <span class="n">NUM_AGENT_STEPS_SAMPLED_LIFETIME</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">{}</span>
                <span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_trained</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                    <span class="n">NUM_AGENT_STEPS_TRAINED_LIFETIME</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">{}</span>
                <span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_sampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_sampled</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_trained</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">failure_tolerance</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">num_consecutive_env_runner_failures_tolerance</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">time_stop</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">get_time_taken_sec</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Returns the time we spent in the context in seconds.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_stop</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_start</span>

    <span class="k">def</span> <span class="nf">should_stop</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">results</span><span class="p">):</span>
        <span class="c1"># Before first call to `step()`.</span>
        <span class="k">if</span> <span class="n">results</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Fail after n retries.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">failures</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">failures</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">failure_tolerance</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                    <span class="s2">&quot;More than `num_consecutive_env_runner_failures_tolerance=&quot;</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">failure_tolerance</span><span class="si">}</span><span class="s2">` consecutive worker failures! &quot;</span>
                    <span class="s2">&quot;Exiting.&quot;</span>
                <span class="p">)</span>
            <span class="c1"># Continue to very first `step()` call or retry `step()` after</span>
            <span class="c1"># a (tolerable) failure.</span>
            <span class="k">return</span> <span class="kc">False</span>

        <span class="c1"># Stopping criteria.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">enable_env_runner_and_connector_v2</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;agent_steps&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="nb">sum</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                            <span class="n">NUM_AGENT_STEPS_SAMPLED_LIFETIME</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">{}</span>
                        <span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_sampled</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="nb">sum</span><span class="p">(</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span>
                            <span class="n">NUM_AGENT_STEPS_TRAINED_LIFETIME</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="p">{}</span>
                        <span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
                    <span class="p">)</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_trained</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span><span class="n">NUM_ENV_STEPS_SAMPLED_LIFETIME</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_sampled</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">peek</span><span class="p">(</span><span class="n">NUM_ENV_STEPS_TRAINED_LIFETIME</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_trained</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">count_steps_by</span> <span class="o">==</span> <span class="s2">&quot;agent_steps&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_SAMPLED</span><span class="p">]</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_sampled</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_AGENT_STEPS_TRAINED</span><span class="p">]</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_agent_steps_trained</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_SAMPLED</span><span class="p">]</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_sampled</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">_counters</span><span class="p">[</span><span class="n">NUM_ENV_STEPS_TRAINED</span><span class="p">]</span>
                    <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_env_steps_trained</span>
                <span class="p">)</span>

        <span class="n">min_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_time_s_per_iteration</span>
        <span class="n">min_sample_ts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_sample_timesteps_per_iteration</span>
        <span class="n">min_train_ts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">algo</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">min_train_timesteps_per_iteration</span>
        <span class="c1"># Repeat if not enough time has passed or if not enough</span>
        <span class="c1"># env|train timesteps have been processed (or these min</span>
        <span class="c1"># values are not provided by the user).</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="p">(</span><span class="ow">not</span> <span class="n">min_t</span> <span class="ow">or</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">time_start</span> <span class="o">&gt;=</span> <span class="n">min_t</span><span class="p">)</span>
            <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">min_sample_ts</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">sampled</span> <span class="o">&gt;=</span> <span class="n">min_sample_ts</span><span class="p">)</span>
            <span class="ow">and</span> <span class="p">(</span><span class="ow">not</span> <span class="n">min_train_ts</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">&gt;=</span> <span class="n">min_train_ts</span><span class="p">)</span>
        <span class="p">):</span>
            <span class="k">return</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="footer-content-items footer-content__inner">
  
    <div class="footer-content-item"><div id="csat">
  <div id="csat-feedback-received" class="csat-hidden">
    <span>Thanks for the feedback!</span>
  </div>
  <div id="csat-inputs">
    <span>Was this helpful?</span>
    <div id="csat-yes" class="csat-button">
      <svg id="csat-yes-icon" class="csat-hidden csat-icon" width="18" height="13" viewBox="0 0 18 13" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 10.172L16.1922 0.979004L17.6072 2.393L7.00023 13L0.63623 6.636L2.05023 5.222L7.00023 10.172Z" />
      </svg>
      <span>Yes<span>
    </div>
    <div id="csat-no" class="csat-button">
      <svg id="csat-no-icon" class="csat-hidden csat-icon" width="14" height="14" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
        <path d="M7.00023 5.58599L11.9502 0.635986L13.3642 2.04999L8.41423 6.99999L13.3642 11.95L11.9502 13.364L7.00023 8.41399L2.05023 13.364L0.63623 11.95L5.58623 6.99999L0.63623 2.04999L2.05023 0.635986L7.00023 5.58599Z" />
      </svg>
      <span>No<span>
    </div>
  </div>
  <div id="csat-textarea-group" class="csat-hidden">
    <span id="csat-feedback-label">Feedback</span>
    <textarea id="csat-textarea"></textarea>
    <div id="csat-submit">Submit</div>
  </div>
</div></div>
  
</div>

          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="https://docs.ray.io/en/master/_static/scripts/bootstrap.js?digest=ac02cc09edc035673794"></script>
<script src="https://docs.ray.io/en/master/_static/scripts/pydata-sphinx-theme.js?digest=ac02cc09edc035673794"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">
  <p class="copyright">
    
      © Copyright 2024, The Ray Team.
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">
  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.3.7.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item"><p class="theme-version">
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.14.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>